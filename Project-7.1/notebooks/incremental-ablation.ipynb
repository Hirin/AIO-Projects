{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.0"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":125907,"databundleVersionId":14910023,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Incremental Ablation Study - VideoMAE\n\n**Chiến lược**: Tích hợp từng kỹ thuật một cách tuần tự để đo lường cải thiện tích lũy.\n\n| Exp | Name | Techniques | Expected Acc |\n|-----|------|------------|--------------|\n| 1 | Baseline | VideoMAE (LR=5e-5) | ~83.92% |\n| 2 | +ConsistentAug | + Consistent Spatial Augmentation | ~84.31% |\n| 3 | +Mixup | + Mixup (α=0.8) | ~82.55%* |\n| 4 | +2Stage | + 2-Stage (Mixup→Label Smoothing) | ~84.71% |\n| 5 | +TTA | + 6-View TTA at Inference | **~85.10%** |\n\n*Mixup đơn lẻ giảm accuracy ngắn hạn nhưng khi kết hợp với 2-Stage sẽ hiệu quả hơn.\n\n**Epochs**: 10 (Phase 1: 7, Phase 2: 3 nếu 2-Stage)\n","metadata":{}},{"cell_type":"code","source":"# =================== CONFIGURATION ===================\n# ===== EXPERIMENT SELECTION =====\nRUN_EXP = 1                 # Which experiment to run: 1, 2, 3, or 4 (4 includes TTA)\n                            # Run separately on different Kaggle sessions\n\n# ===== QUICK TEST MODE =====\nQUICK_TEST = False          # True = test pipeline with 5 batches per phase\nQUICK_TEST_BATCHES = 5      # Number of batches when QUICK_TEST=True\n\n# Model Config\nMODEL_CKPT = 'MCG-NJU/videomae-base-finetuned-kinetics'\nNUM_FRAMES = 16\nIMG_SIZE = 224\nRESIZE_SIZE = 256\n\n# Training Config\nEPOCHS_TOTAL = 1 if QUICK_TEST else 20\nEPOCHS_P1 = 1 if QUICK_TEST else 15      # Phase 1 epochs (when 2-Stage)\nEPOCHS_P2 = 1 if QUICK_TEST else 5      # Phase 2 epochs (when 2-Stage)\nBATCH_SIZE = 8\nACCUM_STEPS = 4\nLR_P1 = 5e-5\nLR_P2 = 1e-6\nWEIGHT_DECAY = 0.05\nWARMUP_RATIO = 0.1\n\n# Augmentation Config\nMIXUP_ALPHA = 0.8\nLABEL_SMOOTHING_EPS = 0.1\n\n# Paths (Kaggle)\nPATH_DATA_TRAIN = '/kaggle/input/action-video/data/data_train'\nPATH_DATA_TEST = '/kaggle/input/action-video/data/test'\nTEST_LABELS_URL = '1Xv2CWOqdBj3kt0rkNJKRsodSIEd3-wX_'\n\nEXP_NAMES = {1: 'Baseline', 2: '+ConsistentAug', 3: '+Mixup', 4: '+2Stage+TTA'}\nprint('='*60)\nprint(f'RUNNING: Exp {RUN_EXP} - {EXP_NAMES[RUN_EXP]}')\nif QUICK_TEST:\n    print(f'  [QUICK_TEST] Only {QUICK_TEST_BATCHES} batches, 1 epoch')\nprint('='*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T15:58:52.866909Z","iopub.execute_input":"2026-02-05T15:58:52.867184Z","iopub.status.idle":"2026-02-05T15:58:52.874219Z","shell.execute_reply.started":"2026-02-05T15:58:52.867163Z","shell.execute_reply":"2026-02-05T15:58:52.873320Z"}},"outputs":[{"name":"stdout","text":"============================================================\nRUNNING: Exp 1 - Baseline\n============================================================\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"import os\nimport sys\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nimport gc\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom pathlib import Path\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF\n\nfrom transformers import VideoMAEForVideoClassification, VideoMAEImageProcessor\nfrom transformers import get_cosine_schedule_with_warmup\nfrom sklearn.metrics import accuracy_score\n\n# Seed\ndef seed_everything(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nseed_everything(42)\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Device: {DEVICE}')\n\nMEAN = [0.485, 0.456, 0.406]\nSTD = [0.229, 0.224, 0.225]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T15:58:52.875697Z","iopub.execute_input":"2026-02-05T15:58:52.876055Z","iopub.status.idle":"2026-02-05T15:58:52.891529Z","shell.execute_reply.started":"2026-02-05T15:58:52.876036Z","shell.execute_reply":"2026-02-05T15:58:52.890915Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":64},{"cell_type":"markdown","source":"## 1. Dataset Classes","metadata":{}},{"cell_type":"code","source":"class VideoDataset(Dataset):\n    \"\"\"Training dataset with toggle for consistent spatial aug.\"\"\"\n    def __init__(self, root, num_frames=16, consistent_aug=False):\n        self.root = Path(root)\n        self.num_frames = num_frames\n        self.consistent_aug = consistent_aug\n        \n        self.classes = sorted([d.name for d in self.root.iterdir() if d.is_dir()])\n        self.class_to_idx = {c: i for i, c in enumerate(self.classes)}\n        \n        self.samples = []\n        for cls in self.classes:\n            for vid in (self.root / cls).iterdir():\n                if vid.is_dir():\n                    self.samples.append((vid, self.class_to_idx[cls]))\n        print(f'Loaded {len(self.samples)} videos, {len(self.classes)} classes')\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        vid_dir, label = self.samples[idx]\n        files = sorted(vid_dir.glob('*.jpg'))\n        indices = torch.linspace(0, len(files)-1, self.num_frames).long()\n        \n        frames = [Image.open(files[i]).convert('RGB') for i in indices]\n        frames = [TF.resize(img, RESIZE_SIZE) for img in frames]\n        \n        if self.consistent_aug:\n            # Same crop/flip for all frames\n            i, j, h, w = T.RandomResizedCrop.get_params(frames[0], (0.8, 1.0), (0.75, 1.33))\n            do_flip = random.random() > 0.5\n            processed = []\n            for img in frames:\n                img = TF.resized_crop(img, i, j, h, w, (IMG_SIZE, IMG_SIZE))\n                if do_flip:\n                    img = TF.hflip(img)\n                img = TF.normalize(TF.to_tensor(img), MEAN, STD)\n                processed.append(img)\n        else:\n            # Simple center crop (no aug)\n            processed = [TF.normalize(TF.to_tensor(TF.center_crop(img, IMG_SIZE)), MEAN, STD) \n                         for img in frames]\n        \n        return torch.stack(processed), label\n\n\nclass TestDatasetSingle(Dataset):\n    \"\"\"Test with single center crop.\"\"\"\n    def __init__(self, root, num_frames=16):\n        self.root = Path(root)\n        self.num_frames = num_frames\n        self.samples = [(d, int(d.name)) for d in self.root.iterdir() if d.is_dir()]\n        self.samples.sort(key=lambda x: x[1])\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        vid_dir, vid_id = self.samples[idx]\n        files = sorted(vid_dir.glob('*.jpg'))\n        indices = torch.linspace(0, len(files)-1, self.num_frames).long()\n        frames = []\n        for i in indices:\n            img = Image.open(files[i]).convert('RGB')\n            img = TF.resize(img, RESIZE_SIZE)\n            img = TF.center_crop(img, IMG_SIZE)\n            img = TF.normalize(TF.to_tensor(img), MEAN, STD)\n            frames.append(img)\n        return torch.stack(frames), vid_id\n\n\nclass TestDatasetTTA(Dataset):\n    \"\"\"6-view TTA: 3 spatial crops × 2 flip states.\"\"\"\n    def __init__(self, root, num_frames=16):\n        self.root = Path(root)\n        self.num_frames = num_frames\n        self.samples = [(d, int(d.name)) for d in self.root.iterdir() if d.is_dir()]\n        self.samples.sort(key=lambda x: x[1])\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        vid_dir, vid_id = self.samples[idx]\n        files = sorted(vid_dir.glob('*.jpg'))\n        indices = torch.linspace(0, len(files)-1, self.num_frames).long()\n        \n        frames = [Image.open(files[i]).convert('RGB') for i in indices]\n        frames = [TF.resize(img, RESIZE_SIZE) for img in frames]\n        \n        w, h = frames[0].size\n        views = []\n        \n        # 3 spatial crops\n        crop_positions = [\n            ((h - IMG_SIZE) // 2, (w - IMG_SIZE) // 2),  # center\n            (0, 0),  # top-left\n            (max(0, h - IMG_SIZE), max(0, w - IMG_SIZE))  # bottom-right\n        ]\n        \n        for top, left in crop_positions:\n            view_frames = []\n            for img in frames:\n                cropped = TF.crop(img, top, left, IMG_SIZE, IMG_SIZE)\n                view_frames.append(TF.normalize(TF.to_tensor(cropped), MEAN, STD))\n            views.append(torch.stack(view_frames))\n            \n            # Flipped version\n            view_frames_flip = []\n            for img in frames:\n                cropped = TF.crop(img, top, left, IMG_SIZE, IMG_SIZE)\n                cropped = TF.hflip(cropped)\n                view_frames_flip.append(TF.normalize(TF.to_tensor(cropped), MEAN, STD))\n            views.append(torch.stack(view_frames_flip))\n        \n        return torch.stack(views), vid_id  # [6, T, C, H, W]\n\nprint('Dataset classes defined.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T15:58:52.892397Z","iopub.execute_input":"2026-02-05T15:58:52.892729Z","iopub.status.idle":"2026-02-05T15:58:52.912347Z","shell.execute_reply.started":"2026-02-05T15:58:52.892703Z","shell.execute_reply":"2026-02-05T15:58:52.911557Z"}},"outputs":[{"name":"stdout","text":"Dataset classes defined.\n","output_type":"stream"}],"execution_count":65},{"cell_type":"markdown","source":"## 2. Training Utilities","metadata":{}},{"cell_type":"code","source":"class MixupCollate:\n    \"\"\"Collate function with Mixup augmentation.\"\"\"\n    def __init__(self, num_classes, alpha=0.8):\n        self.num_classes = num_classes\n        self.alpha = alpha\n\n    def __call__(self, batch):\n        inputs, targets = torch.utils.data.default_collate(batch)\n        batch_size = inputs.size(0)\n        index = torch.randperm(batch_size)\n        lam = np.random.beta(self.alpha, self.alpha)\n        inputs = lam * inputs + (1 - lam) * inputs[index, :]\n        targets_one_hot = F.one_hot(targets, num_classes=self.num_classes).float()\n        targets = lam * targets_one_hot + (1 - lam) * targets_one_hot[index, :]\n        return inputs, targets\n\n\ndef train_epoch(model, loader, optimizer, scheduler, scaler, \n                use_mixup=False, label_smoothing=0.0, max_batches=None):\n    \"\"\"Train one epoch. If max_batches is set, stop early for quick testing.\"\"\"\n    model.train()\n    total_loss, total_correct, total_samples = 0.0, 0, 0\n    pbar = tqdm(loader, desc='Training', leave=False)\n    optimizer.zero_grad()\n    \n    for step, (inputs, targets) in enumerate(pbar):\n        if max_batches is not None and step >= max_batches:\n            break\n        \n        inputs = inputs.to(DEVICE)\n        \n        with torch.amp.autocast('cuda'):\n            output = model(inputs)\n            logits = output.logits if hasattr(output, 'logits') else output\n            \n            if use_mixup:\n                targets = targets.to(DEVICE)\n                log_probs = F.log_softmax(logits, dim=1)\n                loss = -torch.sum(targets * log_probs, dim=1).mean()\n                true_labels = targets.argmax(dim=1)\n            else:\n                targets = targets.to(DEVICE)\n                loss = F.cross_entropy(logits, targets, label_smoothing=label_smoothing)\n                true_labels = targets\n        \n        total_correct += (logits.argmax(1) == true_labels).sum().item()\n        total_samples += inputs.size(0)\n        \n        scaler.scale(loss / ACCUM_STEPS).backward()\n        \n        if (step + 1) % ACCUM_STEPS == 0:\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n            if scheduler:\n                scheduler.step()\n        \n        total_loss += loss.item()\n        pbar.set_postfix({'loss': f'{total_loss/(step+1):.4f}', \n                          'acc': f'{total_correct/max(total_samples,1):.4f}'})\n    \n    return total_loss / max(step+1, 1), total_correct / max(total_samples, 1)\n\n\n@torch.no_grad()\ndef evaluate(model, loader, classes, gt_dict, use_tta=False, max_batches=None):\n    \"\"\"Evaluate model on test set. If max_batches is set, stop early for quick testing.\"\"\"\n    model.eval()\n    predictions = []\n    \n    for step, (videos, video_ids) in enumerate(tqdm(loader, desc='Evaluating', leave=False)):\n        if max_batches is not None and step >= max_batches:\n            break\n        \n        if use_tta:\n            B, V, T, C, H, W = videos.shape\n            videos = videos.view(B * V, T, C, H, W).to(DEVICE)\n            output = model(videos)\n            logits = output.logits if hasattr(output, 'logits') else output\n            logits = logits.view(B, V, -1).mean(dim=1)\n            preds = logits.argmax(1).cpu().tolist()\n        else:\n            videos = videos.to(DEVICE)\n            output = model(videos)\n            logits = output.logits if hasattr(output, 'logits') else output\n            preds = logits.argmax(1).cpu().tolist()\n        \n        predictions.extend(zip(video_ids.tolist(), preds))\n    \n    if max_batches is not None:\n        return 0.0  # Skip accuracy calc in quick test mode\n    \n    y_true = [gt_dict[str(vid)] for vid, _ in predictions]\n    y_pred = [classes[p] for _, p in predictions]\n    return accuracy_score(y_true, y_pred)\n\nprint('Training utilities defined.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T15:58:53.055207Z","iopub.execute_input":"2026-02-05T15:58:53.055815Z","iopub.status.idle":"2026-02-05T15:58:53.070033Z","shell.execute_reply.started":"2026-02-05T15:58:53.055784Z","shell.execute_reply":"2026-02-05T15:58:53.069196Z"}},"outputs":[{"name":"stdout","text":"Training utilities defined.\n","output_type":"stream"}],"execution_count":66},{"cell_type":"markdown","source":"## 3. Load Data","metadata":{}},{"cell_type":"code","source":"# Download test labels\n!gdown \"{TEST_LABELS_URL}\" -O test_labels.csv -q\ngt_df = pd.read_csv('test_labels.csv')\ngt_dict = dict(zip(gt_df['id'].astype(str), gt_df['class']))\nprint(f'Loaded test labels: {len(gt_dict)} samples')\n\n# Results storage\nall_results = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T15:58:53.071712Z","iopub.execute_input":"2026-02-05T15:58:53.072291Z","iopub.status.idle":"2026-02-05T15:58:55.153543Z","shell.execute_reply.started":"2026-02-05T15:58:53.072271Z","shell.execute_reply":"2026-02-05T15:58:55.152758Z"}},"outputs":[{"name":"stdout","text":"Loaded test labels: 510 samples\n","output_type":"stream"}],"execution_count":67},{"cell_type":"markdown","source":"---\n## Exp 1: VideoMAE Baseline\n- **Techniques**: None (just fine-tune with LR=5e-5)\n- **Expected**: ~83.92%","metadata":{}},{"cell_type":"code","source":"if RUN_EXP == 1:\n    print('='*60)\n    print('EXP 1: BASELINE (No augmentation)')\n    print('='*60)\n\n    # Dataset without consistent aug\n    train_ds = VideoDataset(PATH_DATA_TRAIN, NUM_FRAMES, consistent_aug=False)\n    test_ds = TestDatasetSingle(PATH_DATA_TEST, NUM_FRAMES)\n\n    train_loader = DataLoader(train_ds, BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True, persistent_workers=True)\n    test_loader = DataLoader(test_ds, BATCH_SIZE, shuffle=False, num_workers=2, persistent_workers=True)\n\n    # Model\n    model = VideoMAEForVideoClassification.from_pretrained(\n        MODEL_CKPT,\n        num_labels=len(train_ds.classes),\n        ignore_mismatched_sizes=True,\n        num_frames=NUM_FRAMES\n    ).to(DEVICE)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LR_P1, weight_decay=WEIGHT_DECAY)\n    total_steps = len(train_loader) * EPOCHS_TOTAL // ACCUM_STEPS\n    scheduler = get_cosine_schedule_with_warmup(optimizer, int(total_steps * WARMUP_RATIO), total_steps)\n    scaler = torch.amp.GradScaler()\n\n    best_acc = 0.0\n    max_batches = QUICK_TEST_BATCHES if QUICK_TEST else None\n    for epoch in range(1, EPOCHS_TOTAL + 1):\n        loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler, scaler, max_batches=max_batches)\n        test_acc = evaluate(model, test_loader, train_ds.classes, gt_dict, max_batches=max_batches)\n        \n        status = ''\n        if test_acc > best_acc:\n            best_acc = test_acc\n            torch.save(model.state_dict(), 'exp1_best.pt')\n            status = '>>> BEST'\n        \n        print(f'Ep {epoch}/{EPOCHS_TOTAL}: Loss={loss:.4f} TrainAcc={train_acc:.4f} TestAcc={test_acc:.4f} {status}')\n\n    print(f'\\nExp1 Best: {best_acc:.4f}')\n    all_results.append({'exp': 1, 'name': 'Baseline', 'test_acc': best_acc})\n    del model, optimizer, scheduler\n    torch.cuda.empty_cache()\n    gc.collect()\nelse:\n    print('Skipping Exp1 (RUN_EXP != 1)')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T15:58:55.154663Z","iopub.execute_input":"2026-02-05T15:58:55.155320Z","iopub.status.idle":"2026-02-05T16:00:47.395695Z","shell.execute_reply.started":"2026-02-05T15:58:55.155291Z","shell.execute_reply":"2026-02-05T16:00:47.394386Z"}},"outputs":[{"name":"stdout","text":"============================================================\nEXP 1: BASELINE (No augmentation)\n============================================================\nLoaded 6254 videos, 51 classes\n","output_type":"stream"},{"name":"stderr","text":"Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base-finetuned-kinetics and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([51]) in the model instantiated\n- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([51, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/781 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c48fa291a17d4ff3ba9825a30363aa01"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/3546538197.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mmax_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQUICK_TEST_BATCHES\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mQUICK_TEST\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS_TOTAL\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/566333122.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, optimizer, scheduler, scaler, use_mixup, label_smoothing, max_batches)\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         pbar.set_postfix({'loss': f'{total_loss/(step+1):.4f}', \n\u001b[1;32m     62\u001b[0m                           'acc': f'{total_correct/max(total_samples,1):.4f}'})\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":68},{"cell_type":"markdown","source":"---\n## Exp 2: + Consistent Spatial Augmentation\n- **New**: Same crop/flip for all 16 frames\n- **Expected**: +0.39% (~84.31%)","metadata":{}},{"cell_type":"code","source":"if RUN_EXP == 2:\n    print('='*60)\n    print('EXP 2: + CONSISTENT SPATIAL AUGMENTATION')\n    print('='*60)\n\n    train_ds = VideoDataset(PATH_DATA_TRAIN, NUM_FRAMES, consistent_aug=True)\n    test_ds = TestDatasetSingle(PATH_DATA_TEST, NUM_FRAMES)\n    train_loader = DataLoader(train_ds, BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True, persistent_workers=True)\n    test_loader = DataLoader(test_ds, BATCH_SIZE, shuffle=False, num_workers=2, persistent_workers=True)\n\n    model = VideoMAEForVideoClassification.from_pretrained(\n        MODEL_CKPT, num_labels=len(train_ds.classes),\n        ignore_mismatched_sizes=True, num_frames=NUM_FRAMES\n    ).to(DEVICE)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LR_P1, weight_decay=WEIGHT_DECAY)\n    total_steps = len(train_loader) * EPOCHS_TOTAL // ACCUM_STEPS\n    scheduler = get_cosine_schedule_with_warmup(optimizer, int(total_steps * WARMUP_RATIO), total_steps)\n    scaler = torch.amp.GradScaler()\n\n    best_acc = 0.0\n    max_batches = QUICK_TEST_BATCHES if QUICK_TEST else None\n    for epoch in range(1, EPOCHS_TOTAL + 1):\n        loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler, scaler, max_batches=max_batches)\n        test_acc = evaluate(model, test_loader, train_ds.classes, gt_dict, max_batches=max_batches)\n        status = '>>> BEST' if test_acc > best_acc else ''\n        if test_acc > best_acc:\n            best_acc = test_acc\n            torch.save(model.state_dict(), 'exp2_best.pt')\n        print(f'Ep {epoch}/{EPOCHS_TOTAL}: Loss={loss:.4f} TrainAcc={train_acc:.4f} TestAcc={test_acc:.4f} {status}')\n\n    print(f'Exp2 Best: {best_acc:.4f}')\n    all_results.append({'exp': 2, 'name': '+ConsistentAug', 'test_acc': best_acc})\n    del model, optimizer, scheduler\n    torch.cuda.empty_cache(); gc.collect()\nelse:\n    print('Skipping Exp2')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T16:00:47.396328Z","iopub.status.idle":"2026-02-05T16:00:47.396555Z","shell.execute_reply.started":"2026-02-05T16:00:47.396443Z","shell.execute_reply":"2026-02-05T16:00:47.396454Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## Exp 3: + Mixup\n- **New**: Mixup (α=0.8) on top of Consistent Aug\n- **Note**: Mixup alone may decrease accuracy but reduces overfitting gap\n- **Expected**: ~82.55% (trade-off for better generalization)","metadata":{}},{"cell_type":"code","source":"if RUN_EXP == 3:\n    print('='*60)\n    print('EXP 3: + MIXUP')\n    print('='*60)\n\n    train_ds = VideoDataset(PATH_DATA_TRAIN, NUM_FRAMES, consistent_aug=True)\n    test_ds = TestDatasetSingle(PATH_DATA_TEST, NUM_FRAMES)\n    mixup_collate = MixupCollate(len(train_ds.classes), MIXUP_ALPHA)\n    train_loader = DataLoader(train_ds, BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=mixup_collate, drop_last=True, persistent_workers=True)\n    test_loader = DataLoader(test_ds, BATCH_SIZE, shuffle=False, num_workers=2, persistent_workers=True)\n\n    model = VideoMAEForVideoClassification.from_pretrained(\n        MODEL_CKPT, num_labels=len(train_ds.classes),\n        ignore_mismatched_sizes=True, num_frames=NUM_FRAMES\n    ).to(DEVICE)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LR_P1, weight_decay=WEIGHT_DECAY)\n    total_steps = len(train_loader) * EPOCHS_TOTAL // ACCUM_STEPS\n    scheduler = get_cosine_schedule_with_warmup(optimizer, int(total_steps * WARMUP_RATIO), total_steps)\n    scaler = torch.amp.GradScaler()\n\n    best_acc = 0.0\n    max_batches = QUICK_TEST_BATCHES if QUICK_TEST else None\n    for epoch in range(1, EPOCHS_TOTAL + 1):\n        loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler, scaler, use_mixup=True, max_batches=max_batches)\n        test_acc = evaluate(model, test_loader, train_ds.classes, gt_dict, max_batches=max_batches)\n        status = '>>> BEST' if test_acc > best_acc else ''\n        if test_acc > best_acc:\n            best_acc = test_acc\n            torch.save(model.state_dict(), 'exp3_best.pt')\n        print(f'Ep {epoch}/{EPOCHS_TOTAL}: Loss={loss:.4f} TrainAcc={train_acc:.4f} TestAcc={test_acc:.4f} {status}')\n\n    print(f'Exp3 Best: {best_acc:.4f}')\n    all_results.append({'exp': 3, 'name': '+Mixup', 'test_acc': best_acc})\n    del model, optimizer, scheduler\n    torch.cuda.empty_cache(); gc.collect()\nelse:\n    print('Skipping Exp3')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T16:00:47.397594Z","iopub.status.idle":"2026-02-05T16:00:47.397904Z","shell.execute_reply.started":"2026-02-05T16:00:47.397707Z","shell.execute_reply":"2026-02-05T16:00:47.397716Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## Exp 4+5: 2-Stage Training + TTA\n- **Phase 1**: Mixup + LR=5e-5\n- **Phase 2**: Label Smoothing + LR=1e-6\n- **TTA**: 3 spatial crops × 2 flip states\n","metadata":{}},{"cell_type":"code","source":"if RUN_EXP == 4:\n    print('='*60)\n    print('EXP 4+5: 2-STAGE TRAINING + TTA')\n    print('='*60)\n    \n    # Setup\n    train_ds = VideoDataset(PATH_DATA_TRAIN, NUM_FRAMES, consistent_aug=True)\n    test_ds = TestDatasetSingle(PATH_DATA_TEST, NUM_FRAMES)\n    mixup_collate = MixupCollate(len(train_ds.classes), MIXUP_ALPHA)\n    train_loader_p1 = DataLoader(train_ds, BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=mixup_collate, drop_last=True, persistent_workers=True)\n    train_loader_p2 = DataLoader(train_ds, BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True, persistent_workers=True)\n    test_loader = DataLoader(test_ds, BATCH_SIZE, shuffle=False, num_workers=2, persistent_workers=True)\n    \n    model = VideoMAEForVideoClassification.from_pretrained(\n        MODEL_CKPT, num_labels=len(train_ds.classes),\n        ignore_mismatched_sizes=True, num_frames=NUM_FRAMES\n    ).to(DEVICE)\n    \n    best_acc = 0.0\n    max_batches = QUICK_TEST_BATCHES if QUICK_TEST else None\n    \n    # Phase 1: Mixup\n    print(f'\\\\n--- Phase 1: Mixup ({EPOCHS_P1} epochs) ---')\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LR_P1, weight_decay=WEIGHT_DECAY)\n    total_steps = len(train_loader_p1) * EPOCHS_P1 // ACCUM_STEPS\n    scheduler = get_cosine_schedule_with_warmup(optimizer, int(total_steps * WARMUP_RATIO), total_steps)\n    scaler = torch.amp.GradScaler()\n    \n    for epoch in range(1, EPOCHS_P1 + 1):\n        loss, train_acc = train_epoch(model, train_loader_p1, optimizer, scheduler, scaler, use_mixup=True, max_batches=max_batches)\n        test_acc = evaluate(model, test_loader, train_ds.classes, gt_dict, max_batches=max_batches)\n        if test_acc > best_acc:\n            best_acc = test_acc\n            torch.save(model.state_dict(), 'exp4_best.pt')\n        print(f'P1 Ep {epoch}/{EPOCHS_P1}: Loss={loss:.4f} Acc={test_acc:.4f}')\n    \n    if not os.path.exists('exp4_best.pt'):\n        torch.save(model.state_dict(), 'exp4_best.pt')\n    \n    # Phase 2: Label Smoothing\n    print(f'\\\\n--- Phase 2: Label Smoothing ({EPOCHS_P2} epochs) ---')\n    model.load_state_dict(torch.load('exp4_best.pt'))\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LR_P2, weight_decay=WEIGHT_DECAY)\n    total_steps = len(train_loader_p2) * EPOCHS_P2 // ACCUM_STEPS\n    scheduler = get_cosine_schedule_with_warmup(optimizer, int(total_steps * WARMUP_RATIO), total_steps)\n    scaler = torch.amp.GradScaler()\n    \n    for epoch in range(1, EPOCHS_P2 + 1):\n        loss, train_acc = train_epoch(model, train_loader_p2, optimizer, scheduler, scaler, label_smoothing=LABEL_SMOOTHING_EPS, max_batches=max_batches)\n        test_acc = evaluate(model, test_loader, train_ds.classes, gt_dict, max_batches=max_batches)\n        if test_acc > best_acc:\n            best_acc = test_acc\n            torch.save(model.state_dict(), 'exp4_best.pt')\n        print(f'P2 Ep {epoch}/{EPOCHS_P2}: Loss={loss:.4f} Acc={test_acc:.4f}')\n    \n    print(f'\\\\nExp4 (2-Stage) Best: {best_acc:.4f}')\n    all_results.append({'exp': 4, 'name': '+2Stage', 'test_acc': best_acc})\n    \n    # Exp5: TTA\n    print('\\\\n' + '='*60)\n    print('EXP 5: + 6-VIEW TTA')\n    print('='*60)\n    model.load_state_dict(torch.load('exp4_best.pt'))\n    test_ds_tta = TestDatasetTTA(PATH_DATA_TEST, NUM_FRAMES)\n    test_loader_tta = DataLoader(test_ds_tta, BATCH_SIZE, shuffle=False, num_workers=2, persistent_workers=True)\n    test_acc_tta = evaluate(model, test_loader_tta, train_ds.classes, gt_dict, use_tta=True, max_batches=max_batches)\n    print(f'\\\\nExp5 (TTA) Accuracy: {test_acc_tta:.4f}')\n    all_results.append({'exp': 5, 'name': '+TTA', 'test_acc': test_acc_tta})\n    \n    del model, optimizer, scheduler\n    torch.cuda.empty_cache()\n    gc.collect()\nelse:\n    print('Skipping Exp4+5 (RUN_EXP != 4)')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-05T16:00:47.399096Z","iopub.status.idle":"2026-02-05T16:00:47.399456Z","shell.execute_reply.started":"2026-02-05T16:00:47.399283Z","shell.execute_reply":"2026-02-05T16:00:47.399300Z"}},"outputs":[],"execution_count":null}]}