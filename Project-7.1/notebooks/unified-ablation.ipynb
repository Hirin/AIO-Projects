{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified Ablation Study - Video Action Recognition\n",
    "\n",
    "**Toggle ON/OFF để chạy các ablation experiments khác nhau**\n",
    "\n",
    "## Preset Configs\n",
    "\n",
    "| Preset | Toggles | Expected Acc (10 epochs)|\n",
    "|--------|---------|--------------|\n",
    "| ViT-Small Baseline | `MODEL=\"vit_small\"`, all OFF | 69.22% |\n",
    "| ViT-Base | `MODEL=\"vit_base\"`, all OFF | 73.73% |\n",
    "| VideoMAE Baseline | `MODEL=\"videomae\"`, all OFF | 83.92% |\n",
    "| VideoMAE 8-Frame | `MODEL=\"videomae\"`, `NUM_FRAMES=8` | 83.00% @ Phase 1: 50 epochs & Phase 2: 10 epochs |\n",
    "| Phase 3 (Current) | VideoMAE + CONSISTENT + MIXUP + LABEL_SMOOTHING + TWO_PHASE + FLIP_TTA | **85.10%** @ Phase 1: 30 epochs & Phase 2: 10 epochs |\n",
    "| Data Balance | VideoMAE + DATA_BALANCE + FOCAL_LOSS | 84.00% |\n",
    "| Layer Decay | VideoMAE + LAYER_DECAY + MIXUP | 84.51% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== ABLATION STUDY CONFIGURATION ===================\n",
    "# Toggle ON/OFF để bật/tắt các tính năng\n",
    "\n",
    "# ===== EXPERIMENT MODE =====\n",
    "WARMUP = False              # True = test pipeline nhanh (all models, 5 batches mỗi phase)\n",
    "\n",
    "# ===== MODEL SELECTION (chọn 1) =====\n",
    "MODEL_TYPE = \"videomae\"     # \"vit_small\" | \"vit_base\" | \"videomae\"\n",
    "\n",
    "# ===== FRAME CONFIG =====\n",
    "NUM_FRAMES = 16             # 8 hoặc 16\n",
    "\n",
    "# ===== DATA SPLIT =====\n",
    "TRAIN_VAL_RATIO = 0.9       # 0.9 = 90% train, 10% val. Set 1.0 để dùng full train\n",
    "USE_TEST_LABELS = True      # True = download test_labels.csv để tính test accuracy\n",
    "TEST_LABELS_URL = '1Xv2CWOqdBj3kt0rkNJKRsodSIEd3-wX_'  # Google Drive file ID\n",
    "\n",
    "# ===== DATA AUGMENTATION TOGGLES =====\n",
    "USE_DATA_BALANCE = False            # Offline augmentation để cân bằng class\n",
    "USE_CONSISTENT_SPATIAL_AUG = True   # Same crop/flip cho all frames\n",
    "USE_MIXUP = True                     # Mixup augmentation (α=0.8)\n",
    "USE_FOCAL_LOSS = False               # Focal Loss cho imbalanced data\n",
    "\n",
    "# ===== TRAINING TOGGLES =====\n",
    "USE_LABEL_SMOOTHING = True          # Label smoothing (ε=0.1)\n",
    "USE_TWO_PHASE = True                # 2-Phase: Mixup → Label Smoothing\n",
    "USE_LAYER_DECAY = False             # Layer-wise LR decay\n",
    "\n",
    "# ===== INFERENCE TOGGLES =====\n",
    "USE_MULTI_SEGMENT = False           # Multi-segment temporal sampling\n",
    "USE_FLIP_TTA = True                 # 6-view FlipTTA\n",
    "\n",
    "# ===== TRAINING PARAMS =====\n",
    "EPOCHS_P1 = 30 if not WARMUP else 1\n",
    "EPOCHS_P2 = 10 if not WARMUP else 1\n",
    "WARMUP_BATCHES = 5                  # Batches per phase khi WARMUP=True\n",
    "BATCH_SIZE = 8\n",
    "ACCUM_STEPS = 4\n",
    "LR_P1 = 5e-5\n",
    "LR_P2 = 1e-6\n",
    "WEIGHT_DECAY = 0.05\n",
    "WARMUP_RATIO = 0.1\n",
    "MIXUP_ALPHA = 0.8\n",
    "LABEL_SMOOTHING_EPS = 0.1\n",
    "\n",
    "# ===== PATHS (Kaggle) =====\n",
    "PATH_DATA_TRAIN = '/kaggle/input/action-video/data/data_train'\n",
    "PATH_DATA_TEST = '/kaggle/input/action-video/data/test'\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ABLATION CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"WARMUP:            {WARMUP}\")\n",
    "print(f\"MODEL_TYPE:        {MODEL_TYPE}\")\n",
    "print(f\"NUM_FRAMES:        {NUM_FRAMES}\")\n",
    "print(f\"TRAIN_VAL_RATIO:   {TRAIN_VAL_RATIO}\")\n",
    "print(f\"USE_TEST_LABELS:   {USE_TEST_LABELS}\")\n",
    "print(f\"DATA_BALANCE:      {USE_DATA_BALANCE}\")\n",
    "print(f\"CONSISTENT_AUG:    {USE_CONSISTENT_SPATIAL_AUG}\")\n",
    "print(f\"MIXUP:             {USE_MIXUP}\")\n",
    "print(f\"FOCAL_LOSS:        {USE_FOCAL_LOSS}\")\n",
    "print(f\"LABEL_SMOOTHING:   {USE_LABEL_SMOOTHING}\")\n",
    "print(f\"TWO_PHASE:         {USE_TWO_PHASE}\")\n",
    "print(f\"LAYER_DECAY:       {USE_LAYER_DECAY}\")\n",
    "print(f\"MULTI_SEGMENT:     {USE_MULTI_SEGMENT}\")\n",
    "print(f\"FLIP_TTA:          {USE_FLIP_TTA}\")\n",
    "print(f\"EPOCHS:            P1={EPOCHS_P1}, P2={EPOCHS_P2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import shutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import timm\n",
    "from transformers import VideoMAEForVideoClassification, VideoMAEImageProcessor\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Seed\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(42)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {DEVICE}')\n",
    "\n",
    "# Constants\n",
    "IMG_SIZE = 224\n",
    "RESIZE_SIZE = 256\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightweightViTForAction(nn.Module):\n",
    "    \"\"\"ViT frame-level model (ImageNet pretrained).\"\"\"\n",
    "    def __init__(self, num_classes=51, model_name='vit_small_patch16_224'):\n",
    "        super().__init__()\n",
    "        self.vit = timm.create_model(model_name, pretrained=True, num_classes=0)\n",
    "        self.embed_dim = self.vit.num_features\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes)\n",
    "    \n",
    "    def forward(self, video):\n",
    "        B, T, C, H, W = video.shape\n",
    "        x = video.view(B * T, C, H, W)\n",
    "        features = self.vit(x)\n",
    "        features = features.view(B, T, self.embed_dim)\n",
    "        pooled = features.mean(dim=1)\n",
    "        return self.head(pooled)\n",
    "\n",
    "\n",
    "def create_model(model_type, num_classes, num_frames):\n",
    "    \"\"\"Model factory based on MODEL_TYPE toggle.\"\"\"\n",
    "    if model_type == 'vit_small':\n",
    "        model = LightweightViTForAction(num_classes, 'vit_small_patch16_224')\n",
    "        print(f'Created ViT-Small model ({sum(p.numel() for p in model.parameters()):,} params)')\n",
    "    elif model_type == 'vit_base':\n",
    "        model = LightweightViTForAction(num_classes, 'vit_base_patch16_224')\n",
    "        print(f'Created ViT-Base model ({sum(p.numel() for p in model.parameters()):,} params)')\n",
    "    elif model_type == 'videomae':\n",
    "        model = VideoMAEForVideoClassification.from_pretrained(\n",
    "            'MCG-NJU/videomae-base-finetuned-kinetics',\n",
    "            num_labels=num_classes,\n",
    "            ignore_mismatched_sizes=True,\n",
    "            num_frames=num_frames\n",
    "        )\n",
    "        print(f'Created VideoMAE model ({sum(p.numel() for p in model.parameters()):,} params)')\n",
    "    else:\n",
    "        raise ValueError(f'Unknown model_type: {model_type}')\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "print('Model factory defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    \"\"\"Training dataset with toggle for consistent spatial aug.\"\"\"\n",
    "    def __init__(self, root, num_frames=16, consistent_aug=True):\n",
    "        self.root = Path(root)\n",
    "        self.num_frames = num_frames\n",
    "        self.consistent_aug = consistent_aug\n",
    "        \n",
    "        self.classes = sorted([d.name for d in self.root.iterdir() if d.is_dir()])\n",
    "        self.class_to_idx = {c: i for i, c in enumerate(self.classes)}\n",
    "        \n",
    "        self.samples = []\n",
    "        for cls in self.classes:\n",
    "            for vid in (self.root / cls).iterdir():\n",
    "                if vid.is_dir():\n",
    "                    self.samples.append((vid, self.class_to_idx[cls]))\n",
    "        print(f'Loaded {len(self.samples)} videos, {len(self.classes)} classes')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        vid_dir, label = self.samples[idx]\n",
    "        files = sorted(vid_dir.glob('*.jpg'))\n",
    "        indices = torch.linspace(0, len(files)-1, self.num_frames).long()\n",
    "        \n",
    "        frames = [Image.open(files[i]).convert('RGB') for i in indices]\n",
    "        frames = [TF.resize(img, RESIZE_SIZE) for img in frames]\n",
    "        \n",
    "        if self.consistent_aug:\n",
    "            # Same crop/flip for all frames\n",
    "            i, j, h, w = T.RandomResizedCrop.get_params(frames[0], (0.8, 1.0), (0.75, 1.33))\n",
    "            do_flip = random.random() > 0.5\n",
    "            processed = []\n",
    "            for img in frames:\n",
    "                img = TF.resized_crop(img, i, j, h, w, (IMG_SIZE, IMG_SIZE))\n",
    "                if do_flip:\n",
    "                    img = TF.hflip(img)\n",
    "                img = TF.normalize(TF.to_tensor(img), MEAN, STD)\n",
    "                processed.append(img)\n",
    "        else:\n",
    "            processed = [TF.normalize(TF.to_tensor(TF.center_crop(img, IMG_SIZE)), MEAN, STD) for img in frames]\n",
    "        \n",
    "        return torch.stack(processed), label\n",
    "\n",
    "\n",
    "class TestDatasetSingle(Dataset):\n",
    "    \"\"\"Test with single center crop.\"\"\"\n",
    "    def __init__(self, root, num_frames=16):\n",
    "        self.root = Path(root)\n",
    "        self.num_frames = num_frames\n",
    "        self.samples = [(d, int(d.name)) for d in self.root.iterdir() if d.is_dir()]\n",
    "        self.samples.sort(key=lambda x: x[1])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        vid_dir, vid_id = self.samples[idx]\n",
    "        files = sorted(vid_dir.glob('*.jpg'))\n",
    "        indices = torch.linspace(0, len(files)-1, self.num_frames).long()\n",
    "        frames = []\n",
    "        for i in indices:\n",
    "            img = Image.open(files[i]).convert('RGB')\n",
    "            img = TF.resize(img, RESIZE_SIZE)\n",
    "            img = TF.center_crop(img, IMG_SIZE)\n",
    "            img = TF.normalize(TF.to_tensor(img), MEAN, STD)\n",
    "            frames.append(img)\n",
    "        return torch.stack(frames), vid_id\n",
    "\n",
    "\n",
    "class TestDatasetFlipTTA(Dataset):\n",
    "    \"\"\"6-view TTA: 3 spatial crops × 2 flip states.\"\"\"\n",
    "    def __init__(self, root, num_frames=16):\n",
    "        self.root = Path(root)\n",
    "        self.num_frames = num_frames\n",
    "        self.samples = [(d, int(d.name)) for d in self.root.iterdir() if d.is_dir()]\n",
    "        self.samples.sort(key=lambda x: x[1])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        vid_dir, vid_id = self.samples[idx]\n",
    "        files = sorted(vid_dir.glob('*.jpg'))\n",
    "        indices = torch.linspace(0, len(files)-1, self.num_frames).long()\n",
    "        \n",
    "        frames = [Image.open(files[i]).convert('RGB') for i in indices]\n",
    "        frames = [TF.resize(img, RESIZE_SIZE) for img in frames]\n",
    "        \n",
    "        w, h = frames[0].size\n",
    "        views = []\n",
    "        \n",
    "        # 3 spatial crops\n",
    "        crop_positions = [\n",
    "            ((h - IMG_SIZE) // 2, (w - IMG_SIZE) // 2),  # center\n",
    "            (0, 0),  # top-left\n",
    "            (max(0, h - IMG_SIZE), max(0, w - IMG_SIZE))  # bottom-right\n",
    "        ]\n",
    "        \n",
    "        for top, left in crop_positions:\n",
    "            view_frames = []\n",
    "            for img in frames:\n",
    "                cropped = TF.crop(img, top, left, IMG_SIZE, IMG_SIZE)\n",
    "                view_frames.append(TF.normalize(TF.to_tensor(cropped), MEAN, STD))\n",
    "            views.append(torch.stack(view_frames))\n",
    "            \n",
    "            # Flipped version\n",
    "            view_frames_flip = []\n",
    "            for img in frames:\n",
    "                cropped = TF.crop(img, top, left, IMG_SIZE, IMG_SIZE)\n",
    "                cropped = TF.hflip(cropped)\n",
    "                view_frames_flip.append(TF.normalize(TF.to_tensor(cropped), MEAN, STD))\n",
    "            views.append(torch.stack(view_frames_flip))\n",
    "        \n",
    "        return torch.stack(views), vid_id  # [6, T, C, H, W]\n",
    "\n",
    "print('Dataset classes defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Augmentation Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixupCollate:\n",
    "    \"\"\"Collate function with Mixup augmentation.\"\"\"\n",
    "    def __init__(self, num_classes, alpha=0.8, prob=1.0):\n",
    "        self.num_classes = num_classes\n",
    "        self.alpha = alpha\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        inputs, targets = torch.utils.data.default_collate(batch)\n",
    "        if np.random.rand() > self.prob:\n",
    "            return inputs, F.one_hot(targets, num_classes=self.num_classes).float()\n",
    "        batch_size = inputs.size(0)\n",
    "        index = torch.randperm(batch_size)\n",
    "        lam = np.random.beta(self.alpha, self.alpha)\n",
    "        inputs = lam * inputs + (1 - lam) * inputs[index, :]\n",
    "        targets_one_hot = F.one_hot(targets, num_classes=self.num_classes).float()\n",
    "        targets = lam * targets_one_hot + (1 - lam) * targets_one_hot[index, :]\n",
    "        return inputs, targets\n",
    "\n",
    "\n",
    "def focal_loss(logits, targets, alpha=0.25, gamma=2.0):\n",
    "    \"\"\"Focal Loss for imbalanced data.\"\"\"\n",
    "    ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "    pt = torch.exp(-ce_loss)\n",
    "    focal = alpha * (1 - pt) ** gamma * ce_loss\n",
    "    return focal.mean()\n",
    "\n",
    "print('Augmentation utilities defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, scheduler, scaler, \n",
    "                use_mixup=False, use_focal=False, label_smoothing=0.0,\n",
    "                warmup_mode=False, warmup_batches=5):\n",
    "    \"\"\"Train one epoch with toggle support.\"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "    pbar = tqdm(loader, desc='Training', leave=False)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for step, (inputs, targets) in enumerate(pbar):\n",
    "        if warmup_mode and step >= warmup_batches:\n",
    "            break\n",
    "            \n",
    "        inputs = inputs.to(DEVICE)\n",
    "        \n",
    "        with torch.amp.autocast('cuda'):\n",
    "            # Get logits (handle VideoMAE output)\n",
    "            output = model(inputs)\n",
    "            logits = output.logits if hasattr(output, 'logits') else output\n",
    "            \n",
    "            # Compute loss based on toggles\n",
    "            if use_mixup:\n",
    "                targets = targets.to(DEVICE)\n",
    "                log_probs = F.log_softmax(logits, dim=1)\n",
    "                loss = -torch.sum(targets * log_probs, dim=1).mean()\n",
    "                true_labels = targets.argmax(dim=1)\n",
    "            elif use_focal:\n",
    "                targets = targets.to(DEVICE)\n",
    "                loss = focal_loss(logits, targets)\n",
    "                true_labels = targets\n",
    "            else:\n",
    "                targets = targets.to(DEVICE)\n",
    "                loss = F.cross_entropy(logits, targets, label_smoothing=label_smoothing)\n",
    "                true_labels = targets\n",
    "        \n",
    "        total_correct += (logits.argmax(1) == true_labels).sum().item()\n",
    "        total_samples += inputs.size(0)\n",
    "        \n",
    "        scaler.scale(loss / ACCUM_STEPS).backward()\n",
    "        \n",
    "        if (step + 1) % ACCUM_STEPS == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{total_loss/(step+1):.4f}', 'acc': f'{total_correct/max(total_samples,1):.4f}'})\n",
    "    \n",
    "    return total_loss / max(step+1, 1), total_correct / max(total_samples, 1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, classes, gt_dict, use_tta=False, warmup_mode=False, warmup_batches=5):\n",
    "    \"\"\"Evaluate model on test set.\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    for step, (videos, video_ids) in enumerate(tqdm(loader, desc='Evaluating', leave=False)):\n",
    "        if warmup_mode and step >= warmup_batches:\n",
    "            break\n",
    "            \n",
    "        if use_tta:\n",
    "            # videos: [B, 6, T, C, H, W]\n",
    "            B, V, T, C, H, W = videos.shape\n",
    "            videos = videos.view(B * V, T, C, H, W).to(DEVICE)\n",
    "            output = model(videos)\n",
    "            logits = output.logits if hasattr(output, 'logits') else output\n",
    "            logits = logits.view(B, V, -1).mean(dim=1)\n",
    "            preds = logits.argmax(1).cpu().tolist()\n",
    "        else:\n",
    "            videos = videos.to(DEVICE)\n",
    "            output = model(videos)\n",
    "            logits = output.logits if hasattr(output, 'logits') else output\n",
    "            preds = logits.argmax(1).cpu().tolist()\n",
    "        \n",
    "        predictions.extend(zip(video_ids.tolist(), preds))\n",
    "    \n",
    "    if warmup_mode:\n",
    "        return 0.0  # Skip accuracy calc in warmup\n",
    "    \n",
    "    if gt_dict is None:\n",
    "        return None  # No test labels available\n",
    "    \n",
    "    y_true = [gt_dict[str(vid)] for vid, _ in predictions]\n",
    "    y_pred = [classes[p] for _, p in predictions]\n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_val(model, loader):\n",
    "    \"\"\"Evaluate model on validation set (returns accuracy).\"\"\"\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for videos, labels in tqdm(loader, desc='Val Eval', leave=False):\n",
    "        videos, labels = videos.to(DEVICE), labels.to(DEVICE)\n",
    "        output = model(videos)\n",
    "        logits = output.logits if hasattr(output, 'logits') else output\n",
    "        correct += (logits.argmax(1) == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "print('Training functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. WARMUP Mode (Test All Pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_warmup_test():\n",
    "    \"\"\"Test ALL models to ensure pipeline works.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"WARMUP MODE: Testing all model pipelines\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Small dataset for testing\n",
    "    train_ds = VideoDataset(PATH_DATA_TRAIN, NUM_FRAMES, USE_CONSISTENT_SPATIAL_AUG)\n",
    "    test_ds = TestDatasetSingle(PATH_DATA_TEST, NUM_FRAMES)\n",
    "    \n",
    "    # WARMUP uses num_workers=0 to avoid multiprocessing warnings\n",
    "    train_loader = DataLoader(train_ds, BATCH_SIZE, shuffle=True, num_workers=0, drop_last=True)\n",
    "    test_loader = DataLoader(test_ds, BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Ground truth\n",
    "    !gdown \"1Xv2CWOqdBj3kt0rkNJKRsodSIEd3-wX_\" -O test_labels.csv -q\n",
    "    gt_df = pd.read_csv('test_labels.csv')\n",
    "    gt_dict = dict(zip(gt_df['id'].astype(str), gt_df['class']))\n",
    "    \n",
    "    WARMUP_MODELS = ['vit_small', 'vit_base', 'videomae']\n",
    "    \n",
    "    for model_type in WARMUP_MODELS:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"WARMUP: Testing {model_type}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        try:\n",
    "            model = create_model(model_type, len(full_ds.classes), NUM_FRAMES)\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=LR_P1, weight_decay=WEIGHT_DECAY)\n",
    "            scaler = torch.amp.GradScaler()\n",
    "            \n",
    "            # Phase 1 - NO mixup in warmup (just test forward/backward pass)\n",
    "            loss, acc = train_epoch(model, train_loader, optimizer, None, scaler,\n",
    "                                   use_mixup=False, warmup_mode=True, warmup_batches=WARMUP_BATCHES)\n",
    "            print(f\"✓ Phase 1 OK (loss={loss:.4f}, acc={acc:.4f})\")\n",
    "            \n",
    "            # Phase 2 - test label smoothing\n",
    "            loss, acc = train_epoch(model, train_loader, optimizer, None, scaler,\n",
    "                                   label_smoothing=LABEL_SMOOTHING_EPS, warmup_mode=True, warmup_batches=WARMUP_BATCHES)\n",
    "            print(f\"✓ Phase 2 OK (loss={loss:.4f}, acc={acc:.4f})\")\n",
    "            \n",
    "            # Eval\n",
    "            evaluate(model, test_loader, full_ds.classes, gt_dict, warmup_mode=True, warmup_batches=WARMUP_BATCHES)\n",
    "            print(f\"✓ Eval OK\")\n",
    "            \n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ FAILED: {e}\")\n",
    "            raise\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✅ WARMUP COMPLETE - All models pipeline OK!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "if WARMUP:\n",
    "    run_warmup_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Main Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not WARMUP:\n",
    "    # Load full dataset\n",
    "    full_ds = VideoDataset(PATH_DATA_TRAIN, NUM_FRAMES, USE_CONSISTENT_SPATIAL_AUG)\n",
    "    \n",
    "    # Split train/val if ratio < 1.0\n",
    "    if TRAIN_VAL_RATIO < 1.0:\n",
    "        train_size = int(len(full_ds) * TRAIN_VAL_RATIO)\n",
    "        val_size = len(full_ds) - train_size\n",
    "        train_ds, val_ds = torch.utils.data.random_split(full_ds, [train_size, val_size])\n",
    "        print(f'Split: {train_size} train, {val_size} val')\n",
    "    else:\n",
    "        train_ds = full_ds\n",
    "        val_ds = None\n",
    "        print('Using full dataset for training (no validation split)')\n",
    "    \n",
    "    if USE_FLIP_TTA:\n",
    "        test_ds = TestDatasetFlipTTA(PATH_DATA_TEST, NUM_FRAMES)\n",
    "    else:\n",
    "        test_ds = TestDatasetSingle(PATH_DATA_TEST, NUM_FRAMES)\n",
    "    \n",
    "    # DataLoaders\n",
    "    if USE_MIXUP:\n",
    "        mixup_collate = MixupCollate(len(full_ds.classes), MIXUP_ALPHA)\n",
    "        train_loader = DataLoader(train_ds, BATCH_SIZE, shuffle=True, num_workers=2, \n",
    "                                  collate_fn=mixup_collate, drop_last=True)\n",
    "    else:\n",
    "        train_loader = DataLoader(train_ds, BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True)\n",
    "    \n",
    "    if val_ds:\n",
    "        val_loader = DataLoader(val_ds, BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True, persistent_workers=True)\n",
    "    \n",
    "    test_loader = DataLoader(test_ds, BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True, persistent_workers=True)\n",
    "    \n",
    "    # Ground truth for test accuracy (optional)\n",
    "    gt_dict = None\n",
    "    if USE_TEST_LABELS:\n",
    "        !gdown \"{TEST_LABELS_URL}\" -O test_labels.csv -q\n",
    "        gt_df = pd.read_csv('test_labels.csv')\n",
    "        gt_dict = dict(zip(gt_df['id'].astype(str), gt_df['class']))\n",
    "        print(f'Loaded test labels: {len(gt_dict)} samples')\n",
    "    else:\n",
    "        print('Test labels disabled - test accuracy will not be calculated')\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model(MODEL_TYPE, len(full_ds.classes), NUM_FRAMES)\n",
    "    \n",
    "    print(f'\\nTrain samples: {len(train_ds)}')\n",
    "    if val_ds:\n",
    "        print(f'Val samples: {len(val_ds)}')\n",
    "    print(f'Test samples: {len(test_ds)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not WARMUP:\n",
    "    # Visualize original class distribution\n",
    "    print('\\\\n' + '='*60)\n",
    "    print('ORIGINAL CLASS DISTRIBUTION')\n",
    "    print('='*60)\n",
    "    \n",
    "    class_counts = Counter([full_ds.samples[i][1] for i in range(len(full_ds.samples))])\n",
    "    classes_names = full_ds.classes\n",
    "    counts = [class_counts.get(i, 0) for i in range(len(classes_names))]\n",
    "    \n",
    "    # Sort by count\n",
    "    sorted_pairs = sorted(zip(classes_names, counts), key=lambda x: x[1], reverse=True)\n",
    "    sorted_names = [p[0] for p in sorted_pairs]\n",
    "    sorted_counts = [p[1] for p in sorted_pairs]\n",
    "    \n",
    "    plt.figure(figsize=(16, 6))\n",
    "    colors = ['darkgreen' if c >= np.mean(counts) else 'orange' if c >= np.mean(counts)*0.5 else 'darkred' for c in sorted_counts]\n",
    "    plt.bar(range(len(sorted_names)), sorted_counts, color=colors, alpha=0.7)\n",
    "    plt.axhline(y=np.mean(counts), color='blue', linestyle='--', linewidth=2, label=f'Mean ({np.mean(counts):.1f})')\n",
    "    plt.xlabel('Action Category', fontsize=12)\n",
    "    plt.ylabel('Sample Count', fontsize=12)\n",
    "    plt.title('Original Training Data Distribution (Before Augmentation)', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(range(len(sorted_names)), sorted_names, rotation=90, ha='right', fontsize=8)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('original_distribution.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'Total samples: {sum(counts)}')\n",
    "    print(f'Min class: {sorted_names[-1]} ({sorted_counts[-1]} samples)')\n",
    "    print(f'Max class: {sorted_names[0]} ({sorted_counts[0]} samples)')\n",
    "    print(f'Imbalance ratio: {sorted_counts[0]/sorted_counts[-1]:.2f}x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not WARMUP and USE_DATA_BALANCE:\n",
    "    # Visualize balanced class distribution after augmentation\n",
    "    print('\\\\n' + '='*60)\n",
    "    print('BALANCED CLASS DISTRIBUTION (After Augmentation)')\n",
    "    print('='*60)\n",
    "    \n",
    "    # Get balanced counts (after augmentation)\n",
    "    balanced_class_counts = Counter([full_ds.samples[i][1] for i in range(len(full_ds.samples))])\n",
    "    balanced_counts = [balanced_class_counts.get(i, 0) for i in range(len(classes_names))]\n",
    "    \n",
    "    # Sort by original order for comparison\n",
    "    sorted_balanced = [balanced_class_counts.get(full_ds.class_to_idx[n], 0) for n in sorted_names]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    \n",
    "    # Before\n",
    "    axes[0].bar(range(len(sorted_names)), sorted_counts, color='coral', alpha=0.7)\n",
    "    axes[0].axhline(y=np.mean(sorted_counts), color='blue', linestyle='--', linewidth=2)\n",
    "    axes[0].set_title('Before Balance Augmentation', fontweight='bold')\n",
    "    axes[0].set_xlabel('Class')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_xticks(range(len(sorted_names)))\n",
    "    axes[0].set_xticklabels(sorted_names, rotation=90, fontsize=6)\n",
    "    \n",
    "    # After\n",
    "    axes[1].bar(range(len(sorted_names)), sorted_balanced, color='steelblue', alpha=0.7)\n",
    "    axes[1].axhline(y=np.mean(sorted_balanced), color='blue', linestyle='--', linewidth=2)\n",
    "    axes[1].set_title('After Balance Augmentation', fontweight='bold')\n",
    "    axes[1].set_xlabel('Class')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].set_xticks(range(len(sorted_names)))\n",
    "    axes[1].set_xticklabels(sorted_names, rotation=90, fontsize=6)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('balanced_distribution.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'Balanced total samples: {sum(sorted_balanced)}')\n",
    "    print(f'New imbalance ratio: {max(sorted_balanced)/max(1, min(sorted_balanced)):.2f}x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not WARMUP:\n",
    "    history = []\n",
    "    best_acc = 0.0\n",
    "    scaler = torch.amp.GradScaler()\n",
    "    \n",
    "    # PHASE 1\n",
    "    print('\\n' + '='*50)\n",
    "    print(f'PHASE 1: Epochs={EPOCHS_P1}, LR={LR_P1}')\n",
    "    if USE_MIXUP:\n",
    "        print('  Mixup: ON')\n",
    "    if USE_FOCAL_LOSS:\n",
    "        print('  Focal Loss: ON')\n",
    "    print('='*50)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR_P1, weight_decay=WEIGHT_DECAY)\n",
    "    total_steps = len(train_loader) * EPOCHS_P1 // ACCUM_STEPS\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, int(total_steps * WARMUP_RATIO), total_steps)\n",
    "    \n",
    "    for epoch in range(1, EPOCHS_P1 + 1):\n",
    "        loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler, scaler,\n",
    "                                      use_mixup=USE_MIXUP, use_focal=USE_FOCAL_LOSS)\n",
    "        \n",
    "        # Evaluate val if available\n",
    "        val_acc = evaluate_val(model, val_loader) if val_ds else None\n",
    "        # Evaluate test if labels available\n",
    "        test_acc = evaluate(model, test_loader, full_ds.classes, gt_dict, use_tta=USE_FLIP_TTA)\n",
    "        \n",
    "        history.append({'epoch': epoch, 'phase': 1, 'loss': loss, 'train_acc': train_acc, 'val_acc': val_acc, 'test_acc': test_acc})\n",
    "        \n",
    "        # Determine best metric (test_acc preferred, else val_acc)\n",
    "        current_metric = test_acc if test_acc is not None else val_acc\n",
    "        status = ''\n",
    "        if current_metric is not None and current_metric > best_acc:\n",
    "            best_acc = current_metric\n",
    "            torch.save(model.state_dict(), 'best_p1.pt')\n",
    "            status = '>>> BEST'\n",
    "        elif current_metric is None:\n",
    "            torch.save(model.state_dict(), 'best_p1.pt')\n",
    "        \n",
    "        # Build log message\n",
    "        msg = f'Ep {epoch}/{EPOCHS_P1}: L={loss:.4f} TrAcc={train_acc:.4f}'\n",
    "        if val_acc is not None:\n",
    "            msg += f' ValAcc={val_acc:.4f}'\n",
    "        if test_acc is not None:\n",
    "            msg += f' TestAcc={test_acc:.4f}'\n",
    "        print(f'{msg} {status}')\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not WARMUP and USE_TWO_PHASE and EPOCHS_P2 > 0:\n",
    "    # PHASE 2\n",
    "    print('\\n' + '='*50)\n",
    "    print(f'PHASE 2: Epochs={EPOCHS_P2}, LR={LR_P2}')\n",
    "    print(f'  Label Smoothing: {LABEL_SMOOTHING_EPS}')\n",
    "    print('='*50)\n",
    "    \n",
    "    model.load_state_dict(torch.load('best_p1.pt'))\n",
    "    \n",
    "    # New loader without mixup\n",
    "    train_loader_p2 = DataLoader(train_ds, BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR_P2, weight_decay=WEIGHT_DECAY)\n",
    "    total_steps = len(train_loader_p2) * EPOCHS_P2 // ACCUM_STEPS\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, int(total_steps * WARMUP_RATIO), total_steps)\n",
    "    scaler = torch.amp.GradScaler()\n",
    "    \n",
    "    for epoch in range(1, EPOCHS_P2 + 1):\n",
    "        loss, train_acc = train_epoch(model, train_loader_p2, optimizer, scheduler, scaler,\n",
    "                                      label_smoothing=LABEL_SMOOTHING_EPS)\n",
    "        \n",
    "        # Evaluate val if available\n",
    "        val_acc = evaluate_val(model, val_loader) if val_ds else None\n",
    "        # Evaluate test if labels available\n",
    "        test_acc = evaluate(model, test_loader, full_ds.classes, gt_dict, use_tta=USE_FLIP_TTA)\n",
    "        \n",
    "        history.append({'epoch': EPOCHS_P1 + epoch, 'phase': 2, 'loss': loss, 'train_acc': train_acc, 'val_acc': val_acc, 'test_acc': test_acc})\n",
    "        \n",
    "        # Determine best metric (test_acc preferred, else val_acc)\n",
    "        current_metric = test_acc if test_acc is not None else val_acc\n",
    "        status = ''\n",
    "        if current_metric is not None and current_metric > best_acc:\n",
    "            best_acc = current_metric\n",
    "            torch.save(model.state_dict(), 'best_final.pt')\n",
    "            status = '>>> BEST'\n",
    "        elif current_metric is None:\n",
    "            torch.save(model.state_dict(), 'best_final.pt')\n",
    "        \n",
    "        # Build log message\n",
    "        msg = f'P2 Ep {epoch}/{EPOCHS_P2}: L={loss:.4f} TrAcc={train_acc:.4f}'\n",
    "        if val_acc is not None:\n",
    "            msg += f' ValAcc={val_acc:.4f}'\n",
    "        if test_acc is not None:\n",
    "            msg += f' TestAcc={test_acc:.4f}'\n",
    "        print(f'{msg} {status}')\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Save history\n",
    "    df_history = pd.DataFrame(history)\n",
    "    df_history.to_csv('training_history.csv', index=False)\n",
    "    if best_acc > 0:\n",
    "        print(f'\\nTraining Complete! Best Acc: {best_acc:.4f}')\n",
    "    else:\n",
    "        print(f'\\nTraining Complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not WARMUP:\n",
    "    # Training curves\n",
    "    df = pd.DataFrame(history)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    axes[0].plot(df['epoch'], df['test_acc'], 'b-o', markersize=4)\n",
    "    if USE_TWO_PHASE:\n",
    "        axes[0].axvline(x=EPOCHS_P1, color='gray', linestyle='--', alpha=0.5)\n",
    "    axes[0].set_title('Test Accuracy')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].plot(df['epoch'], df['train_acc'], 'g-s', markersize=4)\n",
    "    if USE_TWO_PHASE:\n",
    "        axes[1].axvline(x=EPOCHS_P1, color='gray', linestyle='--', alpha=0.5)\n",
    "    axes[1].set_title('Train Accuracy')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[2].plot(df['epoch'], df['loss'], 'r-^', markersize=4)\n",
    "    if USE_TWO_PHASE:\n",
    "        axes[2].axvline(x=EPOCHS_P1, color='gray', linestyle='--', alpha=0.5)\n",
    "    axes[2].set_title('Loss')\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_curves.png', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not WARMUP:\n",
    "    # Load best model\n",
    "    import os\n",
    "    if os.path.exists('best_final.pt'):\n",
    "        model.load_state_dict(torch.load('best_final.pt'))\n",
    "    elif os.path.exists('best_p1.pt'):\n",
    "        model.load_state_dict(torch.load('best_p1.pt'))\n",
    "    model.eval()\n",
    "    \n",
    "    # Get all predictions\n",
    "    all_preds, all_true = [], []\n",
    "    simple_test_ds = TestDatasetSingle(PATH_DATA_TEST, NUM_FRAMES)\n",
    "    simple_loader = DataLoader(simple_test_ds, BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True, persistent_workers=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for videos, video_ids in tqdm(simple_loader, desc='Final Eval'):\n",
    "            videos = videos.to(DEVICE)\n",
    "            output = model(videos)\n",
    "            logits = output.logits if hasattr(output, 'logits') else output\n",
    "            preds = logits.argmax(1).cpu().tolist()\n",
    "            for vid, pred in zip(video_ids.tolist(), preds):\n",
    "                all_true.append(gt_dict[str(vid)])\n",
    "                all_preds.append(full_ds.classes[pred])\n",
    "    \n",
    "    overall_acc = accuracy_score(all_true, all_preds)\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'OVERALL TEST ACCURACY: {overall_acc:.4f} ({overall_acc*100:.2f}%)')\n",
    "    print(f'{\"=\"*60}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not WARMUP:\n",
    "    # Per-class accuracy\n",
    "    report_dict = classification_report(all_true, all_preds, target_names=full_ds.classes, \n",
    "                                        output_dict=True, zero_division=0)\n",
    "    \n",
    "    class_accs = [(cls, report_dict[cls]['recall'] * 100) for cls in full_ds.classes]\n",
    "    class_accs_sorted = sorted(class_accs, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    cls_names = [c[0] for c in class_accs_sorted]\n",
    "    cls_accs_vals = [c[1] for c in class_accs_sorted]\n",
    "    \n",
    "    plt.figure(figsize=(16, 6))\n",
    "    colors = ['darkgreen' if acc >= 90 else 'orange' if acc >= 70 else 'darkred' for acc in cls_accs_vals]\n",
    "    plt.bar(range(len(cls_names)), cls_accs_vals, color=colors, alpha=0.7)\n",
    "    plt.axhline(y=overall_acc*100, color='blue', linestyle='--', linewidth=2, label=f'Overall ({overall_acc*100:.2f}%)')\n",
    "    plt.xlabel('Action Category', fontsize=12)\n",
    "    plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "    plt.title('Per-Class Test Accuracy', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(range(len(cls_names)), cls_names, rotation=90, ha='right', fontsize=8)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('per_class_accuracy.png', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not WARMUP:\n",
    "    # Prediction distribution vs Ground truth\n",
    "    gt_counts = Counter(all_true)\n",
    "    pred_counts = Counter(all_preds)\n",
    "    \n",
    "    classes_sorted = sorted(full_ds.classes)\n",
    "    gt_vals = [gt_counts.get(c, 0) for c in classes_sorted]\n",
    "    pred_vals = [pred_counts.get(c, 0) for c in classes_sorted]\n",
    "    \n",
    "    x = np.arange(len(classes_sorted))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(16, 6))\n",
    "    ax.bar(x - width/2, gt_vals, width, label='Ground Truth', alpha=0.7, color='steelblue')\n",
    "    ax.bar(x + width/2, pred_vals, width, label='Predictions', alpha=0.7, color='coral')\n",
    "    \n",
    "    ax.set_xlabel('Action Category')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Prediction Distribution vs Ground Truth (detect model bias)', fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(classes_sorted, rotation=90, fontsize=7)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('prediction_distribution.png', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not WARMUP:\n",
    "    # Top confusion pairs\n",
    "    cm = confusion_matrix(all_true, all_preds, labels=full_ds.classes)\n",
    "    \n",
    "    confusions = []\n",
    "    for i in range(len(full_ds.classes)):\n",
    "        for j in range(len(full_ds.classes)):\n",
    "            if i != j and cm[i, j] > 0:\n",
    "                confusions.append((full_ds.classes[i], full_ds.classes[j], cm[i, j]))\n",
    "    \n",
    "    confusions.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    print('\\n' + '='*70)\n",
    "    print('TOP-15 CONFUSION PAIRS (True → Predicted)')\n",
    "    print('='*70)\n",
    "    for i, (true_cls, pred_cls, count) in enumerate(confusions[:15]):\n",
    "        print(f'{i+1:2d}. {true_cls:20s} → {pred_cls:20s} ({int(count):2d} errors)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not WARMUP:\n",
    "    print('\\n' + '='*70)\n",
    "    print('FINAL SUMMARY')\n",
    "    print('='*70)\n",
    "    print(f'Model: {MODEL_TYPE}')\n",
    "    print(f'Best Test Accuracy: {best_acc:.4f} ({best_acc*100:.2f}%)')\n",
    "    print(f'\\nConfiguration:')\n",
    "    print(f'  Consistent Spatial Aug: {USE_CONSISTENT_SPATIAL_AUG}')\n",
    "    print(f'  Mixup: {USE_MIXUP}')\n",
    "    print(f'  Label Smoothing: {USE_LABEL_SMOOTHING}')\n",
    "    print(f'  Two Phase: {USE_TWO_PHASE}')\n",
    "    print(f'  Flip TTA: {USE_FLIP_TTA}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}