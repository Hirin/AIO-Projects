{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":125907,"databundleVersionId":14910023,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# VideoMAE Ablation Study - Dual GPU Workers\n\n**Flow:**\n1. Write 2 worker scripts (GPU 0, GPU 1)\n2. Run both workers in parallel\n3. Merge results and plot\n\n**10 Experiments (5 per GPU):**\n- GPU 0: Exp0, 2, 4, 6, 8\n- GPU 1: Exp1, 1b, 3, 5, 7","metadata":{}},{"cell_type":"code","source":"## 1. Download Test Labels\n!gdown \"1Xv2CWOqdBj3kt0rkNJKRsodSIEd3-wX_\" -O test_labels.csv -q\nprint(\"Downloaded test_labels.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T17:11:07.510558Z","iopub.execute_input":"2026-01-17T17:11:07.510861Z","iopub.status.idle":"2026-01-17T17:11:09.914447Z","shell.execute_reply.started":"2026-01-17T17:11:07.510836Z","shell.execute_reply":"2026-01-17T17:11:09.913523Z"}},"outputs":[{"name":"stdout","text":"Downloaded test_labels.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%writefile worker_gpu0.py\n#!/usr/bin/env python3\n\"\"\"Worker GPU 0 - 5 experiments\"\"\"\nimport os\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nimport warnings\nwarnings.filterwarnings('ignore')\nimport logging\nlogging.getLogger('transformers').setLevel(logging.ERROR)\nlogging.getLogger('tensorflow').setLevel(logging.ERROR)\n\nimport random, numpy as np, torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom pathlib import Path\nfrom PIL import Image\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF\nfrom transformers import VideoMAEForVideoClassification, VideoMAEImageProcessor, get_cosine_schedule_with_warmup\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nimport timm\n\n# Config\nPATH_TRAIN = Path('/kaggle/input/action-video/data/data_train')\nPATH_TEST = Path('/kaggle/input/action-video/data/test')\nMODEL_CKPT = \"MCG-NJU/videomae-base-finetuned-kinetics\"\nNUM_FRAMES, IMG_SIZE, RESIZE = 16, 224, 256\nBATCH, ACCUM, EPOCHS, LR, WD = 20, 2, 10, 5e-5, 0.05\ngpu_id = 0\ndevice = torch.device('cuda:0')\nproc = VideoMAEImageProcessor.from_pretrained(MODEL_CKPT)\nMEAN, STD = proc.image_mean, proc.image_std\n\nclass ViT(nn.Module):\n    def __init__(self, nc):\n        super().__init__()\n        self.vit = timm.create_model('vit_small_patch16_224', pretrained=True, num_classes=0)\n        self.head = nn.Linear(self.vit.num_features, nc)\n    def forward(self, x):\n        B,T,C,H,W = x.shape\n        return self.head(self.vit(x.view(B*T,C,H,W)).view(B,T,-1).mean(1))\n\nclass DS(Dataset):\n    def __init__(self, root, con=False):\n        self.root, self.con = Path(root), con\n        self.classes = sorted([d.name for d in self.root.iterdir() if d.is_dir()])\n        self.c2i = {c:i for i,c in enumerate(self.classes)}\n        self.samples = [(v, self.c2i[c]) for c in self.classes for v in (self.root/c).iterdir() if v.is_dir()]\n    def __len__(self): return len(self.samples)\n    def __getitem__(self, i):\n        v, l = self.samples[i]\n        fs = sorted(v.glob('*.jpg'))\n        idx = torch.linspace(0, len(fs)-1, NUM_FRAMES).long()\n        if self.con:\n            fr = [TF.resize(Image.open(fs[j]).convert('RGB'), RESIZE) for j in idx]\n            i,j,h,w = T.RandomResizedCrop.get_params(fr[0], (0.8,1.0), (0.75,1.33))\n            fl = random.random() > 0.5\n            return torch.stack([TF.normalize(TF.to_tensor(TF.hflip(TF.resized_crop(f,i,j,h,w,(IMG_SIZE,IMG_SIZE))) if fl else TF.resized_crop(f,i,j,h,w,(IMG_SIZE,IMG_SIZE))), MEAN, STD) for f in fr]), l\n        fr = torch.stack([TF.to_tensor(Image.open(fs[j]).convert('RGB')) for j in idx])\n        s = random.uniform(0.8,1.0)\n        fr = TF.resize(fr, [int(fr.shape[-2]*s), int(fr.shape[-1]*s)])\n        i,j = random.randint(0,max(0,fr.shape[-2]-IMG_SIZE)), random.randint(0,max(0,fr.shape[-1]-IMG_SIZE))\n        fr = TF.resize(TF.crop(fr, i, j, min(IMG_SIZE,fr.shape[-2]), min(IMG_SIZE,fr.shape[-1])), [IMG_SIZE,IMG_SIZE])\n        if random.random()<0.5: fr = TF.hflip(fr)\n        return torch.stack([TF.normalize(f, MEAN, STD) for f in fr]), l\n\nclass TDS(Dataset):\n    def __init__(self, root, tta=False):\n        self.root, self.tta = Path(root), tta\n        self.samples = sorted([(d,int(d.name)) for d in self.root.iterdir() if d.is_dir()], key=lambda x:x[1])\n    def __len__(self): return len(self.samples)\n    def __getitem__(self, i):\n        v, vid = self.samples[i]\n        fs = sorted(v.glob('*.jpg'))\n        idx = torch.linspace(0, len(fs)-1, NUM_FRAMES).long()\n        fr = [TF.resize(Image.open(fs[j]).convert('RGB'), RESIZE) for j in idx]\n        if self.tta:\n            w,h = fr[0].size\n            views = []\n            for t,lf in [((h-IMG_SIZE)//2,(w-IMG_SIZE)//2), (0,(w-IMG_SIZE)//2), (max(0,h-IMG_SIZE),(w-IMG_SIZE)//2)]:\n                views.append(torch.stack([TF.normalize(TF.to_tensor(TF.crop(f,t,lf,IMG_SIZE,IMG_SIZE)), MEAN, STD) for f in fr]))\n                views.append(torch.stack([TF.normalize(TF.to_tensor(TF.hflip(TF.crop(f,t,lf,IMG_SIZE,IMG_SIZE))), MEAN, STD) for f in fr]))\n            return torch.stack(views), vid\n        return torch.stack([TF.normalize(TF.to_tensor(TF.center_crop(f, IMG_SIZE)), MEAN, STD) for f in fr]), vid\n\nclass Mix:\n    def __init__(self, nc, a=0.8): self.nc, self.a = nc, a\n    def __call__(self, b):\n        x,y = torch.utils.data.default_collate(b)\n        lam = np.random.beta(self.a, self.a)\n        i = torch.randperm(x.size(0))\n        return lam*x + (1-lam)*x[i], lam*F.one_hot(y,self.nc).float() + (1-lam)*F.one_hot(y[i],self.nc).float()\n\ndef train(m, ld, opt, sch, sc, mix=False, ls=0.0, vit=False):\n    m.train()\n    loss_s, cor, tot = 0.0, 0, 0\n    for bi, (x, y) in enumerate(ld):\n        x, y = x.to(device), y.to(device)\n        with torch.amp.autocast('cuda'):\n            lo = m(x) if vit else m(x).logits\n            if mix:\n                loss = -torch.sum(y * F.log_softmax(lo,1), 1).mean()\n                lb = y.argmax(1)\n            else:\n                loss = F.cross_entropy(lo, y, label_smoothing=ls)\n                lb = y\n        cor += (lo.argmax(1)==lb).sum().item()\n        tot += lb.size(0)\n        loss_s += loss.item() * lb.size(0)\n        sc.scale(loss/ACCUM).backward()\n        if (bi+1) % ACCUM == 0:\n            sc.unscale_(opt)\n            torch.nn.utils.clip_grad_norm_(m.parameters(), 1.0)\n            sc.step(opt); sc.update(); opt.zero_grad(); sch.step()\n    return loss_s/tot, cor/tot\n\ndef evalu(m, ld, multi=False, vit=False):\n    m.eval()\n    ps = []\n    with torch.no_grad():\n        for x, ids in ld:\n            if multi:\n                B,V,T,C,H,W = x.shape\n                lo = (m(x.view(B*V,T,C,H,W).to(device)) if vit else m(x.view(B*V,T,C,H,W).to(device)).logits).view(B,V,-1).mean(1)\n            else:\n                lo = m(x.to(device)) if vit else m(x.to(device)).logits\n            ps.extend(zip(ids.tolist(), lo.argmax(1).cpu().tolist()))\n    return ps\n\nEXPS = [\n    {'name': 'Exp0_ViT', 'vit': True},\n    {'name': 'Exp2_Consistent', 'con': True},\n    {'name': 'Exp4_Mixup', 'con': True, 'mix': True},\n    {'name': 'Exp6_2Stage', 'con': True, 'mix': True, 'two': True},\n    {'name': 'Exp8_LR_High', 'con': True, 'mix': True, 'two': True, 'tta': True, 'lr': 1.25e-4},\n]\n\ndef main():\n    gt = dict(zip(pd.read_csv('test_labels.csv')['id'].astype(str), pd.read_csv('test_labels.csv')['class']))\n    res = []\n    for e in EXPS:\n        print(f\"\\n[GPU0] {e['name']}\")\n        random.seed(42); np.random.seed(42); torch.manual_seed(42)\n        lr = e.get('lr', LR)\n        tds = DS(PATH_TRAIN, e.get('con'))\n        tes = TDS(PATH_TEST, e.get('tta'))\n        cn = tds.classes\n        m = ViT(len(cn)).to(device) if e.get('vit') else VideoMAEForVideoClassification.from_pretrained(MODEL_CKPT, num_labels=len(cn), ignore_mismatched_sizes=True, num_frames=NUM_FRAMES).to(device)\n        col = Mix(len(cn)) if e.get('mix') else None\n        tl = DataLoader(tds, BATCH, shuffle=True, num_workers=2, drop_last=True, collate_fn=col)\n        tel = DataLoader(tes, 4 if e.get('tta') else BATCH, num_workers=2)\n        opt = torch.optim.AdamW(m.parameters(), lr=lr, weight_decay=WD)\n        sc = torch.amp.GradScaler()\n        sch = get_cosine_schedule_with_warmup(opt, int(len(tl)*EPOCHS*0.1/ACCUM), len(tl)*EPOCHS//ACCUM)\n        for ep in range(EPOCHS):\n            import time\n            ep_start = time.time()\n            l, a = train(m, tl, opt, sch, sc, e.get('mix'), 0.0, e.get('vit'))\n            ps = evalu(m, tel, e.get('tta'), e.get('vit'))\n            ta = accuracy_score([gt[str(i)] for i,_ in ps], [cn[p] for _,p in ps])\n            ep_time = time.time() - ep_start; eta = ep_time * (EPOCHS - ep - 1); print(f\"  [GPU{gpu_id}] Ep{ep+1}/{EPOCHS}: L={l:.4f}, Atr={a:.4f}, Ate={ta:.4f} | {ep_time//60:.0f}m{ep_time%60:.0f}s | ETA: {eta//60:.0f}m{eta%60:.0f}s\")\n        if e.get('two'):\n            opt = torch.optim.AdamW(m.parameters(), lr=1e-6, weight_decay=WD)\n            p2 = DataLoader(tds, BATCH, shuffle=True, num_workers=2, drop_last=True)\n            sch = get_cosine_schedule_with_warmup(opt, 0, len(p2)*3//ACCUM)\n            for ep in range(3):\n                l,a = train(m, p2, opt, sch, sc, False, 0.1, e.get('vit'))\n                ps = evalu(m, tel, e.get('tta'), e.get('vit'))\n                ta = accuracy_score([gt[str(i)] for i,_ in ps], [cn[p] for _,p in ps])\n                print(f\"  P2Ep{ep+1}: L={l:.4f}, Atr={a:.4f}, Ate={ta:.4f}\")\n        ps = evalu(m, tel, e.get('tta'), e.get('vit'))\n        fa = accuracy_score([gt[str(i)] for i,_ in ps], [cn[p] for _,p in ps])\n        print(f\"  FINAL: {fa:.4f}\")\n        res.append({'exp': e['name'], 'test_acc': fa, 'lr': lr, 'gpu': 0})\n        del m; torch.cuda.empty_cache()\n    pd.DataFrame(res).to_csv('results_gpu0.csv', index=False)\n    print(\"Saved results_gpu0.csv\")\n\nif __name__ == '__main__': main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T17:11:09.915900Z","iopub.execute_input":"2026-01-17T17:11:09.916235Z","iopub.status.idle":"2026-01-17T17:11:09.925633Z","shell.execute_reply.started":"2026-01-17T17:11:09.916208Z","shell.execute_reply":"2026-01-17T17:11:09.924942Z"}},"outputs":[{"name":"stdout","text":"Writing worker_gpu0.py\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%writefile worker_gpu1.py\n#!/usr/bin/env python3\n\"\"\"Worker GPU 1 - 5 experiments\"\"\"\nimport os\nos.environ['CUDA_VISIBLE_DEVICES'] = '1'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nimport warnings\nwarnings.filterwarnings('ignore')\nimport logging\nlogging.getLogger('transformers').setLevel(logging.ERROR)\nlogging.getLogger('tensorflow').setLevel(logging.ERROR)\n\nimport random, numpy as np, torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom pathlib import Path\nfrom PIL import Image\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF\nfrom transformers import VideoMAEForVideoClassification, VideoMAEImageProcessor, get_cosine_schedule_with_warmup\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\n\nPATH_TRAIN = Path('/kaggle/input/action-video/data/data_train')\nPATH_TEST = Path('/kaggle/input/action-video/data/test')\nMODEL_CKPT = \"MCG-NJU/videomae-base-finetuned-kinetics\"\nNUM_FRAMES, IMG_SIZE, RESIZE = 16, 224, 256\nBATCH, ACCUM, EPOCHS, LR, WD = 20, 2, 10, 5e-5, 0.05\ngpu_id = 1\ndevice = torch.device('cuda:0')\nproc = VideoMAEImageProcessor.from_pretrained(MODEL_CKPT)\nMEAN, STD = proc.image_mean, proc.image_std\n\nclass DS(Dataset):\n    def __init__(self, root, con=False):\n        self.root, self.con = Path(root), con\n        self.classes = sorted([d.name for d in self.root.iterdir() if d.is_dir()])\n        self.c2i = {c:i for i,c in enumerate(self.classes)}\n        self.samples = [(v, self.c2i[c]) for c in self.classes for v in (self.root/c).iterdir() if v.is_dir()]\n    def __len__(self): return len(self.samples)\n    def __getitem__(self, i):\n        v, l = self.samples[i]\n        fs = sorted(v.glob('*.jpg'))\n        idx = torch.linspace(0, len(fs)-1, NUM_FRAMES).long()\n        if self.con:\n            fr = [TF.resize(Image.open(fs[j]).convert('RGB'), RESIZE) for j in idx]\n            i,j,h,w = T.RandomResizedCrop.get_params(fr[0], (0.8,1.0), (0.75,1.33))\n            fl = random.random() > 0.5\n            return torch.stack([TF.normalize(TF.to_tensor(TF.hflip(TF.resized_crop(f,i,j,h,w,(IMG_SIZE,IMG_SIZE))) if fl else TF.resized_crop(f,i,j,h,w,(IMG_SIZE,IMG_SIZE))), MEAN, STD) for f in fr]), l\n        fr = torch.stack([TF.to_tensor(Image.open(fs[j]).convert('RGB')) for j in idx])\n        s = random.uniform(0.8,1.0)\n        fr = TF.resize(fr, [int(fr.shape[-2]*s), int(fr.shape[-1]*s)])\n        i,j = random.randint(0,max(0,fr.shape[-2]-IMG_SIZE)), random.randint(0,max(0,fr.shape[-1]-IMG_SIZE))\n        fr = TF.resize(TF.crop(fr, i, j, min(IMG_SIZE,fr.shape[-2]), min(IMG_SIZE,fr.shape[-1])), [IMG_SIZE,IMG_SIZE])\n        if random.random()<0.5: fr = TF.hflip(fr)\n        return torch.stack([TF.normalize(f, MEAN, STD) for f in fr]), l\n\nclass TDS(Dataset):\n    def __init__(self, root, tta=False, mv=False):\n        self.root, self.tta, self.mv = Path(root), tta, mv\n        self.samples = sorted([(d,int(d.name)) for d in self.root.iterdir() if d.is_dir()], key=lambda x:x[1])\n    def __len__(self): return len(self.samples)\n    def __getitem__(self, i):\n        v, vid = self.samples[i]\n        fs = sorted(v.glob('*.jpg'))\n        idx = torch.linspace(0, len(fs)-1, NUM_FRAMES).long()\n        fr = [TF.resize(Image.open(fs[j]).convert('RGB'), RESIZE) for j in idx]\n        if self.tta:\n            w,h = fr[0].size\n            views = []\n            for t,lf in [((h-IMG_SIZE)//2,(w-IMG_SIZE)//2), (0,(w-IMG_SIZE)//2), (max(0,h-IMG_SIZE),(w-IMG_SIZE)//2)]:\n                views.append(torch.stack([TF.normalize(TF.to_tensor(TF.crop(f,t,lf,IMG_SIZE,IMG_SIZE)), MEAN, STD) for f in fr]))\n                views.append(torch.stack([TF.normalize(TF.to_tensor(TF.hflip(TF.crop(f,t,lf,IMG_SIZE,IMG_SIZE))), MEAN, STD) for f in fr]))\n            return torch.stack(views), vid\n        elif self.mv:\n            w,h = fr[0].size\n            views = []\n            for t,lf in [(0,0), ((h-IMG_SIZE)//2,(w-IMG_SIZE)//2), (max(0,h-IMG_SIZE),max(0,w-IMG_SIZE))]:\n                views.append(torch.stack([TF.normalize(TF.to_tensor(TF.crop(f,t,lf,IMG_SIZE,IMG_SIZE)), MEAN, STD) for f in fr]))\n            return torch.stack(views), vid\n        return torch.stack([TF.normalize(TF.to_tensor(TF.center_crop(f, IMG_SIZE)), MEAN, STD) for f in fr]), vid\n\nclass Mix:\n    def __init__(self, nc, a=0.8): self.nc, self.a = nc, a\n    def __call__(self, b):\n        x,y = torch.utils.data.default_collate(b)\n        lam = np.random.beta(self.a, self.a)\n        i = torch.randperm(x.size(0))\n        return lam*x + (1-lam)*x[i], lam*F.one_hot(y,self.nc).float() + (1-lam)*F.one_hot(y[i],self.nc).float()\n\ndef train(m, ld, opt, sch, sc, mix=False, ls=0.0):\n    m.train()\n    loss_s, cor, tot = 0.0, 0, 0\n    for bi, (x, y) in enumerate(ld):\n        x, y = x.to(device), y.to(device)\n        with torch.amp.autocast('cuda'):\n            lo = m(x).logits\n            if mix:\n                loss = -torch.sum(y * F.log_softmax(lo,1), 1).mean()\n                lb = y.argmax(1)\n            else:\n                loss = F.cross_entropy(lo, y, label_smoothing=ls)\n                lb = y\n        cor += (lo.argmax(1)==lb).sum().item()\n        tot += lb.size(0)\n        loss_s += loss.item() * lb.size(0)\n        sc.scale(loss/ACCUM).backward()\n        if (bi+1) % ACCUM == 0:\n            sc.unscale_(opt)\n            torch.nn.utils.clip_grad_norm_(m.parameters(), 1.0)\n            sc.step(opt); sc.update(); opt.zero_grad(); sch.step()\n    return loss_s/tot, cor/tot\n\ndef evalu(m, ld, multi=False):\n    m.eval()\n    ps = []\n    with torch.no_grad():\n        for x, ids in ld:\n            if multi:\n                B,V,T,C,H,W = x.shape\n                lo = m(x.view(B*V,T,C,H,W).to(device)).logits.view(B,V,-1).mean(1)\n            else:\n                lo = m(x.to(device)).logits\n            ps.extend(zip(ids.tolist(), lo.argmax(1).cpu().tolist()))\n    return ps\n\nEXPS = [\n    {'name': 'Exp1_VideoMAE'},\n    {'name': 'Exp1b_LR_High', 'lr': 1.25e-4},\n    {'name': 'Exp3_MultiSeg', 'mv': True},\n    {'name': 'Exp5_LabelSmooth', 'con': True, 'ls': 0.1},\n    {'name': 'Exp7_FlipTTA', 'con': True, 'mix': True, 'two': True, 'tta': True},\n]\n\ndef main():\n    gt = dict(zip(pd.read_csv('test_labels.csv')['id'].astype(str), pd.read_csv('test_labels.csv')['class']))\n    res = []\n    for e in EXPS:\n        print(f\"\\n[GPU1] {e['name']}\")\n        random.seed(42); np.random.seed(42); torch.manual_seed(42)\n        lr = e.get('lr', LR)\n        tds = DS(PATH_TRAIN, e.get('con'))\n        tes = TDS(PATH_TEST, e.get('tta'), e.get('mv'))\n        cn = tds.classes\n        m = VideoMAEForVideoClassification.from_pretrained(MODEL_CKPT, num_labels=len(cn), ignore_mismatched_sizes=True, num_frames=NUM_FRAMES).to(device)\n        col = Mix(len(cn)) if e.get('mix') else None\n        tl = DataLoader(tds, BATCH, shuffle=True, num_workers=2, drop_last=True, collate_fn=col)\n        tel = DataLoader(tes, 4 if e.get('tta') or e.get('mv') else BATCH, num_workers=2)\n        opt = torch.optim.AdamW(m.parameters(), lr=lr, weight_decay=WD)\n        sc = torch.amp.GradScaler()\n        sch = get_cosine_schedule_with_warmup(opt, int(len(tl)*EPOCHS*0.1/ACCUM), len(tl)*EPOCHS//ACCUM)\n        for ep in range(EPOCHS):\n            import time\n            ep_start = time.time()\n            l, a = train(m, tl, opt, sch, sc, e.get('mix'), e.get('ls', 0.0))\n            ps = evalu(m, tel, e.get('tta') or e.get('mv'))\n            ta = accuracy_score([gt[str(i)] for i,_ in ps], [cn[p] for _,p in ps])\n            ep_time = time.time() - ep_start; eta = ep_time * (EPOCHS - ep - 1); print(f\"  [GPU{gpu_id}] Ep{ep+1}/{EPOCHS}: L={l:.4f}, Atr={a:.4f}, Ate={ta:.4f} | {ep_time//60:.0f}m{ep_time%60:.0f}s | ETA: {eta//60:.0f}m{eta%60:.0f}s\")\n        if e.get('two'):\n            opt = torch.optim.AdamW(m.parameters(), lr=1e-6, weight_decay=WD)\n            p2 = DataLoader(tds, BATCH, shuffle=True, num_workers=2, drop_last=True)\n            sch = get_cosine_schedule_with_warmup(opt, 0, len(p2)*3//ACCUM)\n            for ep in range(3):\n                l,a = train(m, p2, opt, sch, sc, False, 0.1)\n                ps = evalu(m, tel, e.get('tta'))\n                ta = accuracy_score([gt[str(i)] for i,_ in ps], [cn[p] for _,p in ps])\n                print(f\"  P2Ep{ep+1}: L={l:.4f}, Atr={a:.4f}, Ate={ta:.4f}\")\n        ps = evalu(m, tel, e.get('tta') or e.get('mv'))\n        fa = accuracy_score([gt[str(i)] for i,_ in ps], [cn[p] for _,p in ps])\n        print(f\"  FINAL: {fa:.4f}\")\n        res.append({'exp': e['name'], 'test_acc': fa, 'lr': lr, 'gpu': 1})\n        del m; torch.cuda.empty_cache()\n    pd.DataFrame(res).to_csv('results_gpu1.csv', index=False)\n    print(\"Saved results_gpu1.csv\")\n\nif __name__ == '__main__': main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T17:11:09.926284Z","iopub.execute_input":"2026-01-17T17:11:09.926449Z","iopub.status.idle":"2026-01-17T17:11:09.943485Z","shell.execute_reply.started":"2026-01-17T17:11:09.926436Z","shell.execute_reply":"2026-01-17T17:11:09.942800Z"}},"outputs":[{"name":"stdout","text":"Writing worker_gpu1.py\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"## 2. Run Both Workers in Parallel (Real-time Output)\nimport subprocess\nimport sys\nimport threading\nimport time\n\ndef stream_output(proc, name):\n    \"\"\"Stream process output in real-time.\"\"\"\n    for line in iter(proc.stdout.readline, ''):\n        if line:\n            print(f\"{line}\", end='', flush=True)\n    proc.stdout.close()\n\nprint(\"Starting 2 workers in parallel...\")\nprint(\"=\"*60)\n\n# Start both processes with stdout pipe\np0 = subprocess.Popen(\n    ['python', '-u', 'worker_gpu0.py'],  # -u for unbuffered\n    stdout=subprocess.PIPE,\n    stderr=subprocess.STDOUT,\n    text=True,\n    bufsize=1\n)\np1 = subprocess.Popen(\n    ['python', '-u', 'worker_gpu1.py'],\n    stdout=subprocess.PIPE,\n    stderr=subprocess.STDOUT,\n    text=True,\n    bufsize=1\n)\n\n# Create threads to stream output\nt0 = threading.Thread(target=stream_output, args=(p0, 'GPU0'))\nt1 = threading.Thread(target=stream_output, args=(p1, 'GPU1'))\n\nt0.start()\nt1.start()\n\n# Wait for completion\np0.wait()\np1.wait()\nt0.join()\nt1.join()\n\nprint(\"=\"*60)\nprint(\"Both workers finished!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T17:11:09.945329Z","iopub.execute_input":"2026-01-17T17:11:09.945514Z","execution_failed":"2026-01-18T05:04:57.646Z"}},"outputs":[{"name":"stdout","text":"Starting 2 workers in parallel...\n============================================================\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1768669882.734535     126 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768669882.734527     127 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768669882.785061     126 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nE0000 00:00:1768669882.785067     127 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\nAttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n\n[GPU1] Exp1_VideoMAE\n\n[GPU0] Exp0_ViT\nSome weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base-finetuned-kinetics and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([51]) in the model instantiated\n- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([51, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n  [GPU0] Ep1/10: L=3.3274, Atr=0.2189, Ate=0.4020 | 9m51s | ETA: 88m37s\n  [GPU1] Ep1/10: L=3.0612, Atr=0.3117, Ate=0.6549 | 11m60s | ETA: 107m59s\n  [GPU0] Ep2/10: L=1.2904, Atr=0.6510, Ate=0.6255 | 8m48s | ETA: 70m21s\n  [GPU1] Ep2/10: L=0.9255, Atr=0.8186, Ate=0.7824 | 12m6s | ETA: 96m45s\n  [GPU0] Ep3/10: L=0.7474, Atr=0.7910, Ate=0.6471 | 8m49s | ETA: 61m41s\n  [GPU1] Ep3/10: L=0.4535, Atr=0.8979, Ate=0.7980 | 12m5s | ETA: 84m36s\n  [GPU0] Ep4/10: L=0.4948, Atr=0.8636, Ate=0.6627 | 8m48s | ETA: 52m48s\n  [GPU0] Ep5/10: L=0.3272, Atr=0.9138, Ate=0.6725 | 9m7s | ETA: 45m35s\n  [GPU1] Ep4/10: L=0.2639, Atr=0.9446, Ate=0.8137 | 12m3s | ETA: 72m17s\n  [GPU0] Ep6/10: L=0.2261, Atr=0.9455, Ate=0.6765 | 8m49s | ETA: 35m16s\n  [GPU1] Ep5/10: L=0.1764, Atr=0.9630, Ate=0.8176 | 12m2s | ETA: 60m11s\n  [GPU0] Ep7/10: L=0.1442, Atr=0.9696, Ate=0.6882 | 8m18s | ETA: 24m55s\n  [GPU0] Ep8/10: L=0.1028, Atr=0.9816, Ate=0.6961 | 8m47s | ETA: 17m33s\n  [GPU1] Ep6/10: L=0.1080, Atr=0.9777, Ate=0.8137 | 12m0s | ETA: 48m1s\n  [GPU0] Ep9/10: L=0.0771, Atr=0.9881, Ate=0.6922 | 8m34s | ETA: 8m34s\n  [GPU1] Ep7/10: L=0.0778, Atr=0.9859, Ate=0.8373 | 12m0s | ETA: 36m1s\n  [GPU0] Ep10/10: L=0.0694, Atr=0.9909, Ate=0.6922 | 8m40s | ETA: 0m0s\n  FINAL: 0.6922\n\n[GPU0] Exp2_Consistent\nSome weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base-finetuned-kinetics and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([51]) in the model instantiated\n- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([51, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n  [GPU1] Ep8/10: L=0.0562, Atr=0.9889, Ate=0.8314 | 12m2s | ETA: 24m3s\n  [GPU0] Ep1/10: L=3.0315, Atr=0.3207, Ate=0.6804 | 11m51s | ETA: 106m36s\n  [GPU1] Ep9/10: L=0.0430, Atr=0.9918, Ate=0.8412 | 12m2s | ETA: 12m2s\n  [GPU0] Ep2/10: L=0.8644, Atr=0.8369, Ate=0.7667 | 11m53s | ETA: 95m3s\n  [GPU1] Ep10/10: L=0.0374, Atr=0.9933, Ate=0.8392 | 12m1s | ETA: 0m0s\n  FINAL: 0.8392\n\n[GPU1] Exp1b_LR_High\nSome weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base-finetuned-kinetics and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([51]) in the model instantiated\n- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([51, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n  [GPU0] Ep3/10: L=0.3607, Atr=0.9271, Ate=0.8000 | 11m52s | ETA: 83m2s\n  [GPU1] Ep1/10: L=2.4342, Atr=0.4534, Ate=0.7216 | 12m6s | ETA: 108m57s\n  [GPU0] Ep4/10: L=0.1780, Atr=0.9652, Ate=0.8255 | 11m40s | ETA: 69m60s\n  [GPU1] Ep2/10: L=0.6726, Atr=0.8300, Ate=0.7549 | 12m6s | ETA: 96m48s\n  [GPU0] Ep5/10: L=0.0959, Atr=0.9830, Ate=0.8294 | 11m53s | ETA: 59m24s\n  [GPU1] Ep3/10: L=0.4198, Atr=0.8872, Ate=0.8137 | 12m4s | ETA: 84m25s\n  [GPU0] Ep6/10: L=0.0434, Atr=0.9938, Ate=0.8333 | 11m46s | ETA: 47m6s\n  [GPU1] Ep4/10: L=0.2570, Atr=0.9314, Ate=0.8157 | 12m1s | ETA: 72m8s\n  [GPU0] Ep7/10: L=0.0200, Atr=0.9979, Ate=0.8353 | 11m37s | ETA: 34m52s\n  [GPU1] Ep5/10: L=0.1831, Atr=0.9505, Ate=0.8059 | 12m2s | ETA: 60m9s\n  [GPU0] Ep8/10: L=0.0128, Atr=0.9989, Ate=0.8392 | 11m57s | ETA: 23m54s\n  [GPU1] Ep6/10: L=0.1027, Atr=0.9737, Ate=0.8137 | 11m59s | ETA: 47m56s\n  [GPU0] Ep9/10: L=0.0097, Atr=0.9994, Ate=0.8431 | 11m53s | ETA: 11m53s\n  [GPU1] Ep7/10: L=0.0674, Atr=0.9824, Ate=0.8137 | 11m58s | ETA: 35m54s\n  [GPU0] Ep10/10: L=0.0085, Atr=0.9997, Ate=0.8431 | 11m55s | ETA: 0m0s\n  FINAL: 0.8431\n\n[GPU0] Exp4_Mixup\nSome weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base-finetuned-kinetics and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([51]) in the model instantiated\n- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([51, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n  [GPU1] Ep8/10: L=0.0433, Atr=0.9893, Ate=0.8196 | 11m59s | ETA: 23m58s\n  [GPU0] Ep1/10: L=3.3310, Atr=0.2564, Ate=0.6412 | 14m56s | ETA: 134m20s\n  [GPU1] Ep9/10: L=0.0256, Atr=0.9944, Ate=0.8196 | 11m60s | ETA: 11m60s\n  [GPU0] Ep2/10: L=1.7567, Atr=0.7131, Ate=0.7784 | 14m56s | ETA: 119m25s\n  [GPU1] Ep10/10: L=0.0247, Atr=0.9947, Ate=0.8157 | 11m59s | ETA: 0m0s\n  FINAL: 0.8157\n\n[GPU1] Exp3_MultiSeg\nSome weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base-finetuned-kinetics and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([51]) in the model instantiated\n- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([51, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n  [GPU0] Ep3/10: L=1.3075, Atr=0.7708, Ate=0.7902 | 14m32s | ETA: 101m47s\n  [GPU1] Ep1/10: L=3.0649, Atr=0.3083, Ate=0.6333 | 14m33s | ETA: 130m53s\n  [GPU0] Ep4/10: L=1.1181, Atr=0.8252, Ate=0.8020 | 14m1s | ETA: 84m9s\n  [GPU1] Ep2/10: L=0.9255, Atr=0.8191, Ate=0.7725 | 14m32s | ETA: 116m12s\n  [GPU0] Ep5/10: L=0.9992, Atr=0.8399, Ate=0.8157 | 13m32s | ETA: 67m38s\n  [GPU1] Ep3/10: L=0.4555, Atr=0.8954, Ate=0.7922 | 14m30s | ETA: 101m28s\n  [GPU0] Ep6/10: L=0.8905, Atr=0.8688, Ate=0.8157 | 14m6s | ETA: 56m26s\n  [GPU1] Ep4/10: L=0.2591, Atr=0.9460, Ate=0.8255 | 14m30s | ETA: 86m58s\n  [GPU0] Ep7/10: L=0.8846, Atr=0.8551, Ate=0.8275 | 13m60s | ETA: 41m59s\n  [GPU1] Ep5/10: L=0.1763, Atr=0.9612, Ate=0.8118 | 14m30s | ETA: 72m29s\n  [GPU0] Ep8/10: L=0.8250, Atr=0.8691, Ate=0.8275 | 14m5s | ETA: 28m10s\n  [GPU1] Ep6/10: L=0.1058, Atr=0.9788, Ate=0.8255 | 14m28s | ETA: 57m51s\n  [GPU0] Ep9/10: L=0.7654, Atr=0.8875, Ate=0.8255 | 14m7s | ETA: 14m7s\n  [GPU1] Ep7/10: L=0.0773, Atr=0.9851, Ate=0.8392 | 14m27s | ETA: 43m21s\n  [GPU0] Ep10/10: L=0.7813, Atr=0.8771, Ate=0.8255 | 13m51s | ETA: 0m0s\n  FINAL: 0.8255\n\n[GPU0] Exp6_2Stage\nSome weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base-finetuned-kinetics and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([51]) in the model instantiated\n- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([51, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n  [GPU1] Ep8/10: L=0.0553, Atr=0.9893, Ate=0.8471 | 14m27s | ETA: 28m55s\n  [GPU0] Ep1/10: L=3.3317, Atr=0.2580, Ate=0.6471 | 13m53s | ETA: 124m56s\n  [GPU1] Ep9/10: L=0.0434, Atr=0.9928, Ate=0.8510 | 14m28s | ETA: 14m28s\n  [GPU0] Ep2/10: L=1.7560, Atr=0.7107, Ate=0.7627 | 13m56s | ETA: 111m28s\n  [GPU1] Ep10/10: L=0.0381, Atr=0.9938, Ate=0.8510 | 14m27s | ETA: 0m0s\n  FINAL: 0.8510\n\n[GPU1] Exp5_LabelSmooth\nSome weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base-finetuned-kinetics and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([51]) in the model instantiated\n- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([51, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n  [GPU0] Ep3/10: L=1.3065, Atr=0.7707, Ate=0.7902 | 13m18s | ETA: 93m6s\n  [GPU1] Ep1/10: L=3.1474, Atr=0.3252, Ate=0.6804 | 12m28s | ETA: 112m13s\n  [GPU0] Ep4/10: L=1.1158, Atr=0.8231, Ate=0.8078 | 14m45s | ETA: 88m28s\n  [GPU1] Ep2/10: L=1.3342, Atr=0.8396, Ate=0.7725 | 12m33s | ETA: 100m22s\n  [GPU0] Ep5/10: L=0.9969, Atr=0.8383, Ate=0.8118 | 15m1s | ETA: 75m3s\n  [GPU1] Ep3/10: L=0.9675, Atr=0.9282, Ate=0.8000 | 12m23s | ETA: 86m39s\n  [GPU0] Ep6/10: L=0.8899, Atr=0.8694, Ate=0.8157 | 14m50s | ETA: 59m19s\n  [GPU1] Ep4/10: L=0.8383, Atr=0.9692, Ate=0.8157 | 12m34s | ETA: 75m23s\n  [GPU0] Ep7/10: L=0.8852, Atr=0.8530, Ate=0.8235 | 14m57s | ETA: 44m52s\n  [GPU1] Ep5/10: L=0.7778, Atr=0.9862, Ate=0.8235 | 12m31s | ETA: 62m34s\n  [GPU1] Ep6/10: L=0.7442, Atr=0.9947, Ate=0.8314 | 12m36s | ETA: 50m24s\n  [GPU0] Ep8/10: L=0.8222, Atr=0.8716, Ate=0.8235 | 14m45s | ETA: 29m31s\n  [GPU1] Ep7/10: L=0.7263, Atr=0.9982, Ate=0.8373 | 12m24s | ETA: 37m11s\n  [GPU0] Ep9/10: L=0.7660, Atr=0.8889, Ate=0.8314 | 14m52s | ETA: 14m52s\n  [GPU1] Ep8/10: L=0.7201, Atr=0.9986, Ate=0.8412 | 12m35s | ETA: 25m10s\n  [GPU0] Ep10/10: L=0.7773, Atr=0.8808, Ate=0.8314 | 14m59s | ETA: 0m0s\n  [GPU1] Ep9/10: L=0.7167, Atr=0.9994, Ate=0.8471 | 12m28s | ETA: 12m28s\n  P2Ep1: L=0.7827, Atr=0.9947, Ate=0.8275\n  [GPU1] Ep10/10: L=0.7159, Atr=0.9997, Ate=0.8471 | 12m18s | ETA: 0m0s\n  FINAL: 0.8471\n\n[GPU1] Exp7_FlipTTA\nSome weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base-finetuned-kinetics and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([51]) in the model instantiated\n- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([51, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n  P2Ep2: L=0.7639, Atr=0.9970, Ate=0.8275\n  P2Ep3: L=0.7596, Atr=0.9968, Ate=0.8275\n  FINAL: 0.8275\n\n[GPU0] Exp8_LR_High\nSome weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base-finetuned-kinetics and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([51]) in the model instantiated\n- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([51, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n  [GPU1] Ep1/10: L=3.3309, Atr=0.2551, Ate=0.6471 | 20m1s | ETA: 180m12s\n  [GPU0] Ep1/10: L=2.8739, Atr=0.3864, Ate=0.7157 | 20m25s | ETA: 183m44s\n  [GPU1] Ep2/10: L=1.7545, Atr=0.7107, Ate=0.7863 | 20m33s | ETA: 164m21s\n  [GPU0] Ep2/10: L=1.5433, Atr=0.7213, Ate=0.7471 | 19m55s | ETA: 159m22s\n  [GPU1] Ep3/10: L=1.3102, Atr=0.7673, Ate=0.7843 | 20m0s | ETA: 140m3s\n  [GPU0] Ep3/10: L=1.2619, Atr=0.7726, Ate=0.7784 | 20m13s | ETA: 141m28s\n  [GPU1] Ep4/10: L=1.1135, Atr=0.8255, Ate=0.8137 | 20m25s | ETA: 122m28s\n  [GPU0] Ep4/10: L=1.1056, Atr=0.8213, Ate=0.8020 | 20m3s | ETA: 120m18s\n  [GPU1] Ep5/10: L=0.9957, Atr=0.8394, Ate=0.8294 | 20m14s | ETA: 101m10s\n  [GPU0] Ep5/10: L=0.9831, Atr=0.8452, Ate=0.7961 | 19m53s | ETA: 99m24s\n  [GPU1] Ep6/10: L=0.8900, Atr=0.8742, Ate=0.8255 | 20m3s | ETA: 80m12s\n  [GPU0] Ep6/10: L=0.8640, Atr=0.8712, Ate=0.8059 | 20m14s | ETA: 80m57s\n  [GPU1] Ep7/10: L=0.8868, Atr=0.8554, Ate=0.8294 | 20m20s | ETA: 60m59s\n  [GPU0] Ep7/10: L=0.8365, Atr=0.8495, Ate=0.8235 | 20m13s | ETA: 60m40s\n  [GPU1] Ep8/10: L=0.8248, Atr=0.8678, Ate=0.8451 | 20m19s | ETA: 40m37s\n  [GPU0] Ep8/10: L=0.7585, Atr=0.8769, Ate=0.8176 | 20m20s | ETA: 40m40s\n  [GPU1] Ep9/10: L=0.7666, Atr=0.8881, Ate=0.8431 | 20m24s | ETA: 20m24s\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"## 3. Merge Results and Plot\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load results\ndf0 = pd.read_csv('results_gpu0.csv')\ndf1 = pd.read_csv('results_gpu1.csv')\ndf = pd.concat([df0, df1]).sort_values('exp').reset_index(drop=True)\n\nprint(\"=\"*60)\nprint(\"ALL RESULTS\")\nprint(\"=\"*60)\nprint(df.to_string(index=False))\n\n# LR Comparison\nprint(\"\\n--- LR Comparison ---\")\nlr_low = df[df['exp'] == 'Exp1_VideoMAE']['test_acc'].values[0]\nlr_high = df[df['exp'] == 'Exp1b_LR_High']['test_acc'].values[0]\nprint(f\"LR 5e-5:    {lr_low:.4f}\")\nprint(f\"LR 1.25e-4: {lr_high:.4f}\")\nprint(f\"Difference: {(lr_low - lr_high)*100:+.2f}%\")\n\n# Plot\nfig, ax = plt.subplots(figsize=(14, 6))\ncolors = ['#e74c3c' if 'High' in e or '1.25' in str(l) else '#3498db' for e, l in zip(df['exp'], df['lr'])]\nbars = ax.bar(range(len(df)), df['test_acc'] * 100, color=colors, edgecolor='black')\n\nax.set_xticks(range(len(df)))\nax.set_xticklabels([e.replace('Exp', '').replace('_', '\\n') for e in df['exp']], fontsize=9)\nax.set_ylabel('Test Accuracy (%)', fontsize=12)\nax.set_title('VideoMAE Ablation Study (Blue=LR 5e-5, Red=LR 1.25e-4)', fontsize=14, fontweight='bold')\n\nfor bar, acc in zip(bars, df['test_acc']):\n    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, f'{acc*100:.1f}%',\n            ha='center', fontsize=9, fontweight='bold')\n\nax.set_ylim([50, 100])\nax.grid(axis='y', alpha=0.3)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-18T05:04:57.647Z"}},"outputs":[],"execution_count":null}]}