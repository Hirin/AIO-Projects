# Phase 3 - VideoMAE Advanced Training Analysis

> **TÃ i liá»‡u phÃ¢n tÃ­ch chi tiáº¿t** vá» notebook `project-7-1-phase-3.ipynb` - Giáº£i phÃ¡p nÃ¢ng cao sá»­ dá»¥ng VideoMAE cho bÃ i toÃ¡n Video Action Recognition.

---

## ğŸ“‹ Tá»•ng Quan

| ThÃ´ng tin | Chi tiáº¿t |
|-----------|----------|
| **BÃ i toÃ¡n** | Video Action Recognition (Nháº­n dáº¡ng hÃ nh Ä‘á»™ng trong video) |
| **Dataset** | HMDB51 (51 classes) |
| **Model** | VideoMAE-Base (Kinetics-400 pretrained) |
| **Framework** | PyTorch + HuggingFace Transformers |
| **Platform** | Kaggle (Tesla T4 GPU) |
| **Training Time** | ~6.8 giá» (40 epochs total) |
| **Target Accuracy** | > 0.83 |

---

## ğŸ¯ CÃ¡c Cáº£i Tiáº¿n ChÃ­nh

1. **Model**: `VideoMAE-base` - SOTA cho dataset video nhá»
2. **Augmentation**: `Mixup` + Consistent Spatial Transform
3. **Training Strategy**: **2-Stage Fine-tuning**
4. **Inference**: **6-View TTA** (Test Time Augmentation)

---

## ğŸ—ï¸ Kiáº¿n TrÃºc Model

### VideoMAE Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INPUT VIDEO                               â”‚
â”‚                  [B, T, C, H, W]                             â”‚
â”‚            (Batch, 16 Frames, 3, 224, 224)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              VideoMAE Encoder                                â”‚
â”‚     (MCG-NJU/videomae-base-finetuned-kinetics)              â”‚
â”‚                                                              â”‚
â”‚    â€¢ Pretrained on Kinetics-400 (video dataset)             â”‚
â”‚    â€¢ Temporal + Spatial Attention                           â”‚
â”‚    â€¢ Masked Autoencoder pretraining                         â”‚
â”‚    â€¢ Hidden Dim: 768                                        â”‚
â”‚    â€¢ Heads: 12                                              â”‚
â”‚    â€¢ Layers: 12                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              VIDEO FEATURES                                  â”‚
â”‚         [B, sequence_length, hidden_dim]                     â”‚
â”‚                                                              â”‚
â”‚    Temporal attention learns motion patterns                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            CLASSIFICATION HEAD                               â”‚
â”‚              nn.Linear(768, 51)                              â”‚
â”‚                                                              â”‚
â”‚                 [B, num_classes]                             â”‚
â”‚                   (B, 51)                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Model Specifications

| Component | Specification |
|-----------|---------------|
| **Checkpoint** | `MCG-NJU/videomae-base-finetuned-kinetics` |
| **Pretrain Dataset** | Kinetics-400 (video action recognition) |
| **Hidden Dim** | 768 |
| **Attention Heads** | 12 |
| **Transformer Layers** | 12 |
| **Input Frames** | 16 |
| **Input Size** | 224Ã—224 |

---

## âš™ï¸ Configuration & Hyperparameters

### Model Config

```python
MODEL_CKPT = "MCG-NJU/videomae-base-finetuned-kinetics"
NUM_FRAMES = 16
IMG_SIZE = 224
RESIZE_SIZE = 256  # Resize short edge trÆ°á»›c khi crop
```

### Phase 1 Config (Heavy Augmentation)

```python
EPOCHS_P1 = 30
LR_P1 = 5e-5          # Higher LR cho exploration
MIXUP_ALPHA = 0.8     # Mixup strength
MIXUP_PROB = 1.0      # Always apply mixup
```

### Phase 2 Config (Fine-tuning / Polishing)

```python
EPOCHS_P2 = 10
LR_P2 = 1e-6          # Very low LR Ä‘á»ƒ polish
LABEL_SMOOTHING = 0.1 # Prevent overconfidence
```

### Common Config

```python
BATCH_SIZE = 8
ACCUM_STEPS = 4       # Effective batch = 32
WEIGHT_DECAY = 0.05
WARMUP_RATIO = 0.1
```

---

## ğŸ“Š Data Pipeline

### 1. Frame Sampling Strategy

```python
def __getitem__(self, idx):
    # Training: Random stride cho diversity
    if self.is_train:
        max_stride = max(1, (total_frames - 1) // (self.num_frames - 1))
        stride = random.randint(1, min(max_stride, 4))
    else:
        # Validation: Fixed stride
        stride = max(1, (total_frames - 1) // (self.num_frames - 1))
    
    # Uniform sampling
    frame_indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)
```

### 2. Consistent Spatial Transform (Key Innovation)

```python
# CRITICAL: Táº¥t cáº£ frames trong 1 video pháº£i cÃ³ CÃ™NG transform parameters
if self.is_train:
    # 1. Láº¥y random parameters Má»˜T Láº¦N cho cáº£ video
    i, j, h, w = T.RandomResizedCrop.get_params(
        frames[0], 
        scale=(0.8, 1.0), 
        ratio=(0.75, 1.33)
    )
    is_flip = random.random() > 0.5
    
    # 2. Apply CÃ™NG parameters cho Táº¤T Cáº¢ frames
    for img in frames:
        img = TF.resized_crop(img, i, j, h, w, size=(224, 224))
        if is_flip:
            img = TF.hflip(img)
        img = TF.normalize(TF.to_tensor(img), mean=MEAN, std=STD)
```

> ğŸ’¡ **Táº¡i sao Consistent Transform quan trá»ng?**
> - Video cÃ³ temporal coherence - cÃ¡c frames liÃªn tiáº¿p pháº£i nháº¥t quÃ¡n
> - Inconsistent transform cÃ³ thá»ƒ phÃ¡ vá»¡ motion patterns
> - GiÃºp model há»c Ä‘Æ°á»£c chuyá»ƒn Ä‘á»™ng thá»±c sá»±, khÃ´ng bá»‹ nhiá»…u bá»Ÿi artifacts

### 3. Mixup Augmentation

```python
class MixupCollate:
    def __init__(self, num_classes, alpha=0.8, prob=1.0):
        self.alpha = alpha
        self.prob = prob
    
    def __call__(self, batch):
        if np.random.rand() > self.prob:
            return inputs, F.one_hot(targets, num_classes).float()
        
        # Mix two samples
        lam = np.random.beta(self.alpha, self.alpha)
        index = torch.randperm(batch_size)
        
        # Mixed inputs
        inputs = lam * inputs + (1 - lam) * inputs[index, :]
        
        # Soft labels
        targets_one_hot = F.one_hot(targets, num_classes).float()
        targets = lam * targets_one_hot + (1 - lam) * targets_one_hot[index, :]
        
        return inputs, targets
```

---

## ğŸ‹ï¸ 2-Stage Training Strategy

### Phase 1: Heavy Augmentation (30 Epochs)

```python
print("STARTING PHASE 1 (Mixup Enabled, LR=5e-5, Epochs=30)")

optimizer = torch.optim.AdamW(model.parameters(), lr=LR_P1, weight_decay=0.05)
scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)

for epoch in range(EPOCHS_P1):
    loss, acc = train_epoch(
        model, train_loader_p1, optimizer, scheduler, scaler, 
        DEVICE, ACCUM_STEPS, 
        use_mixup=True  # Mixup ON
    )
```

**Má»¥c Ä‘Ã­ch Phase 1:**
- Há»c robust features vá»›i strong augmentation
- Higher LR cho faster exploration
- Mixup ngÄƒn overfitting sá»›m

### Phase 2: Fine-tuning (10 Epochs)

```python
print("STARTING PHASE 2 (No Mixup, Label Smooth=0.1, Low LR=1e-6)")

# Load best model tá»« Phase 1
model = VideoMAEForVideoClassification.from_pretrained("./videomae_phase1_best")

# Very low LR Ä‘á»ƒ khÃ´ng phÃ¡ vá»¡ learned features
optimizer = torch.optim.AdamW(model.parameters(), lr=LR_P2, weight_decay=0.05)

for epoch in range(EPOCHS_P2):
    loss, acc = train_epoch(
        model, train_loader_p2, optimizer, scheduler, scaler,
        DEVICE, ACCUM_STEPS,
        use_mixup=False,           # Mixup OFF
        label_smoothing=0.1        # Label Smoothing ON
    )
```

**Má»¥c Ä‘Ã­ch Phase 2:**
- Polish model vá»›i clean data (no mixup)
- Label Smoothing ngÄƒn overconfident predictions
- Very low LR Ä‘á»ƒ fine-tune nháº¹

### Training Loop vá»›i Dual Loss Support

```python
def train_epoch(..., use_mixup=True, label_smoothing=0.0):
    for inputs, targets in loader:
        outputs = model(inputs)
        logits = outputs.logits
        
        if use_mixup:
            # Soft cross-entropy cho mixup labels
            log_probs = F.log_softmax(logits, dim=1)
            loss = -torch.sum(targets * log_probs, dim=1).mean()
        else:
            # Standard CE vá»›i optional label smoothing
            loss = F.cross_entropy(logits, targets, label_smoothing=label_smoothing)
        
        # Gradient accumulation + AMP
        loss = loss / accum_steps
        scaler.scale(loss).backward()
        
        if (step + 1) % accum_steps == 0:
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            scaler.step(optimizer)
            scaler.update()
            scheduler.step()
```

---

## ğŸ” 6-View Test Time Augmentation (TTA)

### MultiViewTestDataset

```python
class MultiViewTestDataset(Dataset):
    """Táº¡o 6 gÃ³c nhÃ¬n cho má»—i video."""
    
    def __getitem__(self, idx):
        # 1. Resize shortest side to 256
        frames = [TF.resize(img, 256) for img in frames]
        
        views = []
        w, h = frames[0].size
        crop_size = 224
        
        # --- 3 STANDARD CROPS ---
        # Center Crop
        views.append([TF.center_crop(img, (224, 224)) for img in frames])
        
        # Side Crops (Left/Right hoáº·c Top/Bottom tÃ¹y aspect ratio)
        if w > h:
            views.append([TF.crop(img, 0, 0, 224, 224) for img in frames])        # Left
            views.append([TF.crop(img, 0, w-224, 224, 224) for img in frames])    # Right
        else:
            views.append([TF.crop(img, 0, 0, 224, 224) for img in frames])        # Top
            views.append([TF.crop(img, h-224, 0, 224, 224) for img in frames])    # Bottom
        
        # --- 3 FLIPPED CROPS ---
        flipped_views = []
        for v_frames in views:
            flipped_views.append([TF.hflip(img) for img in v_frames])
        
        all_views = views + flipped_views  # Total: 6 views
        
        return torch.stack(view_tensors), video_id  # Shape: (6, T, C, H, W)
```

### 6-View Inference

```python
with torch.no_grad():
    for multi_view_videos, video_ids in test_loader:
        # Shape: (B, 6, T, C, H, W)
        B, V, T, C, H, W = multi_view_videos.shape
        
        # Flatten views into batch: (B*6, T, C, H, W)
        flat_videos = multi_view_videos.view(B * V, T, C, H, W).to(DEVICE)
        
        outputs = model(flat_videos)
        logits = outputs.logits  # (B*6, 51)
        
        # Reshape: (B, 6, 51)
        logits = logits.view(B, V, -1)
        
        # Average over 6 views
        avg_logits = logits.mean(dim=1)  # (B, 51)
        
        preds = avg_logits.argmax(dim=1)
```

### TTA Visualization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Original Video                            â”‚
â”‚                  (W x H resolution)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼           â–¼           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Left/Top â”‚ â”‚  Center  â”‚ â”‚Right/Bot â”‚
    â”‚   Crop   â”‚ â”‚   Crop   â”‚ â”‚   Crop   â”‚
    â”‚ 224x224  â”‚ â”‚ 224x224  â”‚ â”‚ 224x224  â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         â”‚            â”‚            â”‚
         â–¼            â–¼            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ + Flip   â”‚ â”‚ + Flip   â”‚ â”‚ + Flip   â”‚
    â”‚ (View 4) â”‚ â”‚ (View 5) â”‚ â”‚ (View 6) â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         â”‚            â”‚            â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Average Pool â”‚
              â”‚   Logits     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Prediction  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ˆ Training Techniques Summary

| Technique | Phase 1 | Phase 2 | Purpose |
|-----------|---------|---------|---------|
| **Mixup** | âœ… Î±=0.8 | âŒ | Regularization, prevent overfitting |
| **Label Smoothing** | âŒ | âœ… 0.1 | Calibration, prevent overconfidence |
| **Learning Rate** | 5e-5 | 1e-6 | Explore â†’ Polish |
| **LR Schedule** | Cosine + Warmup | Cosine | Smooth decay |
| **Gradient Clipping** | âœ… max=1.0 | âœ… max=1.0 | Stability |
| **Mixed Precision** | âœ… FP16 | âœ… FP16 | Memory + Speed |

---

## ğŸ’¡ Key Insights

### 1. VideoMAE vs Image ViT
- **VideoMAE** Ä‘Æ°á»£c pretrain vá»›i Masked Autoencoder trÃªn video
- Há»c Ä‘Æ°á»£c temporal patterns vÃ  motion features
- Kinetics-400 pretrain phÃ¹ há»£p hÆ¡n ImageNet cho action recognition

### 2. 2-Stage Training Rationale
- **Phase 1**: Strong augmentation + high LR = diverse feature learning
- **Phase 2**: Clean data + low LR = refinement without forgetting

### 3. Consistent Transform Importance
- Video frames pháº£i giá»¯ spatial coherence
- Random per-frame transforms phÃ¡ vá»¡ motion information

### 4. 6-View TTA Benefits
- Robust hÆ¡n single-view prediction
- Capture different regions of action
- Horizontal flip handles left/right invariance

---

## ğŸ“ Submission Format

```csv
id,class
0,brush_hair
1,catch
2,clap
...
```

Output file: `submission_multiview_6crops.csv`

---

## ğŸ”§ Dependencies

```python
pip install transformers accelerate evaluate

# Main imports
from transformers import VideoMAEForVideoClassification, VideoMAEImageProcessor
from transformers import get_cosine_schedule_with_warmup
```

---

## â±ï¸ Training Timeline

| Phase | Duration | Purpose |
|-------|----------|---------|
| Phase 1 | ~6.7 hours | Heavy training vá»›i Mixup |
| Phase 2 | ~0.1 hours | Light fine-tuning |
| Inference | ~7 minutes | 6-View TTA |
| **Total** | **~6.8 hours** | |
