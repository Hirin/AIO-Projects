# Baseline BTC - Video Action Recognition Analysis

> **TÃ i liá»‡u phÃ¢n tÃ­ch chi tiáº¿t** vá» notebook `baseline-btc.ipynb` - Giáº£i phÃ¡p baseline cho bÃ i toÃ¡n Video Action Recognition trÃªn dataset HMDB51.

---

## ï¿½ Tá»•ng Quan

| ThÃ´ng tin | Chi tiáº¿t |
|-----------|----------|
| **BÃ i toÃ¡n** | Video Action Recognition (Nháº­n dáº¡ng hÃ nh Ä‘á»™ng trong video) |
| **Dataset** | HMDB51 (51 classes) |
| **Backbone** | ViT-Small (ImageNet pretrained) |
| **Framework** | PyTorch + timm |
| **Platform** | Kaggle (Tesla T4 GPU) |
| **Training Time** | ~21 phÃºt (4 epochs) |

---

## ğŸ—ï¸ Kiáº¿n TrÃºc Model

### LightweightViTForAction

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INPUT VIDEO                               â”‚
â”‚                  [B, T, C, H, W]                             â”‚
â”‚            (Batch, 16 Frames, 3, 224, 224)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              RESHAPE TO FRAMES                               â”‚
â”‚                 [B*T, C, H, W]                               â”‚
â”‚              (B*16, 3, 224, 224)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               ViT-Small Backbone                             â”‚
â”‚        (vit_small_patch16_224 - ImageNet pretrained)         â”‚
â”‚                                                              â”‚
â”‚    â€¢ Patch Size: 16x16                                       â”‚
â”‚    â€¢ Hidden Dim: 384                                         â”‚
â”‚    â€¢ Heads: 6                                                â”‚
â”‚    â€¢ Layers: 12                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FRAME FEATURES                                  â”‚
â”‚                [B*T, embed_dim]                              â”‚
â”‚                  (B*16, 384)                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              RESHAPE BACK                                    â”‚
â”‚              [B, T, embed_dim]                               â”‚
â”‚                (B, 16, 384)                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           TEMPORAL MEAN POOLING                              â”‚
â”‚              [B, embed_dim]                                  â”‚
â”‚                 (B, 384)                                     â”‚
â”‚                                                              â”‚
â”‚    pooled = features.mean(dim=1)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            LINEAR CLASSIFICATION HEAD                        â”‚
â”‚              nn.Linear(384, 51)                              â”‚
â”‚                                                              â”‚
â”‚                 [B, num_classes]                             â”‚
â”‚                   (B, 51)                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Äáº·c Ä‘iá»ƒm Model

| Component | Specification |
|-----------|---------------|
| **Backbone** | `vit_small_patch16_224` |
| **Parameters** | ~22M |
| **Model Size** | ~84 MB |
| **Embed Dim** | 384 |
| **Temporal Modeling** | Mean Pooling (khÃ´ng cÃ³ learnable temporal layer) |
| **Classification Head** | Single Linear layer |

---

## âš™ï¸ Configuration & Hyperparameters

### Data Parameters

```python
PATH_DATA_TRAIN = '/kaggle/input/action-video/data/data_train'
PATH_DATA_TEST = '/kaggle/input/action-video/data/test'

NUM_FRAMES = 16        # Sá»‘ frames láº¥y tá»« má»—i video
FRAME_STRIDE = 2       # BÆ°á»›c nháº£y khi sampling frames
IMG_SIZE = 224         # KÃ­ch thÆ°á»›c áº£nh Ä‘áº§u vÃ o
```

### Training Parameters

```python
BATCH_SIZE = 16
EPOCHS = 4
BASE_LR = 1e-4         # Learning rate cho backbone
HEAD_LR = 5e-4         # Learning rate cho classification head (5x cao hÆ¡n)
WEIGHT_DECAY = 0.05
GRAD_ACCUM_STEPS = 4   # Effective batch size = 16 * 4 = 64
```

---

## ï¿½ Data Pipeline

### 1. Frame Sampling Strategy

```python
def _select_indices(self, total):
    """
    Uniform sampling vá»›i stride tá»« video.
    
    VÃ­ dá»¥: video 100 frames, NUM_FRAMES=16, FRAME_STRIDE=2
    -> steps = max(16*2, 16) = 32
    -> grid = linspace(0, 99, 32) = [0, 3.2, 6.4, ..., 99]
    -> idxs = grid[::2] = láº¥y má»—i 2 bÆ°á»›c = 16 frames
    """
    steps = max(self.num_frames * self.frame_stride, self.num_frames)
    grid = torch.linspace(0, total - 1, steps=steps)
    idxs = grid[::self.frame_stride].long()
    
    # Padding náº¿u khÃ´ng Ä‘á»§ frames
    if idxs.numel() < self.num_frames:
        pad = idxs.new_full((self.num_frames - idxs.numel(),), idxs[-1].item())
        idxs = torch.cat([idxs, pad], dim=0)
    
    return idxs[:self.num_frames]
```

### 2. Data Augmentation (VideoTransform)

#### Training Augmentation

```python
# 1. Random Scale (0.8 - 1.0)
scale = random.uniform(0.8, 1.0)
new_h, new_w = int(h * scale), int(w * scale)

# 2. Random Crop to 224x224
i = random.randint(0, max(0, new_h - 224))
j = random.randint(0, max(0, new_w - 224))

# 3. Resize náº¿u cáº§n
frames = TF.resize(frames, [224, 224])

# 4. Random Horizontal Flip (p=0.5)
if random.random() < 0.5:
    frames = TF.hflip(frames)

# 5. Normalize
mean = [0.5, 0.5, 0.5]
std = [0.5, 0.5, 0.5]
```

#### Test Augmentation

```python
# Chá»‰ Resize + Normalize (khÃ´ng random transform)
frames = TF.resize(frames, [224, 224])
normalized = TF.normalize(frame, mean, std)
```

> âš ï¸ **LÆ°u Ã½**: Baseline khÃ´ng sá»­ dá»¥ng consistent spatial transform - má»—i frame cÃ³ thá»ƒ bá»‹ crop/flip khÃ¡c nhau trong cÃ¹ng má»™t video.

---

## ğŸ‹ï¸ Training Loop

### Mixed Precision Training vá»›i Gradient Accumulation

```python
def train_one_epoch(model, loader, optimizer, scaler, device, grad_accum_steps=1):
    model.train()
    optimizer.zero_grad()
    
    for batch_idx, (videos, labels) in enumerate(loader):
        # Forward vá»›i AMP
        with torch.amp.autocast(device_type='cuda', enabled=True):
            logits = model(videos)
            loss = F.cross_entropy(logits, labels)
        
        # Scale loss cho gradient accumulation
        loss = loss / grad_accum_steps
        scaler.scale(loss).backward()
        
        # Update weights má»—i grad_accum_steps batches
        should_step = ((batch_idx + 1) % grad_accum_steps == 0)
        if should_step:
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()
```

### Differential Learning Rates

```python
# Backbone (ViT): Lower LR Ä‘á»ƒ giá»¯ pretrained features
backbone_params = [p for n, p in model.named_parameters() if 'head' not in n]

# Head (Linear): Higher LR Ä‘á»ƒ há»c nhanh task má»›i
head_params = [p for n, p in model.named_parameters() if 'head' in n]

optimizer = torch.optim.AdamW([
    {"params": backbone_params, "lr": 1e-4},   # BASE_LR
    {"params": head_params, "lr": 5e-4},       # HEAD_LR (5x)
], weight_decay=0.05)
```

---

## ğŸ” Inference

### Standard Single-View Inference

```python
model.eval()
with torch.no_grad():
    for videos, video_ids in test_loader:
        videos = videos.to(DEVICE)
        logits = model(videos)
        preds = logits.argmax(dim=1)  # Hard prediction
```

> âš ï¸ **KhÃ´ng cÃ³ TTA (Test Time Augmentation)** - Chá»‰ sá»­ dá»¥ng center resize, khÃ´ng flip hay multi-crop.

---

## ğŸ“ Dataset Structure

### Training Data

```
data_train/
â”œâ”€â”€ brush_hair/
â”‚   â”œâ”€â”€ video_001/
â”‚   â”‚   â”œâ”€â”€ frame_0001.jpg
â”‚   â”‚   â”œâ”€â”€ frame_0002.jpg
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ video_002/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ cartwheel/
â”œâ”€â”€ catch/
â””â”€â”€ ... (51 classes)
```

### Test Data

```
test/
â”œâ”€â”€ 0/
â”‚   â”œâ”€â”€ frame_0001.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ 1/
â”œâ”€â”€ 2/
â””â”€â”€ ... (video IDs)
```

---

## ï¿½ Káº¿t Quáº£ Training

| Epoch | Train Loss | Train Acc |
|-------|-----------|-----------|
| 1 | - | ~0.4x |
| 2 | - | ~0.5x |
| 3 | - | ~0.6x |
| 4 | - | ~0.65-0.70 |

---

## âš ï¸ Háº¡n Cháº¿ cá»§a Baseline

| Váº¥n Ä‘á» | MÃ´ táº£ |
|--------|-------|
| **No Temporal Learning** | Mean pooling khÃ´ng capture Ä‘Æ°á»£c motion patterns giá»¯a cÃ¡c frames |
| **Weak Augmentation** | KhÃ´ng cÃ³ Mixup/CutMix, dá»… bá»‹ overfit |
| **Short Training** | Chá»‰ 4 epochs, model chÆ°a converge hoÃ n toÃ n |
| **No TTA** | Single view inference kÃ©m robust |
| **Inconsistent Transforms** | Má»—i frame cÃ³ thá»ƒ bá»‹ transform khÃ¡c nhau |
| **ImageNet Pretrain Only** | Pretrain trÃªn áº£nh tÄ©nh, khÃ´ng phÃ¹ há»£p cho video understanding |

---

## ï¿½ Code Snippets Quan Trá»ng

### Model Definition

```python
class LightweightViTForAction(nn.Module):
    def __init__(self, num_classes=51, pretrained_name='vit_small_patch16_224'):
        super().__init__()
        self.vit = timm.create_model(pretrained_name, pretrained=True, num_classes=0)
        self.embed_dim = self.vit.num_features  # 384
        self.head = nn.Linear(self.embed_dim, num_classes)
    
    def forward(self, video):
        B, T, C, H, W = video.shape
        x = video.view(B * T, C, H, W)
        features = self.vit(x)  # [B*T, 384]
        features = features.view(B, T, self.embed_dim)
        pooled = features.mean(dim=1)  # Temporal pooling
        return self.head(pooled)
```

### Checkpoint Saving

```python
torch.save({
    'model': model.state_dict(),
    'classes': train_dataset.classes,
    'acc': best_acc
}, checkpoint_path)
```

---

## ğŸ“ Submission Format

```csv
id,class
0,brush_hair
1,catch
2,clap
...
```

---

## ï¿½ HÆ°á»›ng Cáº£i Tiáº¿n Tiá»m NÄƒng

1. **Thay backbone**: Sá»­ dá»¥ng video-pretrained models
2. **ThÃªm temporal modeling**: LSTM, Transformer layers sau ViT
3. **Augmentation máº¡nh hÆ¡n**: Mixup, CutMix, RandAugment
4. **Regularization**: Label Smoothing, Dropout, DropPath
5. **Training dÃ i hÆ¡n**: 20-40 epochs vá»›i LR scheduler
6. **TTA**: Multi-crop, Multi-scale, Flip ensemble
