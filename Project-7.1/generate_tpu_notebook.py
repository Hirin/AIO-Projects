"""
Generate TPU-optimized VideoMAE notebook with offline augmentation
"""
import json

nb = {
    "cells": [],
    "metadata": {
        "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
        "language_info": {"name": "python", "version": "3.10.0"}
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

def add_markdown(text):
    nb["cells"].append({
        "cell_type": "markdown",
        "metadata": {},
        "source": text if isinstance(text, list) else [text]
    })

def add_code(code):
    nb["cells"].append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "source": code if isinstance(code, list) else [code]
    })

# Title
add_markdown([
    "# VideoMAE TPU Training + Offline Augmentation\n",
    "\n",
    "**TPU v5e-8 Optimized**\n",
    "- Batch size: 32 (t·∫≠n d·ª•ng 330GB RAM)\n",
    "- No AMP (TPU t·ª± optimize bfloat16)\n",
    "- Offline augmentation to balance classes\n",
    "- 2-Stage Training: Mixup ‚Üí Label Smoothing"
])

# Install TPU packages
add_markdown("## 0. TPU Setup")
add_code([
    "# Install required packages\n",
    "!pip install uv -q\n",
    "!uv pip install -q --system transformers accelerate gdown\n",
    "\n",
    "# TPU-specific imports\n",
    "import torch\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "\n",
    "# Get TPU device (use new API)\n",
    "DEVICE = torch_xla.device()\n",
    "print(f'TPU Device: {DEVICE}')"
])

# Imports
add_code([
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from transformers import VideoMAEForVideoClassification\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import random\n",
    "import os\n",
    "import gc\n",
    "import shutil\n",
    "from collections import Counter\n",
    "\n",
    "# Paths\n",
    "PATH_DATA_TRAIN = '/kaggle/input/action-video/data/data_train'\n",
    "PATH_DATA_TEST = '/kaggle/input/action-video/data/test'"
])

# Config - TPU optimized
add_markdown("## 1. Configuration (TPU Optimized)")
add_code([
    "# Model Config\n",
    "MODEL_CKPT = 'MCG-NJU/videomae-base-finetuned-kinetics'\n",
    "NUM_FRAMES = 16\n",
    "IMG_SIZE = 224\n",
    "RESIZE_SIZE = 256\n",
    "\n",
    "# Phase 1 Config (Mixup)\n",
    "EPOCHS_P1 = 30\n",
    "LR_P1 = 5e-5\n",
    "\n",
    "# Phase 2 Config (Label Smoothing)\n",
    "EPOCHS_P2 = 10\n",
    "LR_P2 = 1e-6\n",
    "LABEL_SMOOTHING = 0.1\n",
    "\n",
    "# TPU Optimized Config\n",
    "BATCH_SIZE = 4  # VideoMAE is memory-heavy, use small batch\n",
    "ACCUM_STEPS = 8  # Effective batch = 4 * 8 = 32\n",
    "WEIGHT_DECAY = 0.05\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "# Augmentation\n",
    "MIXUP_ALPHA = 0.8\n",
    "MIXUP_PROB = 1.0\n",
    "\n",
    "# Normalization\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "print(f'Batch size: {BATCH_SIZE}')\n",
    "print(f'Effective batch: {BATCH_SIZE * ACCUM_STEPS}')"
])

# Augmentation functions
add_markdown("## 2. Offline Augmentation Functions")
add_code([
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def get_class_distribution(train_path):\n",
    "    \"\"\"Count samples per class\"\"\"\n",
    "    train_path = Path(train_path)\n",
    "    class_counts = {}\n",
    "    for cls_dir in train_path.iterdir():\n",
    "        if cls_dir.is_dir():\n",
    "            video_count = len([d for d in cls_dir.iterdir() if d.is_dir()])\n",
    "            class_counts[cls_dir.name] = video_count\n",
    "    return class_counts\n",
    "\n",
    "def augment_video_frames(src_dir, dst_dir, transform_type):\n",
    "    \"\"\"Apply augmentation to all frames in a video\"\"\"\n",
    "    dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for frame_path in sorted(src_dir.glob('*.jpg')):\n",
    "        img = Image.open(frame_path)\n",
    "        \n",
    "        if transform_type == 'flip':\n",
    "            img = TF.hflip(img)\n",
    "        elif transform_type == 'rotate_neg':\n",
    "            img = TF.rotate(img, -10, fill=0)\n",
    "        elif transform_type == 'rotate_pos':\n",
    "            img = TF.rotate(img, 10, fill=0)\n",
    "        elif transform_type == 'crop':\n",
    "            w, h = img.size\n",
    "            crop_size = int(min(w, h) * 0.9)\n",
    "            left = (w - crop_size) // 2\n",
    "            top = (h - crop_size) // 2\n",
    "            img = TF.crop(img, top, left, crop_size, crop_size)\n",
    "            img = TF.resize(img, (h, w))\n",
    "        elif transform_type == 'flip_bright':\n",
    "            img = TF.hflip(img)\n",
    "            img = TF.adjust_brightness(img, 1.2)\n",
    "        elif transform_type == 'rotate_crop':\n",
    "            img = TF.rotate(img, -8, fill=0)\n",
    "            w, h = img.size\n",
    "            crop_size = int(min(w, h) * 0.92)\n",
    "            left = (w - crop_size) // 2\n",
    "            top = (h - crop_size) // 2\n",
    "            img = TF.crop(img, top, left, crop_size, crop_size)\n",
    "            img = TF.resize(img, (h, w))\n",
    "        \n",
    "        img.save(dst_dir / frame_path.name, quality=95)\n",
    "    return True\n",
    "\n",
    "def copy_video(src, dst):\n",
    "    \"\"\"Copy video folder\"\"\"\n",
    "    if not dst.exists():\n",
    "        shutil.copytree(src, dst)\n",
    "    return True\n",
    "\n",
    "def balance_dataset(train_path, output_path, target_per_class=None, max_workers=16):\n",
    "    \"\"\"Balance dataset with MULTI-THREADING (16 workers)\"\"\"\n",
    "    train_path = Path(train_path)\n",
    "    output_path = Path(output_path)\n",
    "    \n",
    "    class_counts = get_class_distribution(train_path)\n",
    "    max_count = max(class_counts.values())\n",
    "    target = target_per_class or max_count\n",
    "    \n",
    "    print(f'Target samples per class: {target}')\n",
    "    print(f'Using {max_workers} parallel workers')\n",
    "    aug_types = ['flip', 'rotate_neg', 'rotate_pos', 'crop', 'flip_bright', 'rotate_crop']\n",
    "    \n",
    "    total_created = 0\n",
    "    \n",
    "    for cls_name, count in tqdm(class_counts.items(), desc='Balancing'):\n",
    "        cls_src = train_path / cls_name\n",
    "        cls_dst = output_path / cls_name\n",
    "        cls_dst.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        videos = sorted([d for d in cls_src.iterdir() if d.is_dir()])\n",
    "        \n",
    "        # Step 1: Copy originals in parallel\n",
    "        copy_tasks = [(v, cls_dst / v.name) for v in videos]\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            list(executor.map(lambda x: copy_video(x[0], x[1]), copy_tasks))\n",
    "        \n",
    "        needed = target - count\n",
    "        if needed <= 0:\n",
    "            continue\n",
    "        \n",
    "        # Step 2: Prepare augmentation tasks\n",
    "        aug_tasks = []\n",
    "        aug_idx = 0\n",
    "        created = 0\n",
    "        while created < needed:\n",
    "            for v in videos:\n",
    "                if created >= needed:\n",
    "                    break\n",
    "                aug_type = aug_types[aug_idx % len(aug_types)]\n",
    "                aug_name = f'{v.name}_aug_{aug_type}_{aug_idx // len(aug_types)}'\n",
    "                dst = cls_dst / aug_name\n",
    "                if not dst.exists():\n",
    "                    aug_tasks.append((v, dst, aug_type))\n",
    "                    created += 1\n",
    "            aug_idx += 1\n",
    "        \n",
    "        # Step 3: Run augmentation in parallel\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = [executor.submit(augment_video_frames, src, dst, aug_type) \n",
    "                       for src, dst, aug_type in aug_tasks]\n",
    "            for f in as_completed(futures):\n",
    "                f.result()\n",
    "        \n",
    "        total_created += len(aug_tasks)\n",
    "    \n",
    "    print(f'\\n‚úì Created {total_created} augmented videos')\n",
    "    return output_path\n",
    "\n",
    "print('Augmentation functions defined (multi-threaded, 16 workers)')"
])

# Before augmentation analysis
add_markdown("## 3. Analyze & Run Augmentation")
add_code([
    "# BEFORE augmentation\n",
    "print('=' * 60)\n",
    "print('BEFORE AUGMENTATION')\n",
    "print('=' * 60)\n",
    "\n",
    "before_counts = get_class_distribution(PATH_DATA_TRAIN)\n",
    "print(f'Total samples: {sum(before_counts.values())}')\n",
    "print(f'Max: {max(before_counts.values())}, Min: {min(before_counts.values())}')\n",
    "print(f'Imbalance ratio: {max(before_counts.values()) / min(before_counts.values()):.2f}x')\n",
    "\n",
    "# Plot\n",
    "before_df = pd.DataFrame([{'class': k, 'count': v} for k, v in before_counts.items()]).sort_values('count', ascending=False)\n",
    "plt.figure(figsize=(16, 5))\n",
    "plt.bar(range(len(before_df)), before_df['count'].values, color='steelblue', alpha=0.7)\n",
    "plt.title('BEFORE Augmentation')\n",
    "plt.xticks(range(len(before_df)), before_df['class'].values, rotation=90, fontsize=7)\n",
    "plt.tight_layout()\n",
    "plt.savefig('distribution_before.png', dpi=150)\n",
    "plt.show()"
])

# Run augmentation (with skip check)
add_code([
    "# Run augmentation - SKIP if already done\n",
    "PATH_DATA_AUGMENTED = '/kaggle/working/data_train_augmented'\n",
    "\n",
    "# Check if augmentation already completed\n",
    "aug_path = Path(PATH_DATA_AUGMENTED)\n",
    "if aug_path.exists() and len(list(aug_path.iterdir())) >= 50:\n",
    "    print('‚úì Augmentation already done, SKIPPING...')\n",
    "    PATH_DATA_TRAIN = str(aug_path)\n",
    "else:\n",
    "    print('Running augmentation...')\n",
    "    balanced_path = balance_dataset(PATH_DATA_TRAIN, PATH_DATA_AUGMENTED)\n",
    "    PATH_DATA_TRAIN = str(balanced_path)\n",
    "\n",
    "# AFTER augmentation\n",
    "print('\\n' + '=' * 60)\n",
    "print('AFTER AUGMENTATION')\n",
    "print('=' * 60)\n",
    "\n",
    "after_counts = get_class_distribution(PATH_DATA_TRAIN)\n",
    "print(f'Total samples: {sum(after_counts.values())}')\n",
    "print(f'Imbalance ratio: {max(after_counts.values()) / min(after_counts.values()):.2f}x')\n",
    "\n",
    "# Plot\n",
    "after_df = pd.DataFrame([{'class': k, 'count': v} for k, v in after_counts.items()]).sort_values('count', ascending=False)\n",
    "plt.figure(figsize=(16, 5))\n",
    "plt.bar(range(len(after_df)), after_df['count'].values, color='darkgreen', alpha=0.7)\n",
    "plt.title('AFTER Augmentation (Balanced)')\n",
    "plt.xticks(range(len(after_df)), after_df['class'].values, rotation=90, fontsize=7)\n",
    "plt.tight_layout()\n",
    "plt.savefig('distribution_after.png', dpi=150)\n",
    "plt.show()"
])

# Augmentation demo
add_code([
    "# Augmentation Demo - Show sample transformations\n",
    "def show_augmentation_demo(train_path, class_name=None, num_samples=2):\n",
    "    \"\"\"Show original vs augmented samples\"\"\"\n",
    "    train_path = Path(train_path)\n",
    "    \n",
    "    # Find a class with augmented samples\n",
    "    if class_name is None:\n",
    "        for cls_dir in train_path.iterdir():\n",
    "            if cls_dir.is_dir():\n",
    "                aug_videos = [v for v in cls_dir.iterdir() if '_aug_' in v.name]\n",
    "                if len(aug_videos) > 5:\n",
    "                    class_name = cls_dir.name\n",
    "                    break\n",
    "    \n",
    "    cls_dir = train_path / class_name\n",
    "    videos = sorted([v for v in cls_dir.iterdir() if v.is_dir()])\n",
    "    \n",
    "    # Find originals and their augmented versions\n",
    "    originals = [v for v in videos if '_aug_' not in v.name][:num_samples]\n",
    "    \n",
    "    aug_types = ['flip', 'rotate_neg', 'crop', 'flip_bright', 'rotate_crop']\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 6, figsize=(18, 3*num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for row, orig in enumerate(originals):\n",
    "        # Show original\n",
    "        frame = sorted(orig.glob('*.jpg'))[0]\n",
    "        axes[row, 0].imshow(Image.open(frame))\n",
    "        axes[row, 0].set_title('Original', fontsize=9)\n",
    "        axes[row, 0].axis('off')\n",
    "        \n",
    "        # Show augmented versions\n",
    "        for col, aug_type in enumerate(aug_types, 1):\n",
    "            aug_name = f'{orig.name}_aug_{aug_type}_0'\n",
    "            aug_dir = cls_dir / aug_name\n",
    "            \n",
    "            if aug_dir.exists():\n",
    "                frame = sorted(aug_dir.glob('*.jpg'))[0]\n",
    "                axes[row, col].imshow(Image.open(frame))\n",
    "            else:\n",
    "                axes[row, col].text(0.5, 0.5, 'N/A', ha='center', va='center')\n",
    "            \n",
    "            axes[row, col].set_title(aug_type.replace('_', ' ').title(), fontsize=9)\n",
    "            axes[row, col].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Augmentation Demo: {class_name}', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('augmentation_demo.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Show demo\n",
    "show_augmentation_demo(PATH_DATA_TRAIN, num_samples=2)"
])

# Dataset classes
add_markdown("## 4. Dataset Classes")
add_code([
    "class MixupCollate:\n",
    "    def __init__(self, num_classes, alpha=0.8, prob=1.0):\n",
    "        self.num_classes = num_classes\n",
    "        self.alpha = alpha\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        inputs, targets = torch.utils.data.default_collate(batch)\n",
    "        if np.random.rand() > self.prob:\n",
    "            return inputs, F.one_hot(targets, num_classes=self.num_classes).float()\n",
    "        batch_size = inputs.size(0)\n",
    "        index = torch.randperm(batch_size)\n",
    "        lam = np.random.beta(self.alpha, self.alpha)\n",
    "        inputs = lam * inputs + (1 - lam) * inputs[index, :]\n",
    "        targets_one_hot = F.one_hot(targets, num_classes=self.num_classes).float()\n",
    "        targets = lam * targets_one_hot + (1 - lam) * targets_one_hot[index, :]\n",
    "        return inputs, targets\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, root, num_frames=16, is_train=True):\n",
    "        self.root = Path(root)\n",
    "        self.num_frames = num_frames\n",
    "        self.is_train = is_train\n",
    "        self.classes = sorted([d.name for d in self.root.iterdir() if d.is_dir()])\n",
    "        self.class_to_idx = {c: i for i, c in enumerate(self.classes)}\n",
    "        self.samples = []\n",
    "        for cls in self.classes:\n",
    "            cls_dir = self.root / cls\n",
    "            for video_dir in sorted([d for d in cls_dir.iterdir() if d.is_dir()]):\n",
    "                self.samples.append((video_dir, self.class_to_idx[cls]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_dir, label = self.samples[idx]\n",
    "        frame_paths = sorted(video_dir.glob('*.jpg'))\n",
    "        indices = np.linspace(0, len(frame_paths) - 1, self.num_frames, dtype=int)\n",
    "        frames = [TF.resize(Image.open(frame_paths[i]).convert('RGB'), RESIZE_SIZE) for i in indices]\n",
    "        \n",
    "        if self.is_train:\n",
    "            i, j, h, w = T.RandomResizedCrop.get_params(frames[0], (0.8, 1.0), (0.75, 1.33))\n",
    "            do_flip = random.random() > 0.5\n",
    "            processed = []\n",
    "            for img in frames:\n",
    "                img = TF.resized_crop(img, i, j, h, w, (IMG_SIZE, IMG_SIZE))\n",
    "                if do_flip:\n",
    "                    img = TF.hflip(img)\n",
    "                img = TF.normalize(TF.to_tensor(img), MEAN, STD)\n",
    "                processed.append(img)\n",
    "        else:\n",
    "            processed = [TF.normalize(TF.to_tensor(TF.center_crop(img, IMG_SIZE)), MEAN, STD) for img in frames]\n",
    "        \n",
    "        return torch.stack(processed), label\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, root, num_frames=16):\n",
    "        self.root = Path(root)\n",
    "        self.num_frames = num_frames\n",
    "        self.samples = sorted([(d, int(d.name)) for d in self.root.iterdir() if d.is_dir()], key=lambda x: x[1])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video_dir, video_id = self.samples[idx]\n",
    "        frame_paths = sorted(video_dir.glob('*.jpg'))\n",
    "        indices = np.linspace(0, len(frame_paths) - 1, self.num_frames, dtype=int)\n",
    "        frames = [TF.resize(Image.open(frame_paths[i]).convert('RGB'), RESIZE_SIZE) for i in indices]\n",
    "        processed = [TF.normalize(TF.to_tensor(TF.center_crop(img, IMG_SIZE)), MEAN, STD) for img in frames]\n",
    "        return torch.stack(processed), video_id\n",
    "\n",
    "print('Dataset classes defined')"
])

# Load data
add_markdown("## 5. Load Data & Model")
add_code([
    "# Download test labels\n",
    "!gdown \"1Xv2CWOqdBj3kt0rkNJKRsodSIEd3-wX_\" -O test_labels.csv -q\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = VideoDataset(PATH_DATA_TRAIN, NUM_FRAMES, is_train=True)\n",
    "test_dataset = TestDataset(PATH_DATA_TEST, NUM_FRAMES)\n",
    "\n",
    "gt_df = pd.read_csv('test_labels.csv')\n",
    "gt_dict = dict(zip(gt_df['id'].astype(str), gt_df['class']))\n",
    "\n",
    "print(f'Train samples: {len(train_dataset)}')\n",
    "print(f'Test samples: {len(test_dataset)}')\n",
    "print(f'Classes: {len(train_dataset.classes)}')\n",
    "\n",
    "# DataLoaders - TPU: num_workers=0\n",
    "mixup_collate = MixupCollate(len(train_dataset.classes), MIXUP_ALPHA, MIXUP_PROB)\n",
    "train_loader_p1 = DataLoader(train_dataset, BATCH_SIZE, shuffle=True, num_workers=0, collate_fn=mixup_collate, drop_last=True)\n",
    "train_loader_p2 = DataLoader(train_dataset, BATCH_SIZE, shuffle=True, num_workers=0, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, BATCH_SIZE, shuffle=False, num_workers=0)"
])

add_code([
    "# Load model to TPU\n",
    "model = VideoMAEForVideoClassification.from_pretrained(\n",
    "    MODEL_CKPT,\n",
    "    num_labels=len(train_dataset.classes),\n",
    "    ignore_mismatched_sizes=True,\n",
    "    num_frames=NUM_FRAMES\n",
    ").to(DEVICE)\n",
    "\n",
    "print('Model loaded to TPU')"
])

# Training functions - TPU version
add_markdown("## 6. TPU Training Functions")
add_code([
    "def train_epoch_tpu(model, loader, optimizer, scheduler, use_mixup=True, label_smoothing=0.0):\n",
    "    \"\"\"TPU-optimized training loop - NO AMP\"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "    \n",
    "    # Wrap with ParallelLoader\n",
    "    para_loader = pl.ParallelLoader(loader, [DEVICE])\n",
    "    pbar = tqdm(para_loader.per_device_loader(DEVICE), desc='Training', leave=False)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for step, (inputs, targets) in enumerate(pbar):\n",
    "        # Data already on TPU via ParallelLoader\n",
    "        logits = model(inputs).logits\n",
    "        \n",
    "        if use_mixup:\n",
    "            # Stabilized mixup loss\n",
    "            log_probs = F.log_softmax(logits, dim=1).clamp(min=-100)\n",
    "            loss = -(targets * log_probs).sum(dim=1).mean()\n",
    "            true_labels = targets.argmax(dim=1)\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, targets, label_smoothing=label_smoothing)\n",
    "            true_labels = targets\n",
    "        \n",
    "        total_correct += (logits.argmax(1) == true_labels).sum().item()\n",
    "        total_samples += inputs.size(0)\n",
    "        \n",
    "        (loss / ACCUM_STEPS).backward()\n",
    "        \n",
    "        if (step + 1) % ACCUM_STEPS == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            xm.optimizer_step(optimizer)  # TPU optimizer step\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            xm.mark_step()  # TPU sync\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{total_loss/(step+1):.4f}', 'acc': f'{total_correct/total_samples:.4f}'})\n",
    "    \n",
    "    return total_loss / len(loader), total_correct / total_samples\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_tpu(model, loader, classes, gt_dict):\n",
    "    \"\"\"TPU evaluation\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    para_loader = pl.ParallelLoader(loader, [DEVICE])\n",
    "    for videos, video_ids in tqdm(para_loader.per_device_loader(DEVICE), desc='Evaluating', leave=False):\n",
    "        preds = model(videos).logits.argmax(1).cpu().tolist()\n",
    "        predictions.extend(zip(video_ids.tolist(), preds))\n",
    "        xm.mark_step()\n",
    "    \n",
    "    y_true = [gt_dict[str(vid)] for vid, _ in predictions]\n",
    "    y_pred = [classes[p] for _, p in predictions]\n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "print('TPU training functions defined')"
])

# Memory cleanup before training
add_code([
    "# Clear memory before training\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# Clear any cached tensors\n",
    "if 'train_dataset' in dir():\n",
    "    # Force clear image cache if any\n",
    "    pass\n",
    "\n",
    "# TPU memory sync\n",
    "import torch_xla.core.xla_model as xm\n",
    "xm.mark_step()\n",
    "\n",
    "print('Memory cleared')\n",
    "print(f'Dataset size: {len(train_dataset)} samples')\n",
    "print(f'Batch size: {BATCH_SIZE}, Accum steps: {ACCUM_STEPS}')\n",
    "print(f'Effective batch: {BATCH_SIZE * ACCUM_STEPS}')"
])

# Training loop
add_markdown("## 7. Training Loop")
add_code([
    "# Initialize\n",
    "history = []\n",
    "best_acc = 0.0\n",
    "\n",
    "# Phase 1: Mixup\n",
    "print('=' * 60)\n",
    "print(f'PHASE 1: Mixup Training (Epochs: {EPOCHS_P1}, LR: {LR_P1})')\n",
    "print('=' * 60)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR_P1, weight_decay=WEIGHT_DECAY)\n",
    "total_steps = len(train_loader_p1) * EPOCHS_P1 // ACCUM_STEPS\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, int(total_steps * WARMUP_RATIO), total_steps)\n",
    "\n",
    "for epoch in range(1, EPOCHS_P1 + 1):\n",
    "    loss, train_acc = train_epoch_tpu(model, train_loader_p1, optimizer, scheduler, use_mixup=True)\n",
    "    test_acc = evaluate_tpu(model, test_loader, train_dataset.classes, gt_dict)\n",
    "    \n",
    "    history.append({'epoch': epoch, 'phase': 1, 'loss': loss, 'train_acc': train_acc, 'test_acc': test_acc})\n",
    "    \n",
    "    status = '>>> BEST' if test_acc > best_acc else ''\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        xm.save(model.state_dict(), 'best_p1.pt')  # TPU save\n",
    "    print(f'Ep {epoch}/{EPOCHS_P1}: L={loss:.4f} TrAcc={train_acc:.4f} TeAcc={test_acc:.4f} {status}')\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "# Phase 2: Label Smoothing\n",
    "print('\\n' + '=' * 60)\n",
    "print(f'PHASE 2: Label Smoothing (Epochs: {EPOCHS_P2}, LR: {LR_P2})')\n",
    "print('=' * 60)\n",
    "\n",
    "model.load_state_dict(torch.load('best_p1.pt'))\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR_P2, weight_decay=WEIGHT_DECAY)\n",
    "total_steps = len(train_loader_p2) * EPOCHS_P2 // ACCUM_STEPS\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, int(total_steps * WARMUP_RATIO), total_steps)\n",
    "\n",
    "for epoch in range(1, EPOCHS_P2 + 1):\n",
    "    loss, train_acc = train_epoch_tpu(model, train_loader_p2, optimizer, scheduler, use_mixup=False, label_smoothing=LABEL_SMOOTHING)\n",
    "    test_acc = evaluate_tpu(model, test_loader, train_dataset.classes, gt_dict)\n",
    "    \n",
    "    history.append({'epoch': EPOCHS_P1 + epoch, 'phase': 2, 'loss': loss, 'train_acc': train_acc, 'test_acc': test_acc})\n",
    "    \n",
    "    status = '>>> BEST' if test_acc > best_acc else ''\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        xm.save(model.state_dict(), 'best_final.pt')\n",
    "    print(f'P2 Ep {epoch}/{EPOCHS_P2}: L={loss:.4f} TrAcc={train_acc:.4f} TeAcc={test_acc:.4f} {status}')\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "# Save history\n",
    "df_history = pd.DataFrame(history)\n",
    "df_history.to_csv('training_history.csv', index=False)\n",
    "print(f'\\nTraining Complete! Best Test Acc: {best_acc:.4f}')"
])

# Plot curves
add_markdown("## 8. Training Curves")
add_code([
    "df = pd.read_csv('training_history.csv')\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(df['epoch'], df['test_acc'], 'b-o', markersize=4, label='Test Acc')\n",
    "axes[0].axvline(x=EPOCHS_P1, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0].set_title('Test Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(df['epoch'], df['train_acc'], 'g-s', markersize=4, label='Train Acc')\n",
    "axes[1].axvline(x=EPOCHS_P1, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[1].set_title('Train Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(df['epoch'], df['loss'], 'r-^', markersize=4, label='Loss')\n",
    "axes[2].axvline(x=EPOCHS_P1, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[2].set_title('Loss')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150)\n",
    "plt.show()"
])

# Per-class analysis
add_markdown("## 9. Per-Class Analysis")
add_code([
    "# Load best model\n",
    "if os.path.exists('best_final.pt'):\n",
    "    model.load_state_dict(torch.load('best_final.pt'))\n",
    "elif os.path.exists('best_p1.pt'):\n",
    "    model.load_state_dict(torch.load('best_p1.pt'))\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# Get predictions\n",
    "all_preds, all_true = [], []\n",
    "para_loader = pl.ParallelLoader(test_loader, [DEVICE])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for videos, video_ids in tqdm(para_loader.per_device_loader(DEVICE), desc='Final Eval'):\n",
    "        preds = model(videos).logits.argmax(1).cpu().tolist()\n",
    "        for vid, pred in zip(video_ids.tolist(), preds):\n",
    "            all_true.append(gt_dict[str(vid)])\n",
    "            all_preds.append(train_dataset.classes[pred])\n",
    "        xm.mark_step()\n",
    "\n",
    "overall_acc = accuracy_score(all_true, all_preds)\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'OVERALL TEST ACCURACY: {overall_acc:.4f} ({overall_acc*100:.2f}%)')\n",
    "print(f'{\"=\"*60}')\n",
    "\n",
    "# Classification report\n",
    "report = classification_report(all_true, all_preds, target_names=train_dataset.classes, digits=4, zero_division=0)\n",
    "print(report)\n",
    "\n",
    "with open('per_class_report.txt', 'w') as f:\n",
    "    f.write(f'Overall Accuracy: {overall_acc:.4f}\\n\\n')\n",
    "    f.write(report)\n",
    "print('‚úì Saved per_class_report.txt')"
])

# Per-class plot
add_code([
    "# Per-class accuracy plot\n",
    "report_dict = classification_report(all_true, all_preds, target_names=train_dataset.classes, output_dict=True, zero_division=0)\n",
    "class_accs = [(cls, report_dict[cls]['recall']) for cls in train_dataset.classes]\n",
    "class_accs_sorted = sorted(class_accs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "cls_names = [c[0] for c in class_accs_sorted]\n",
    "cls_accs_vals = [c[1] * 100 for c in class_accs_sorted]\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "colors = ['darkgreen' if acc >= 90 else 'orange' if acc >= 70 else 'darkred' for acc in cls_accs_vals]\n",
    "plt.bar(range(len(cls_names)), cls_accs_vals, color=colors, alpha=0.7)\n",
    "plt.axhline(y=overall_acc*100, color='blue', linestyle='--', linewidth=2, label=f'Overall ({overall_acc*100:.2f}%)')\n",
    "plt.xlabel('Action Category')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Per-Class Test Accuracy')\n",
    "plt.xticks(range(len(cls_names)), cls_names, rotation=90, fontsize=8)\n",
    "plt.ylim(0, 105)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('per_class_accuracy.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nüèÜ Best: {cls_names[0]} ({cls_accs_vals[0]:.2f}%)')\n",
    "print(f'‚ö†Ô∏è  Worst: {cls_names[-1]} ({cls_accs_vals[-1]:.2f}%)')"
])

# Save notebook
output_path = "/mnt/hdd/Learning/AIO-Projects/Project-7.1/videoMAE-TPU.ipynb"
with open(output_path, 'w') as f:
    json.dump(nb, f, indent=2)

print(f"\n‚úÖ Created: {output_path}")
print("\nTPU Notebook includes:")
print("  - TPU v5e-8 optimized (batch_size=32, no AMP)")
print("  - Offline augmentation (6 variations)")
print("  - Before/After distribution plots")
print("  - 2-Stage training: Mixup ‚Üí Label Smoothing")
print("  - Stabilized mixup loss (no NaN)")
print("  - Per-class analysis")
print("  - Training curves")
