{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VideoMAE Ablation Study - Dual GPU (T4 x2)\n",
    "\n",
    "**8 Experiments, 10 epochs each, 2 GPUs running in parallel**\n",
    "\n",
    "| GPU 0 | GPU 1 |\n",
    "|-------|-------|\n",
    "| Exp 0: ViT Baseline | Exp 1: VideoMAE Paper |\n",
    "| Exp 2: Multi-Seg TTA | Exp 3: Consistent Transform |\n",
    "| Exp 4: Mixup | Exp 5: Label Smoothing |\n",
    "| Exp 6: 2-Stage | Exp 7: Flip TTA |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Imports & Setup\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from transformers import VideoMAEForVideoClassification, VideoMAEImageProcessor\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import timm\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPUs\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "print(f\"Available GPUs: {NUM_GPUS}\")\n",
    "for i in range(NUM_GPUS):\n",
    "    print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Configuration\n",
    "\n",
    "PATH_DATA_TRAIN = Path('/kaggle/input/action-video/data/data_train')\n",
    "PATH_DATA_TEST = Path('/kaggle/input/action-video/data/test')\n",
    "\n",
    "MODEL_CKPT = \"MCG-NJU/videomae-base-finetuned-kinetics\"\n",
    "VIT_CKPT = \"vit_small_patch16_224\"\n",
    "\n",
    "NUM_FRAMES = 16\n",
    "IMAGE_SIZE = 224\n",
    "RESIZE_SIZE = 256\n",
    "BATCH_SIZE = 16\n",
    "GRAD_ACCUM_STEPS = 2\n",
    "EPOCHS = 10  # Increased from 4\n",
    "WEIGHT_DECAY = 0.05\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "# LR scaling\n",
    "LR = 5e-5  # Fine-tuning LR\n",
    "\n",
    "processor = VideoMAEImageProcessor.from_pretrained(MODEL_CKPT)\n",
    "MEAN = processor.image_mean\n",
    "STD = processor.image_std\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. ViT Baseline Model\n",
    "\n",
    "class LightweightViTForAction(nn.Module):\n",
    "    \"\"\"ViT baseline model (same as baseline-btc.ipynb).\"\"\"\n",
    "    def __init__(self, num_classes=51):\n",
    "        super().__init__()\n",
    "        self.vit = timm.create_model(VIT_CKPT, pretrained=True, num_classes=0)\n",
    "        self.embed_dim = self.vit.num_features\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes)\n",
    "    \n",
    "    def forward(self, video):\n",
    "        B, T, C, H, W = video.shape\n",
    "        x = video.view(B * T, C, H, W)\n",
    "        features = self.vit(x)  # [B*T, embed_dim]\n",
    "        features = features.view(B, T, -1)\n",
    "        features = features.mean(dim=1)  # Temporal pooling\n",
    "        return self.head(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Transform Classes\n",
    "\n",
    "class VideoTransformBaseline:\n",
    "    def __init__(self, is_train=True):\n",
    "        self.is_train = is_train\n",
    "    \n",
    "    def __call__(self, frames):\n",
    "        if self.is_train:\n",
    "            h, w = frames.shape[-2:]\n",
    "            scale = random.uniform(0.8, 1.0)\n",
    "            new_h, new_w = int(h * scale), int(w * scale)\n",
    "            frames = TF.resize(frames, [new_h, new_w])\n",
    "            i = random.randint(0, max(0, new_h - IMAGE_SIZE))\n",
    "            j = random.randint(0, max(0, new_w - IMAGE_SIZE))\n",
    "            frames = TF.crop(frames, i, j, min(IMAGE_SIZE, new_h), min(IMAGE_SIZE, new_w))\n",
    "            frames = TF.resize(frames, [IMAGE_SIZE, IMAGE_SIZE])\n",
    "            if random.random() < 0.5:\n",
    "                frames = TF.hflip(frames)\n",
    "        else:\n",
    "            frames = TF.resize(frames, [IMAGE_SIZE, IMAGE_SIZE])\n",
    "        return torch.stack([TF.normalize(f, MEAN, STD) for f in frames])\n",
    "\n",
    "\n",
    "class VideoTransformConsistent:\n",
    "    def __init__(self, is_train=True):\n",
    "        self.is_train = is_train\n",
    "    \n",
    "    def __call__(self, frames):\n",
    "        frames = [TF.resize(img, RESIZE_SIZE) for img in frames]\n",
    "        if self.is_train:\n",
    "            i, j, h, w = T.RandomResizedCrop.get_params(frames[0], scale=(0.8, 1.0), ratio=(0.75, 1.33))\n",
    "            is_flip = random.random() > 0.5\n",
    "            transformed = []\n",
    "            for img in frames:\n",
    "                img = TF.resized_crop(img, i, j, h, w, size=(IMAGE_SIZE, IMAGE_SIZE))\n",
    "                if is_flip:\n",
    "                    img = TF.hflip(img)\n",
    "                img = TF.to_tensor(img)\n",
    "                img = TF.normalize(img, MEAN, STD)\n",
    "                transformed.append(img)\n",
    "            return torch.stack(transformed)\n",
    "        else:\n",
    "            return torch.stack([TF.normalize(TF.to_tensor(TF.center_crop(img, IMAGE_SIZE)), MEAN, STD) for img in frames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Dataset Classes\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, root, transform, use_pil=False):\n",
    "        self.root = Path(root)\n",
    "        self.transform = transform\n",
    "        self.use_pil = use_pil\n",
    "        self.classes = sorted([d.name for d in self.root.iterdir() if d.is_dir()])\n",
    "        self.class_to_idx = {c: i for i, c in enumerate(self.classes)}\n",
    "        self.samples = []\n",
    "        for cls in self.classes:\n",
    "            for vid in (self.root / cls).iterdir():\n",
    "                if vid.is_dir():\n",
    "                    self.samples.append((vid, self.class_to_idx[cls]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        vid_dir, label = self.samples[idx]\n",
    "        files = sorted(vid_dir.glob('*.jpg'))\n",
    "        indices = torch.linspace(0, len(files)-1, NUM_FRAMES).long()\n",
    "        if self.use_pil:\n",
    "            frames = [Image.open(files[i]).convert('RGB') for i in indices]\n",
    "        else:\n",
    "            frames = torch.stack([TF.to_tensor(Image.open(files[i]).convert('RGB')) for i in indices])\n",
    "        return self.transform(frames), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Test Dataset Classes (same as before)\n",
    "\n",
    "class TestDatasetSingle(Dataset):\n",
    "    def __init__(self, root):\n",
    "        self.root = Path(root)\n",
    "        self.samples = [(d, int(d.name)) for d in self.root.iterdir() if d.is_dir()]\n",
    "        self.samples.sort(key=lambda x: x[1])\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        vid_dir, vid_id = self.samples[idx]\n",
    "        files = sorted(vid_dir.glob('*.jpg'))\n",
    "        indices = torch.linspace(0, len(files)-1, NUM_FRAMES).long()\n",
    "        frames = [TF.normalize(TF.to_tensor(TF.resize(TF.center_crop(TF.resize(Image.open(files[i]).convert('RGB'), RESIZE_SIZE), IMAGE_SIZE), [IMAGE_SIZE, IMAGE_SIZE])), MEAN, STD) for i in indices]\n",
    "        return torch.stack(frames), vid_id\n",
    "\n",
    "class TestDatasetMultiSegment(Dataset):\n",
    "    def __init__(self, root, num_segments=2):\n",
    "        self.root, self.num_segments = Path(root), num_segments\n",
    "        self.samples = [(d, int(d.name)) for d in self.root.iterdir() if d.is_dir()]\n",
    "        self.samples.sort(key=lambda x: x[1])\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        vid_dir, vid_id = self.samples[idx]\n",
    "        files = sorted(vid_dir.glob('*.jpg'))\n",
    "        total, views = len(files), []\n",
    "        for seg in range(self.num_segments):\n",
    "            start, end = (total // self.num_segments) * seg, min((total // self.num_segments) * (seg + 1), total)\n",
    "            indices = torch.linspace(start, max(start, end-1), NUM_FRAMES).long()\n",
    "            frames = [TF.resize(Image.open(files[i]).convert('RGB'), RESIZE_SIZE) for i in indices]\n",
    "            w, h = frames[0].size\n",
    "            for top, left in [(0, 0), ((h-IMAGE_SIZE)//2, (w-IMAGE_SIZE)//2), (max(0,h-IMAGE_SIZE), max(0,w-IMAGE_SIZE))]:\n",
    "                views.append(torch.stack([TF.normalize(TF.to_tensor(TF.crop(img, top, left, IMAGE_SIZE, IMAGE_SIZE)), MEAN, STD) for img in frames]))\n",
    "        return torch.stack(views), vid_id\n",
    "\n",
    "class TestDatasetFlipTTA(Dataset):\n",
    "    def __init__(self, root):\n",
    "        self.root = Path(root)\n",
    "        self.samples = [(d, int(d.name)) for d in self.root.iterdir() if d.is_dir()]\n",
    "        self.samples.sort(key=lambda x: x[1])\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        vid_dir, vid_id = self.samples[idx]\n",
    "        files = sorted(vid_dir.glob('*.jpg'))\n",
    "        indices = torch.linspace(0, len(files)-1, NUM_FRAMES).long()\n",
    "        frames = [TF.resize(Image.open(files[i]).convert('RGB'), RESIZE_SIZE) for i in indices]\n",
    "        w, h = frames[0].size\n",
    "        views = []\n",
    "        for top, left in [((h-IMAGE_SIZE)//2, (w-IMAGE_SIZE)//2), (0, (w-IMAGE_SIZE)//2), (max(0,h-IMAGE_SIZE), (w-IMAGE_SIZE)//2)]:\n",
    "            views.append(torch.stack([TF.normalize(TF.to_tensor(TF.crop(img, top, left, IMAGE_SIZE, IMAGE_SIZE)), MEAN, STD) for img in frames]))\n",
    "            views.append(torch.stack([TF.normalize(TF.to_tensor(TF.hflip(TF.crop(img, top, left, IMAGE_SIZE, IMAGE_SIZE))), MEAN, STD) for img in frames]))\n",
    "        return torch.stack(views), vid_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. Mixup Collate\n",
    "\n",
    "class MixupCollate:\n",
    "    def __init__(self, num_classes, alpha=0.8):\n",
    "        self.num_classes, self.alpha = num_classes, alpha\n",
    "    def __call__(self, batch):\n",
    "        inputs, targets = torch.utils.data.default_collate(batch)\n",
    "        lam = np.random.beta(self.alpha, self.alpha)\n",
    "        idx = torch.randperm(inputs.size(0))\n",
    "        inputs = lam * inputs + (1 - lam) * inputs[idx]\n",
    "        onehot = F.one_hot(targets, self.num_classes).float()\n",
    "        return inputs, lam * onehot + (1 - lam) * onehot[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. Training Functions\n",
    "\n",
    "def train_epoch(model, loader, optimizer, scheduler, scaler, device, use_mixup=False, label_smoothing=0.0, is_vit=False):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    optimizer.zero_grad()\n",
    "    for batch_idx, (videos, targets) in enumerate(loader):\n",
    "        videos, targets = videos.to(device), targets.to(device)\n",
    "        with torch.amp.autocast(device_type='cuda'):\n",
    "            if is_vit:\n",
    "                logits = model(videos)\n",
    "            else:\n",
    "                logits = model(videos).logits\n",
    "            if use_mixup:\n",
    "                loss = -torch.sum(targets * F.log_softmax(logits, dim=1), dim=1).mean()\n",
    "                true_labels = targets.argmax(dim=1)\n",
    "            else:\n",
    "                loss = F.cross_entropy(logits, targets, label_smoothing=label_smoothing)\n",
    "                true_labels = targets\n",
    "        correct += (logits.argmax(dim=1) == true_labels).sum().item()\n",
    "        total += true_labels.size(0)\n",
    "        total_loss += loss.item() * true_labels.size(0)\n",
    "        scaler.scale(loss / GRAD_ACCUM_STEPS).backward()\n",
    "        if (batch_idx + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "def evaluate(model, loader, device, multi_view=False, id2label=None, is_vit=False):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for data, vid_ids in loader:\n",
    "            if multi_view:\n",
    "                B, V, T, C, H, W = data.shape\n",
    "                data = data.view(B * V, T, C, H, W).to(device)\n",
    "                logits = model(data) if is_vit else model(data).logits\n",
    "                logits = logits.view(B, V, -1).mean(dim=1)\n",
    "            else:\n",
    "                data = data.to(device)\n",
    "                logits = model(data) if is_vit else model(data).logits\n",
    "            for vid, pred in zip(vid_ids.tolist(), logits.argmax(dim=1).tolist()):\n",
    "                predictions.append((vid, id2label[pred]))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9. Load Test Labels\n",
    "\n",
    "!gdown \"1Xv2CWOqdBj3kt0rkNJKRsodSIEd3-wX_\" -O test_labels.csv -q\n",
    "gt_df = pd.read_csv(\"test_labels.csv\")\n",
    "GT_LABELS = dict(zip(gt_df['id'].astype(str), gt_df['class']))\n",
    "\n",
    "def calc_accuracy(predictions):\n",
    "    y_pred, y_true = [], []\n",
    "    for vid_id, pred_cls in predictions:\n",
    "        if str(vid_id) in GT_LABELS:\n",
    "            y_pred.append(pred_cls)\n",
    "            y_true.append(GT_LABELS[str(vid_id)])\n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Loaded {len(GT_LABELS)} test labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10. Single GPU Experiment Runner (with per-epoch test eval)\n\ndef run_single_experiment(exp_config, gpu_id, results_queue):\n    \"\"\"Run a single experiment on specified GPU with per-epoch test eval.\"\"\"\n    import matplotlib.pyplot as plt\n    \n    torch.cuda.set_device(gpu_id)\n    device = torch.device(f'cuda:{gpu_id}')\n    \n    exp_name = exp_config['name']\n    print(f\"[GPU {gpu_id}] Starting: {exp_name}\")\n    \n    random.seed(42)\n    np.random.seed(42)\n    torch.manual_seed(42)\n    \n    # Create model\n    if exp_config.get('is_vit', False):\n        model = LightweightViTForAction(num_classes=51).to(device)\n        is_vit = True\n    else:\n        label2id = exp_config['train_ds'].class_to_idx\n        id2label = {v: k for k, v in label2id.items()}\n        model = VideoMAEForVideoClassification.from_pretrained(\n            MODEL_CKPT, label2id=label2id, id2label=id2label,\n            ignore_mismatched_sizes=True, num_frames=NUM_FRAMES\n        ).to(device)\n        is_vit = False\n    \n    train_ds = exp_config['train_ds']\n    test_ds = exp_config['test_ds']\n    label2id = train_ds.class_to_idx\n    id2label = {v: k for k, v in label2id.items()}\n    \n    # DataLoaders\n    collate_fn = exp_config.get('mixup_collate', None)\n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True, drop_last=True, collate_fn=collate_fn)\n    test_loader = DataLoader(test_ds, batch_size=4 if exp_config.get('multi_view') else BATCH_SIZE, shuffle=False, num_workers=2)\n    \n    # Optimizer & Scheduler\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n    scaler = torch.amp.GradScaler()\n    num_steps = len(train_loader) * EPOCHS // GRAD_ACCUM_STEPS\n    scheduler = get_cosine_schedule_with_warmup(optimizer, int(num_steps * WARMUP_RATIO), num_steps)\n    \n    # Training history\n    history = {'epoch': [], 'loss_train': [], 'acc_train': [], 'acc_test': []}\n    best_acc = 0.0\n    \n    for epoch in range(EPOCHS):\n        # Train\n        loss, acc = train_epoch(\n            model, train_loader, optimizer, scheduler, scaler, device,\n            use_mixup=exp_config.get('use_mixup', False),\n            label_smoothing=exp_config.get('label_smoothing', 0.0),\n            is_vit=is_vit\n        )\n        \n        # Evaluate on test\n        predictions = evaluate(model, test_loader, device, \n                               multi_view=exp_config.get('multi_view', False), \n                               id2label=id2label, is_vit=is_vit)\n        test_acc = calc_accuracy(predictions)\n        \n        print(f\"[GPU {gpu_id}] {exp_name} Epoch {epoch+1}/{EPOCHS}: Loss_train={loss:.4f}, Acc_train={acc:.4f}, Acc_test={test_acc:.4f}\")\n        \n        history['epoch'].append(epoch + 1)\n        history['loss_train'].append(loss)\n        history['acc_train'].append(acc)\n        history['acc_test'].append(test_acc)\n        \n        if acc > best_acc:\n            best_acc = acc\n            torch.save(model.state_dict(), f'{exp_name}_best.pt')\n    \n    # 2-Stage Phase 2\n    if exp_config.get('two_stage', False):\n        print(f\"[GPU {gpu_id}] {exp_name} Phase 2...\")\n        model.load_state_dict(torch.load(f'{exp_name}_best.pt'))\n        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6, weight_decay=WEIGHT_DECAY)\n        p2_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n        scheduler = get_cosine_schedule_with_warmup(optimizer, 0, len(p2_loader) * 3 // GRAD_ACCUM_STEPS)\n        for epoch in range(3):\n            loss, acc = train_epoch(model, p2_loader, optimizer, scheduler, scaler, device, label_smoothing=0.1, is_vit=is_vit)\n            predictions = evaluate(model, test_loader, device, multi_view=exp_config.get('multi_view', False), id2label=id2label, is_vit=is_vit)\n            test_acc = calc_accuracy(predictions)\n            print(f\"[GPU {gpu_id}] {exp_name} P2 Epoch {epoch+1}/3: Loss_train={loss:.4f}, Acc_train={acc:.4f}, Acc_test={test_acc:.4f}\")\n            history['epoch'].append(EPOCHS + epoch + 1)\n            history['loss_train'].append(loss)\n            history['acc_train'].append(acc)\n            history['acc_test'].append(test_acc)\n            best_acc = max(best_acc, acc)\n    else:\n        model.load_state_dict(torch.load(f'{exp_name}_best.pt'))\n    \n    # Final test\n    final_test_acc = history['acc_test'][-1]\n    \n    # Plot training curves for this experiment\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n    \n    ax1 = axes[0]\n    ax1.plot(history['epoch'], history['loss_train'], 'b-o', markersize=4)\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Loss')\n    ax1.set_title(f'{exp_name} - Training Loss')\n    ax1.grid(True, alpha=0.3)\n    \n    ax2 = axes[1]\n    ax2.plot(history['epoch'], [a*100 for a in history['acc_train']], 'b-o', label='Train', markersize=4)\n    ax2.plot(history['epoch'], [a*100 for a in history['acc_test']], 'r-s', label='Test', markersize=4)\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Accuracy (%)')\n    ax2.set_title(f'{exp_name} - Accuracy')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n    \n    plt.suptitle(f'{exp_name} Training Curves (GPU {gpu_id})', fontsize=12, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"[GPU {gpu_id}] {exp_name} >>> FINAL TEST ACC: {final_test_acc:.4f}\")\n    \n    results_queue.put({\n        'exp': exp_name, \n        'train_acc': best_acc, \n        'test_acc': final_test_acc, \n        'gpu': gpu_id,\n        'history': history\n    })\n    \n    del model\n    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11. Create Datasets\n",
    "\n",
    "train_ds_baseline = VideoDataset(PATH_DATA_TRAIN, VideoTransformBaseline(is_train=True), use_pil=False)\n",
    "train_ds_consistent = VideoDataset(PATH_DATA_TRAIN, VideoTransformConsistent(is_train=True), use_pil=True)\n",
    "\n",
    "test_ds_single = TestDatasetSingle(PATH_DATA_TEST)\n",
    "test_ds_multi = TestDatasetMultiSegment(PATH_DATA_TEST, num_segments=2)\n",
    "test_ds_flip = TestDatasetFlipTTA(PATH_DATA_TEST)\n",
    "\n",
    "mixup_collate = MixupCollate(num_classes=len(train_ds_consistent.classes), alpha=0.8)\n",
    "\n",
    "print(f\"Train samples: {len(train_ds_baseline)}, Test samples: {len(test_ds_single)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 12. Define All 8 Experiments\n",
    "\n",
    "EXPERIMENTS = [\n",
    "    # GPU 0 experiments\n",
    "    {'name': 'Exp0_ViT_Baseline', 'train_ds': train_ds_baseline, 'test_ds': test_ds_single, 'is_vit': True},\n",
    "    {'name': 'Exp2_MultiSegment_TTA', 'train_ds': train_ds_baseline, 'test_ds': test_ds_multi, 'multi_view': True},\n",
    "    {'name': 'Exp4_Mixup', 'train_ds': train_ds_consistent, 'test_ds': test_ds_single, 'use_mixup': True, 'mixup_collate': mixup_collate},\n",
    "    {'name': 'Exp6_2Stage', 'train_ds': train_ds_consistent, 'test_ds': test_ds_single, 'use_mixup': True, 'mixup_collate': mixup_collate, 'two_stage': True},\n",
    "    \n",
    "    # GPU 1 experiments\n",
    "    {'name': 'Exp1_VideoMAE_Paper', 'train_ds': train_ds_baseline, 'test_ds': test_ds_single},\n",
    "    {'name': 'Exp3_Consistent_Transform', 'train_ds': train_ds_consistent, 'test_ds': test_ds_single},\n",
    "    {'name': 'Exp5_LabelSmoothing', 'train_ds': train_ds_consistent, 'test_ds': test_ds_single, 'label_smoothing': 0.1},\n",
    "    {'name': 'Exp7_FlipTTA', 'train_ds': train_ds_consistent, 'test_ds': test_ds_flip, 'use_mixup': True, 'mixup_collate': mixup_collate, 'two_stage': True, 'multi_view': True},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 13. Run Experiments (Sequential - more reliable in Jupyter)\n\n# Note: Dual GPU parallel using multiprocessing has issues in Jupyter notebooks\n# Running sequentially on GPU 0 for reliability\n\nfrom queue import Queue\n\nRESULTS = []\nq = Queue()\n\n# Run experiments sequentially on GPU 0\n# For parallel execution, use a Python script instead of notebook\n\nfor exp_config in EXPERIMENTS:\n    print(f\"\\n{'='*60}\")\n    print(f\"Running: {exp_config['name']}\")\n    print('='*60)\n    run_single_experiment(exp_config, 0, q)\n    while not q.empty():\n        RESULTS.append(q.get())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 14. Results Summary & Plot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALL EXPERIMENTS RESULTS (8 Experiments, 10 Epochs, Dual GPU)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_df = pd.DataFrame(RESULTS)\n",
    "results_df = results_df.sort_values('exp').reset_index(drop=True)\n",
    "\n",
    "# Calculate delta from VideoMAE baseline (Exp1)\n",
    "baseline_acc = results_df[results_df['exp'] == 'Exp1_VideoMAE_Paper']['test_acc'].values[0]\n",
    "results_df['delta'] = (results_df['test_acc'] - baseline_acc) * 100\n",
    "results_df['delta_str'] = results_df['delta'].apply(lambda x: f\"+{x:.2f}%\" if x > 0 else f\"{x:.2f}%\")\n",
    "\n",
    "print(results_df[['exp', 'train_acc', 'test_acc', 'delta_str', 'gpu']].to_string(index=False))\n",
    "\n",
    "vit_acc = results_df[results_df['exp'] == 'Exp0_ViT_Baseline']['test_acc'].values[0]\n",
    "print(f\"\\nViT Baseline (Exp0): {vit_acc:.4f}\")\n",
    "print(f\"VideoMAE Baseline (Exp1): {baseline_acc:.4f}\")\n",
    "print(f\"Best: {results_df['test_acc'].max():.4f}\")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "exp_names = [e.replace('Exp', '').replace('_', '\\n') for e in results_df['exp']]\n",
    "colors = ['#3498db' if 'ViT' in e else '#2ecc71' for e in results_df['exp']]\n",
    "bars = ax.bar(exp_names, results_df['test_acc'] * 100, color=colors, edgecolor='black')\n",
    "\n",
    "# Add baseline line\n",
    "ax.axhline(y=baseline_acc * 100, color='red', linestyle='--', label=f'VideoMAE Baseline ({baseline_acc*100:.1f}%)')\n",
    "ax.axhline(y=vit_acc * 100, color='blue', linestyle=':', label=f'ViT Baseline ({vit_acc*100:.1f}%)')\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, results_df['test_acc']):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, f'{acc*100:.1f}%', \n",
    "            ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Test Accuracy (%)')\n",
    "ax.set_xlabel('Experiment')\n",
    "ax.set_title('VideoMAE Ablation Study Results (10 Epochs, Dual T4 GPU)', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.set_ylim([50, 100])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}