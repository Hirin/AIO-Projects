{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ViT-Base Experiment for Video Action Recognition\n",
                "\n",
                "**Objective**: Compare ViT-Base vs ViT-Small for frame-level video classification.\n",
                "\n",
                "**Model**: `vit_base_patch16_224` (ImageNet-21k pretrained)\n",
                "\n",
                "**Expected**: Higher accuracy than ViT-Small (63.92%) due to larger capacity."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import DataLoader, Dataset\n",
                "from torchvision import transforms\n",
                "from torchvision.transforms import InterpolationMode\n",
                "import torchvision.transforms.functional as TF\n",
                "from pathlib import Path\n",
                "from PIL import Image\n",
                "from tqdm.auto import tqdm\n",
                "import timm\n",
                "import random\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from sklearn.metrics import accuracy_score, classification_report\n",
                "\n",
                "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Using device: {DEVICE}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data paths\n",
                "PATH_DATA_TRAIN = r'/kaggle/input/action-video/data/data_train'\n",
                "PATH_DATA_TEST = r'/kaggle/input/action-video/data/test'\n",
                "\n",
                "# Model parameters \n",
                "NUM_FRAMES = 16\n",
                "IMG_SIZE = 224\n",
                "RESIZE_SIZE = 256\n",
                "\n",
                "# Training parameters\n",
                "BATCH_SIZE = 8  # Smaller due to larger model\n",
                "EPOCHS = 10\n",
                "BASE_LR = 5e-5\n",
                "HEAD_LR = 5e-4\n",
                "WEIGHT_DECAY = 0.05\n",
                "GRAD_ACCUM_STEPS = 4\n",
                "\n",
                "# Model choice - ViT-Base\n",
                "PRETRAINED_NAME = 'vit_base_patch16_224'\n",
                "    \n",
                "print(f\"Train data: {PATH_DATA_TRAIN}\")\n",
                "print(f\"Test data: {PATH_DATA_TEST}\")\n",
                "print(f\"Model: {PRETRAINED_NAME}\")\n",
                "print(f\"Frames per video: {NUM_FRAMES}\")\n",
                "print(f\"Batch size: {BATCH_SIZE}\")\n",
                "print(f\"Epochs: {EPOCHS}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. ViT-Base Model with Temporal Pooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ViTBaseForAction(nn.Module):\n",
                "    \"\"\"ViT-Base for action recognition with temporal mean pooling.\"\"\"\n",
                "    \n",
                "    def __init__(self, num_classes=51, pretrained_name='vit_base_patch16_224'):\n",
                "        super().__init__()\n",
                "        \n",
                "        # Load pretrained ViT-Base\n",
                "        self.vit = timm.create_model(pretrained_name, pretrained=True, num_classes=0)\n",
                "        \n",
                "        # Get embedding dimension (768 for ViT-Base)\n",
                "        self.embed_dim = self.vit.num_features\n",
                "        \n",
                "        # Classification head with dropout\n",
                "        self.head = nn.Sequential(\n",
                "            nn.Dropout(0.3),\n",
                "            nn.Linear(self.embed_dim, num_classes)\n",
                "        )\n",
                "    \n",
                "    def forward(self, video):\n",
                "        '''\n",
                "        Args:\n",
                "            video: [B, T, C, H, W] - batch of video clips\n",
                "        Returns:\n",
                "            logits: [B, num_classes]\n",
                "        '''\n",
                "        B, T, C, H, W = video.shape\n",
                "        \n",
                "        # Reshape to process all frames\n",
                "        x = video.view(B * T, C, H, W)\n",
                "        \n",
                "        # Extract features with ViT\n",
                "        features = self.vit(x)  # [B*T, embed_dim]\n",
                "        \n",
                "        # Reshape back\n",
                "        features = features.view(B, T, self.embed_dim)\n",
                "        \n",
                "        # Temporal pooling (mean)\n",
                "        pooled = features.mean(dim=1)  # [B, embed_dim]\n",
                "        \n",
                "        # Classification\n",
                "        logits = self.head(pooled)\n",
                "        \n",
                "        return logits\n",
                "\n",
                "print(f\"ViT-Base model defined\")\n",
                "print(f\"  Backbone: {PRETRAINED_NAME}\")\n",
                "print(f\"  Expected embed_dim: 768\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Augmentation (Consistent Spatial Transform)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class VideoTransform:\n",
                "    def __init__(self, image_size=224, resize_size=256, is_train=True):\n",
                "        self.image_size = image_size\n",
                "        self.resize_size = resize_size\n",
                "        self.is_train = is_train\n",
                "        self.mean = [0.485, 0.456, 0.406]  # ImageNet stats\n",
                "        self.std = [0.229, 0.224, 0.225]\n",
                "    \n",
                "    def __call__(self, frames):\n",
                "        \"\"\"Apply consistent transform across all frames.\"\"\"\n",
                "        # Resize all frames first\n",
                "        frames = [TF.resize(f, self.resize_size, interpolation=InterpolationMode.BILINEAR) for f in frames]\n",
                "        \n",
                "        if self.is_train:\n",
                "            # Get random crop params (same for all frames)\n",
                "            i, j, h, w = transforms.RandomResizedCrop.get_params(\n",
                "                frames[0], scale=(0.8, 1.0), ratio=(0.75, 1.33)\n",
                "            )\n",
                "            do_flip = random.random() > 0.5\n",
                "            \n",
                "            transformed = []\n",
                "            for img in frames:\n",
                "                img = TF.resized_crop(img, i, j, h, w, (self.image_size, self.image_size))\n",
                "                if do_flip:\n",
                "                    img = TF.hflip(img)\n",
                "                img = TF.to_tensor(img)\n",
                "                img = TF.normalize(img, self.mean, self.std)\n",
                "                transformed.append(img)\n",
                "        else:\n",
                "            # Center crop for validation\n",
                "            transformed = []\n",
                "            for img in frames:\n",
                "                img = TF.center_crop(img, self.image_size)\n",
                "                img = TF.to_tensor(img)\n",
                "                img = TF.normalize(img, self.mean, self.std)\n",
                "                transformed.append(img)\n",
                "                \n",
                "        return torch.stack(transformed)\n",
                "\n",
                "print(\"Augmentation defined (Consistent Spatial Transform)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Dataset Classes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class VideoDataset(Dataset):\n",
                "    def __init__(self, root, num_frames=16, image_size=224, is_train=True):\n",
                "        self.root = Path(root)\n",
                "        self.num_frames = num_frames\n",
                "        self.transform = VideoTransform(image_size, is_train=is_train)\n",
                "        \n",
                "        self.classes = sorted([d.name for d in self.root.iterdir() if d.is_dir()])\n",
                "        self.class_to_idx = {name: idx for idx, name in enumerate(self.classes)}\n",
                "        \n",
                "        self.samples = []\n",
                "        for cls in self.classes:\n",
                "            cls_dir = self.root / cls\n",
                "            for video_dir in sorted([d for d in cls_dir.iterdir() if d.is_dir()]):\n",
                "                frame_paths = sorted([p for p in video_dir.iterdir() if p.suffix.lower() in {'.jpg', '.jpeg', '.png'}])\n",
                "                if frame_paths:\n",
                "                    self.samples.append((frame_paths, self.class_to_idx[cls]))\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.samples)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        frame_paths, label = self.samples[idx]\n",
                "        total = len(frame_paths)\n",
                "        \n",
                "        # Uniform sampling\n",
                "        indices = torch.linspace(0, total - 1, self.num_frames).long()\n",
                "        \n",
                "        frames = []\n",
                "        for i in indices:\n",
                "            img = Image.open(frame_paths[i]).convert(\"RGB\")\n",
                "            frames.append(img)\n",
                "        \n",
                "        video = self.transform(frames)\n",
                "        return video, label\n",
                "\n",
                "\n",
                "class TestDataset(Dataset):\n",
                "    def __init__(self, root, num_frames=16, image_size=224):\n",
                "        self.root = Path(root)\n",
                "        self.num_frames = num_frames\n",
                "        self.transform = VideoTransform(image_size, is_train=False)\n",
                "        self.video_dirs = sorted([d for d in self.root.iterdir() if d.is_dir()], key=lambda x: int(x.name))\n",
                "        self.video_ids = [int(d.name) for d in self.video_dirs]\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.video_dirs)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        video_dir = self.video_dirs[idx]\n",
                "        video_id = self.video_ids[idx]\n",
                "        frame_paths = sorted([p for p in video_dir.iterdir() if p.suffix.lower() in {'.jpg', '.jpeg', '.png'}])\n",
                "        \n",
                "        total = len(frame_paths)\n",
                "        indices = torch.linspace(0, total - 1, self.num_frames).long()\n",
                "        \n",
                "        frames = []\n",
                "        for i in indices:\n",
                "            img = Image.open(frame_paths[i]).convert(\"RGB\")\n",
                "            frames.append(img)\n",
                "        \n",
                "        video = self.transform(frames)\n",
                "        return video, video_id\n",
                "\n",
                "print(\"Dataset classes defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Training Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_one_epoch(model, loader, optimizer, scaler, device, grad_accum_steps=1):\n",
                "    model.train()\n",
                "    total_loss = 0.0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    optimizer.zero_grad()\n",
                "    progress = tqdm(loader, desc=\"Train\", leave=False)\n",
                "    \n",
                "    for batch_idx, (videos, labels) in enumerate(progress):\n",
                "        videos = videos.to(device)\n",
                "        labels = labels.to(device)\n",
                "        \n",
                "        with torch.amp.autocast(device_type='cuda', enabled=(device.type == 'cuda')):\n",
                "            logits = model(videos)\n",
                "            loss = F.cross_entropy(logits, labels)\n",
                "        \n",
                "        preds = logits.argmax(dim=1)\n",
                "        correct += (preds == labels).sum().item()\n",
                "        total += labels.size(0)\n",
                "        \n",
                "        loss_value = loss.item()\n",
                "        loss = loss / grad_accum_steps\n",
                "        scaler.scale(loss).backward()\n",
                "        \n",
                "        should_step = ((batch_idx + 1) % grad_accum_steps == 0) or (batch_idx + 1 == len(loader))\n",
                "        if should_step:\n",
                "            scaler.unscale_(optimizer)\n",
                "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
                "            scaler.step(optimizer)\n",
                "            scaler.update()\n",
                "            optimizer.zero_grad()\n",
                "        \n",
                "        batch_size = videos.size(0)\n",
                "        total_loss += loss_value * batch_size\n",
                "        progress.set_postfix(loss=f\"{loss_value:.4f}\", acc=f\"{correct / max(total, 1):.4f}\")\n",
                "    \n",
                "    avg_loss = total_loss / max(total, 1)\n",
                "    avg_acc = correct / max(total, 1)\n",
                "    return avg_loss, avg_acc\n",
                "\n",
                "print(\"Training functions defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Load Data & Create Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Loading training dataset...\")\n",
                "train_dataset = VideoDataset(PATH_DATA_TRAIN, num_frames=NUM_FRAMES, image_size=IMG_SIZE, is_train=True)\n",
                "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n",
                "\n",
                "print(f\"Train samples: {len(train_dataset)}\")\n",
                "print(f\"Classes: {len(train_dataset.classes)}\")\n",
                "print(f\"Batches per epoch: {len(train_loader)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Creating ViT-Base model...\")\n",
                "model = ViTBaseForAction(num_classes=len(train_dataset.classes), pretrained_name=PRETRAINED_NAME).to(DEVICE)\n",
                "\n",
                "num_params = sum(p.numel() for p in model.parameters())\n",
                "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "\n",
                "print(f\"Total parameters: {num_params:,}\")\n",
                "print(f\"Trainable parameters: {trainable_params:,}\")\n",
                "print(f\"Model size: {num_params * 4 / 1024 / 1024:.2f} MB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Optimizer with different LR for backbone and head\n",
                "backbone_params = []\n",
                "head_params = []\n",
                "\n",
                "for name, param in model.named_parameters():\n",
                "    if not param.requires_grad:\n",
                "        continue\n",
                "    if 'head' in name:\n",
                "        head_params.append(param)\n",
                "    else:\n",
                "        backbone_params.append(param)\n",
                "\n",
                "optimizer = torch.optim.AdamW([\n",
                "    {\"params\": backbone_params, \"lr\": BASE_LR},\n",
                "    {\"params\": head_params, \"lr\": HEAD_LR},\n",
                "], weight_decay=WEIGHT_DECAY)\n",
                "\n",
                "scaler = torch.amp.GradScaler(enabled=torch.cuda.is_available())\n",
                "print(f\"Optimizer: AdamW | Base LR: {BASE_LR} | Head LR: {HEAD_LR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "best_acc = 0.0\n",
                "checkpoint_path = Path('./vit_base_best.pt')\n",
                "\n",
                "print(\"=\"*50)\n",
                "print(\"TRAINING VIT-BASE\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "for epoch in range(EPOCHS):\n",
                "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
                "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, scaler, DEVICE, GRAD_ACCUM_STEPS)\n",
                "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
                "    \n",
                "    if train_acc > best_acc:\n",
                "        best_acc = train_acc\n",
                "        torch.save({\n",
                "            'model': model.state_dict(), \n",
                "            'classes': train_dataset.classes, \n",
                "            'acc': best_acc,\n",
                "            'epoch': epoch + 1\n",
                "        }, checkpoint_path)\n",
                "        print(f\"  >>> Best model saved (acc: {best_acc:.4f})\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(f\"Training completed! Best train accuracy: {best_acc:.4f}\")\n",
                "print(f\"Model saved to: {checkpoint_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Inference on Test Set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"INFERENCE ON TEST SET\")\n",
                "\n",
                "# Load best checkpoint\n",
                "print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
                "checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
                "classes = checkpoint['classes']\n",
                "\n",
                "model = ViTBaseForAction(num_classes=len(classes), pretrained_name=PRETRAINED_NAME).to(DEVICE)\n",
                "model.load_state_dict(checkpoint['model'])\n",
                "model.eval()\n",
                "print(f\"Model loaded (trained acc: {checkpoint['acc']:.4f})\")\n",
                "\n",
                "# Load test dataset\n",
                "print(\"\\nLoading test dataset...\")\n",
                "test_dataset = TestDataset(PATH_DATA_TEST, num_frames=NUM_FRAMES, image_size=IMG_SIZE)\n",
                "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
                "print(f\"Test samples: {len(test_dataset)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\nRunning inference...\")\n",
                "predictions = []\n",
                "\n",
                "with torch.no_grad():\n",
                "    for videos, video_ids in tqdm(test_loader, desc=\"Inference\"):\n",
                "        videos = videos.to(DEVICE)\n",
                "        logits = model(videos)\n",
                "        preds = logits.argmax(dim=1)\n",
                "        \n",
                "        for video_id, pred_idx in zip(video_ids.cpu().numpy(), preds.cpu().numpy()):\n",
                "            pred_class = classes[pred_idx]\n",
                "            predictions.append((video_id, pred_class))\n",
                "\n",
                "predictions.sort(key=lambda x: x[0])\n",
                "print(f\"\\nTotal predictions: {len(predictions)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Evaluate with Ground Truth Labels"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!gdown \"1Xv2CWOqdBj3kt0rkNJKRsodSIEd3-wX_\" -O test_labels.csv -q\n",
                "print(\"Downloaded test_labels.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load ground truth\n",
                "gt_df = pd.read_csv(\"test_labels.csv\")\n",
                "test_labels = dict(zip(gt_df['id'].astype(str), gt_df['class']))\n",
                "\n",
                "# Match predictions with ground truth\n",
                "y_pred = []\n",
                "y_true = []\n",
                "for video_id, pred_class in predictions:\n",
                "    video_id_str = str(video_id)\n",
                "    if video_id_str in test_labels:\n",
                "        y_pred.append(pred_class)\n",
                "        y_true.append(test_labels[video_id_str])\n",
                "\n",
                "# Calculate accuracy\n",
                "accuracy = accuracy_score(y_true, y_pred)\n",
                "\n",
                "print(\"=\" * 50)\n",
                "print(\"VIT-BASE TEST SET EVALUATION\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"Total: {len(y_true)} | Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
                "print()\n",
                "print(\"Comparison:\")\n",
                "print(f\"  ViT-Small (baseline): 63.92%\")\n",
                "print(f\"  ViT-Base (this):      {accuracy*100:.2f}%\")\n",
                "print(f\"  Improvement:          {(accuracy - 0.6392)*100:+.2f}%\")\n",
                "print()\n",
                "print(classification_report(y_true, y_pred, zero_division=0))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Save Submission"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create submission file\n",
                "submission = pd.DataFrame(predictions, columns=['id', 'class'])\n",
                "submission.to_csv('submission_vit_base.csv', index=False)\n",
                "print(f\"Saved submission_vit_base.csv ({len(submission)} rows)\")\n",
                "print(submission.head())"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        },
        "kaggle": {
            "accelerator": "nvidiaTeslaT4",
            "dataSources": [
                {
                    "sourceId": 125907,
                    "databundleVersionId": 14910023,
                    "sourceType": "competition"
                }
            ],
            "dockerImageVersionId": 31193,
            "isInternetEnabled": true,
            "language": "python",
            "sourceType": "notebook",
            "isGpuEnabled": true
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}