{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":125907,"databundleVersionId":14910023,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ViT-Base Experiment for Video Action Recognition\n\n**Objective**: Compare ViT-Base vs ViT-Small for frame-level video classification.\n\n**Model**: `vit_base_patch16_224` (ImageNet-21k pretrained)\n\n**Expected**: Higher accuracy than ViT-Small (63.92%) due to larger capacity.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom torchvision.transforms import InterpolationMode\nimport torchvision.transforms.functional as TF\nfrom pathlib import Path\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nimport timm\nimport random\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, classification_report\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {DEVICE}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T06:00:48.904089Z","iopub.execute_input":"2026-01-18T06:00:48.904326Z","iopub.status.idle":"2026-01-18T06:01:00.897086Z","shell.execute_reply.started":"2026-01-18T06:00:48.904302Z","shell.execute_reply":"2026-01-18T06:01:00.896272Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## 1. Configuration","metadata":{}},{"cell_type":"code","source":"# Data paths\nPATH_DATA_TRAIN = r'/kaggle/input/action-video/data/data_train'\nPATH_DATA_TEST = r'/kaggle/input/action-video/data/test'\n\n# Model parameters \nNUM_FRAMES = 16\nIMG_SIZE = 224\nRESIZE_SIZE = 256\n\n# Training parameters\nBATCH_SIZE = 8  # Smaller due to larger model\nEPOCHS = 10\nBASE_LR = 5e-5\nHEAD_LR = 5e-4\nWEIGHT_DECAY = 0.05\nGRAD_ACCUM_STEPS = 4\n\n# Model choice - ViT-Base\nPRETRAINED_NAME = 'vit_base_patch16_224'\n    \nprint(f\"Train data: {PATH_DATA_TRAIN}\")\nprint(f\"Test data: {PATH_DATA_TEST}\")\nprint(f\"Model: {PRETRAINED_NAME}\")\nprint(f\"Frames per video: {NUM_FRAMES}\")\nprint(f\"Batch size: {BATCH_SIZE}\")\nprint(f\"Epochs: {EPOCHS}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T06:01:00.897915Z","iopub.execute_input":"2026-01-18T06:01:00.898368Z","iopub.status.idle":"2026-01-18T06:01:00.904365Z","shell.execute_reply.started":"2026-01-18T06:01:00.898347Z","shell.execute_reply":"2026-01-18T06:01:00.903432Z"}},"outputs":[{"name":"stdout","text":"Train data: /kaggle/input/action-video/data/data_train\nTest data: /kaggle/input/action-video/data/test\nModel: vit_base_patch16_224\nFrames per video: 16\nBatch size: 8\nEpochs: 10\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## 2. ViT-Base Model with Temporal Pooling","metadata":{}},{"cell_type":"code","source":"class ViTBaseForAction(nn.Module):\n    \"\"\"ViT-Base for action recognition with temporal mean pooling.\"\"\"\n    \n    def __init__(self, num_classes=51, pretrained_name='vit_base_patch16_224'):\n        super().__init__()\n        \n        # Load pretrained ViT-Base\n        self.vit = timm.create_model(pretrained_name, pretrained=True, num_classes=0)\n        \n        # Get embedding dimension (768 for ViT-Base)\n        self.embed_dim = self.vit.num_features\n        \n        # Classification head with dropout\n        self.head = nn.Sequential(\n            nn.Dropout(0.3),\n            nn.Linear(self.embed_dim, num_classes)\n        )\n    \n    def forward(self, video):\n        '''\n        Args:\n            video: [B, T, C, H, W] - batch of video clips\n        Returns:\n            logits: [B, num_classes]\n        '''\n        B, T, C, H, W = video.shape\n        \n        # Reshape to process all frames\n        x = video.view(B * T, C, H, W)\n        \n        # Extract features with ViT\n        features = self.vit(x)  # [B*T, embed_dim]\n        \n        # Reshape back\n        features = features.view(B, T, self.embed_dim)\n        \n        # Temporal pooling (mean)\n        pooled = features.mean(dim=1)  # [B, embed_dim]\n        \n        # Classification\n        logits = self.head(pooled)\n        \n        return logits\n\nprint(f\"ViT-Base model defined\")\nprint(f\"  Backbone: {PRETRAINED_NAME}\")\nprint(f\"  Expected embed_dim: 768\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T06:01:00.906039Z","iopub.execute_input":"2026-01-18T06:01:00.906259Z","iopub.status.idle":"2026-01-18T06:01:00.917934Z","shell.execute_reply.started":"2026-01-18T06:01:00.906242Z","shell.execute_reply":"2026-01-18T06:01:00.917243Z"}},"outputs":[{"name":"stdout","text":"ViT-Base model defined\n  Backbone: vit_base_patch16_224\n  Expected embed_dim: 768\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## 3. Data Augmentation (Consistent Spatial Transform)","metadata":{}},{"cell_type":"code","source":"class VideoTransform:\n    def __init__(self, image_size=224, resize_size=256, is_train=True):\n        self.image_size = image_size\n        self.resize_size = resize_size\n        self.is_train = is_train\n        self.mean = [0.485, 0.456, 0.406]  # ImageNet stats\n        self.std = [0.229, 0.224, 0.225]\n    \n    def __call__(self, frames):\n        \"\"\"Apply consistent transform across all frames.\"\"\"\n        # Resize all frames first\n        frames = [TF.resize(f, self.resize_size, interpolation=InterpolationMode.BILINEAR) for f in frames]\n        \n        if self.is_train:\n            # Get random crop params (same for all frames)\n            i, j, h, w = transforms.RandomResizedCrop.get_params(\n                frames[0], scale=(0.8, 1.0), ratio=(0.75, 1.33)\n            )\n            do_flip = random.random() > 0.5\n            \n            transformed = []\n            for img in frames:\n                img = TF.resized_crop(img, i, j, h, w, (self.image_size, self.image_size))\n                if do_flip:\n                    img = TF.hflip(img)\n                img = TF.to_tensor(img)\n                img = TF.normalize(img, self.mean, self.std)\n                transformed.append(img)\n        else:\n            # Center crop for validation\n            transformed = []\n            for img in frames:\n                img = TF.center_crop(img, self.image_size)\n                img = TF.to_tensor(img)\n                img = TF.normalize(img, self.mean, self.std)\n                transformed.append(img)\n                \n        return torch.stack(transformed)\n\nprint(\"Augmentation defined (Consistent Spatial Transform)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T06:01:00.918714Z","iopub.execute_input":"2026-01-18T06:01:00.919101Z","iopub.status.idle":"2026-01-18T06:01:00.933589Z","shell.execute_reply.started":"2026-01-18T06:01:00.919083Z","shell.execute_reply":"2026-01-18T06:01:00.933016Z"}},"outputs":[{"name":"stdout","text":"Augmentation defined (Consistent Spatial Transform)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## 4. Dataset Classes","metadata":{}},{"cell_type":"code","source":"class VideoDataset(Dataset):\n    def __init__(self, root, num_frames=16, image_size=224, is_train=True):\n        self.root = Path(root)\n        self.num_frames = num_frames\n        self.transform = VideoTransform(image_size, is_train=is_train)\n        \n        self.classes = sorted([d.name for d in self.root.iterdir() if d.is_dir()])\n        self.class_to_idx = {name: idx for idx, name in enumerate(self.classes)}\n        \n        self.samples = []\n        for cls in self.classes:\n            cls_dir = self.root / cls\n            for video_dir in sorted([d for d in cls_dir.iterdir() if d.is_dir()]):\n                frame_paths = sorted([p for p in video_dir.iterdir() if p.suffix.lower() in {'.jpg', '.jpeg', '.png'}])\n                if frame_paths:\n                    self.samples.append((frame_paths, self.class_to_idx[cls]))\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        frame_paths, label = self.samples[idx]\n        total = len(frame_paths)\n        \n        # Uniform sampling\n        indices = torch.linspace(0, total - 1, self.num_frames).long()\n        \n        frames = []\n        for i in indices:\n            img = Image.open(frame_paths[i]).convert(\"RGB\")\n            frames.append(img)\n        \n        video = self.transform(frames)\n        return video, label\n\n\nclass TestDataset(Dataset):\n    def __init__(self, root, num_frames=16, image_size=224):\n        self.root = Path(root)\n        self.num_frames = num_frames\n        self.transform = VideoTransform(image_size, is_train=False)\n        self.video_dirs = sorted([d for d in self.root.iterdir() if d.is_dir()], key=lambda x: int(x.name))\n        self.video_ids = [int(d.name) for d in self.video_dirs]\n    \n    def __len__(self):\n        return len(self.video_dirs)\n    \n    def __getitem__(self, idx):\n        video_dir = self.video_dirs[idx]\n        video_id = self.video_ids[idx]\n        frame_paths = sorted([p for p in video_dir.iterdir() if p.suffix.lower() in {'.jpg', '.jpeg', '.png'}])\n        \n        total = len(frame_paths)\n        indices = torch.linspace(0, total - 1, self.num_frames).long()\n        \n        frames = []\n        for i in indices:\n            img = Image.open(frame_paths[i]).convert(\"RGB\")\n            frames.append(img)\n        \n        video = self.transform(frames)\n        return video, video_id\n\nprint(\"Dataset classes defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T06:01:00.934269Z","iopub.execute_input":"2026-01-18T06:01:00.934722Z","iopub.status.idle":"2026-01-18T06:01:00.949436Z","shell.execute_reply.started":"2026-01-18T06:01:00.934705Z","shell.execute_reply":"2026-01-18T06:01:00.948809Z"}},"outputs":[{"name":"stdout","text":"Dataset classes defined\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## 5. Training Functions","metadata":{}},{"cell_type":"code","source":"def train_one_epoch(model, loader, optimizer, scaler, device, grad_accum_steps=1):\n    model.train()\n    total_loss = 0.0\n    correct = 0\n    total = 0\n    \n    optimizer.zero_grad()\n    progress = tqdm(loader, desc=\"Train\", leave=False)\n    \n    for batch_idx, (videos, labels) in enumerate(progress):\n        videos = videos.to(device)\n        labels = labels.to(device)\n        \n        with torch.amp.autocast(device_type='cuda', enabled=(device.type == 'cuda')):\n            logits = model(videos)\n            loss = F.cross_entropy(logits, labels)\n        \n        preds = logits.argmax(dim=1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n        \n        loss_value = loss.item()\n        loss = loss / grad_accum_steps\n        scaler.scale(loss).backward()\n        \n        should_step = ((batch_idx + 1) % grad_accum_steps == 0) or (batch_idx + 1 == len(loader))\n        if should_step:\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n        \n        batch_size = videos.size(0)\n        total_loss += loss_value * batch_size\n        progress.set_postfix(loss=f\"{loss_value:.4f}\", acc=f\"{correct / max(total, 1):.4f}\")\n    \n    avg_loss = total_loss / max(total, 1)\n    avg_acc = correct / max(total, 1)\n    return avg_loss, avg_acc\n\nprint(\"Training functions defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T06:01:00.950087Z","iopub.execute_input":"2026-01-18T06:01:00.950273Z","iopub.status.idle":"2026-01-18T06:01:00.963219Z","shell.execute_reply.started":"2026-01-18T06:01:00.950259Z","shell.execute_reply":"2026-01-18T06:01:00.962508Z"}},"outputs":[{"name":"stdout","text":"Training functions defined\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## 6. Load Data & Create Model","metadata":{}},{"cell_type":"code","source":"print(\"Loading training dataset...\")\ntrain_dataset = VideoDataset(PATH_DATA_TRAIN, num_frames=NUM_FRAMES, image_size=IMG_SIZE, is_train=True)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n\nprint(f\"Train samples: {len(train_dataset)}\")\nprint(f\"Classes: {len(train_dataset.classes)}\")\nprint(f\"Batches per epoch: {len(train_loader)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T06:01:00.963890Z","iopub.execute_input":"2026-01-18T06:01:00.964116Z","iopub.status.idle":"2026-01-18T06:01:40.191854Z","shell.execute_reply.started":"2026-01-18T06:01:00.964093Z","shell.execute_reply":"2026-01-18T06:01:40.191105Z"}},"outputs":[{"name":"stdout","text":"Loading training dataset...\nTrain samples: 6254\nClasses: 51\nBatches per epoch: 781\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"print(\"Creating ViT-Base model...\")\nmodel = ViTBaseForAction(num_classes=len(train_dataset.classes), pretrained_name=PRETRAINED_NAME).to(DEVICE)\n\nnum_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"Total parameters: {num_params:,}\")\nprint(f\"Trainable parameters: {trainable_params:,}\")\nprint(f\"Model size: {num_params * 4 / 1024 / 1024:.2f} MB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T06:01:40.192711Z","iopub.execute_input":"2026-01-18T06:01:40.193092Z","iopub.status.idle":"2026-01-18T06:01:43.376558Z","shell.execute_reply.started":"2026-01-18T06:01:40.193062Z","shell.execute_reply":"2026-01-18T06:01:43.375944Z"}},"outputs":[{"name":"stdout","text":"Creating ViT-Base model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a03529c4ea5444d6b1b81cb7cad32c94"}},"metadata":{}},{"name":"stdout","text":"Total parameters: 85,837,875\nTrainable parameters: 85,837,875\nModel size: 327.45 MB\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Optimizer with different LR for backbone and head\nbackbone_params = []\nhead_params = []\n\nfor name, param in model.named_parameters():\n    if not param.requires_grad:\n        continue\n    if 'head' in name:\n        head_params.append(param)\n    else:\n        backbone_params.append(param)\n\noptimizer = torch.optim.AdamW([\n    {\"params\": backbone_params, \"lr\": BASE_LR},\n    {\"params\": head_params, \"lr\": HEAD_LR},\n], weight_decay=WEIGHT_DECAY)\n\nscaler = torch.amp.GradScaler(enabled=torch.cuda.is_available())\nprint(f\"Optimizer: AdamW | Base LR: {BASE_LR} | Head LR: {HEAD_LR}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T06:01:43.378635Z","iopub.execute_input":"2026-01-18T06:01:43.378896Z","iopub.status.idle":"2026-01-18T06:01:43.385023Z","shell.execute_reply.started":"2026-01-18T06:01:43.378866Z","shell.execute_reply":"2026-01-18T06:01:43.384274Z"}},"outputs":[{"name":"stdout","text":"Optimizer: AdamW | Base LR: 5e-05 | Head LR: 0.0005\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## 7. Training Loop","metadata":{}},{"cell_type":"code","source":"best_acc = 0.0\ncheckpoint_path = Path('./vit_base_best.pt')\n\nprint(\"=\"*50)\nprint(\"TRAINING VIT-BASE\")\nprint(\"=\"*50)\n\nfor epoch in range(EPOCHS):\n    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, scaler, DEVICE, GRAD_ACCUM_STEPS)\n    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n    \n    if train_acc > best_acc:\n        best_acc = train_acc\n        torch.save({\n            'model': model.state_dict(), \n            'classes': train_dataset.classes, \n            'acc': best_acc,\n            'epoch': epoch + 1\n        }, checkpoint_path)\n        print(f\"  >>> Best model saved (acc: {best_acc:.4f})\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(f\"Training completed! Best train accuracy: {best_acc:.4f}\")\nprint(f\"Model saved to: {checkpoint_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T06:01:43.385751Z","iopub.execute_input":"2026-01-18T06:01:43.385940Z","iopub.status.idle":"2026-01-18T08:27:44.648256Z","shell.execute_reply.started":"2026-01-18T06:01:43.385926Z","shell.execute_reply":"2026-01-18T08:27:44.647206Z"}},"outputs":[{"name":"stdout","text":"==================================================\nTRAINING VIT-BASE\n==================================================\n\nEpoch 1/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/781 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"  Train Loss: 1.7703 | Train Acc: 0.5210\n  >>> Best model saved (acc: 0.5210)\n\nEpoch 2/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/781 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"  Train Loss: 0.7282 | Train Acc: 0.7785\n  >>> Best model saved (acc: 0.7785)\n\nEpoch 3/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/781 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"  Train Loss: 0.4253 | Train Acc: 0.8657\n  >>> Best model saved (acc: 0.8657)\n\nEpoch 4/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/781 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"  Train Loss: 0.2605 | Train Acc: 0.9161\n  >>> Best model saved (acc: 0.9161)\n\nEpoch 5/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/781 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"  Train Loss: 0.2152 | Train Acc: 0.9342\n  >>> Best model saved (acc: 0.9342)\n\nEpoch 6/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/781 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ba0f4080a40>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    Exception ignored in: if w.is_alive():<function _MultiProcessingDataLoaderIter.__del__ at 0x7ba0f4080a40>\n \n  Traceback (most recent call last):\n    File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n      self._shutdown_workers()^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^    ^if w.is_alive():\n^^^ ^ ^ ^^  ^^ \n ^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^  ^^ ^ ^  ^^ ^ ^ \n    File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^ ^ ^  ^ ^ ^  ^^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n^AssertionError^: can only test a child process^\n^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7ba0f4080a40>\nTraceback (most recent call last):\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n^    self._shutdown_workers()\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^    ^^if w.is_alive():^\n\n AssertionError  :  can only test a child process  ^ ^^^^\n^^Exception ignored in: ^^^<function _MultiProcessingDataLoaderIter.__del__ at 0x7ba0f4080a40>^^\nTraceback (most recent call last):\n\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n        assert self._parent_pid == os.getpid(), 'can only test a child process'self._shutdown_workers()\n\n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n        if w.is_alive(): \n            ^^^^^^^^^^^^^^^^^^^^^^^^\n^^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^^   ^ ^^  ^  ^ ^ ^ ^^^^^^^^^^^^^^^^\nAssertionError^: ^can only test a child process^\n^^Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ba0f4080a40>^\nTraceback (most recent call last):\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    ^self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^    ^^if w.is_alive():\n ^ ^  ^ ^^ ^^ ^^^^^^\n^AssertionError^: can only test a child process\n^^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7ba0f4080a40>^^\n^Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n        assert self._parent_pid == os.getpid(), 'can only test a child process'self._shutdown_workers()\n\n   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n      if w.is_alive(): \n              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    ^^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n ^  ^ ^ ^ ^ ^   ^^ ^^^^^^^^^^^^\n^AssertionError^^: ^can only test a child process\n^^^Exception ignored in: ^^<function _MultiProcessingDataLoaderIter.__del__ at 0x7ba0f4080a40>^\n^^Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n^    self._shutdown_workers()^\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n^^    ^if w.is_alive():\n ^ ^^^ ^^ ^ \nAssertionError  : can only test a child process^\n^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7ba0f4080a40>^\n^Traceback (most recent call last):\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n^^^    self._shutdown_workers()^\n^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    \nif w.is_alive():  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n\n     assert self._parent_pid == os.getpid(), 'can only test a child process'\n             ^^ ^ ^ ^ ^^^^^^^^^^^^^\n^^^^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n^    ^^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n^^ ^ ^ ^^ ^^  ^ ^ ^   ^^^^^^^^^^^^^^^^^\n^AssertionError^: ^can only test a child process^\n^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1696 | Train Acc: 0.9486\n  >>> Best model saved (acc: 0.9486)\n\nEpoch 7/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/781 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"  Train Loss: 0.1243 | Train Acc: 0.9609\n  >>> Best model saved (acc: 0.9609)\n\nEpoch 8/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/781 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"  Train Loss: 0.1130 | Train Acc: 0.9657\n  >>> Best model saved (acc: 0.9657)\n\nEpoch 9/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/781 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"  Train Loss: 0.1126 | Train Acc: 0.9675\n  >>> Best model saved (acc: 0.9675)\n\nEpoch 10/10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/781 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"  Train Loss: 0.0991 | Train Acc: 0.9726\n  >>> Best model saved (acc: 0.9726)\n\n==================================================\nTraining completed! Best train accuracy: 0.9726\nModel saved to: vit_base_best.pt\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## 8. Inference on Test Set","metadata":{}},{"cell_type":"code","source":"print(\"INFERENCE ON TEST SET\")\n\n# Load best checkpoint\nprint(f\"Loading checkpoint from {checkpoint_path}...\")\ncheckpoint = torch.load(checkpoint_path, map_location=DEVICE)\nclasses = checkpoint['classes']\n\nmodel = ViTBaseForAction(num_classes=len(classes), pretrained_name=PRETRAINED_NAME).to(DEVICE)\nmodel.load_state_dict(checkpoint['model'])\nmodel.eval()\nprint(f\"Model loaded (trained acc: {checkpoint['acc']:.4f})\")\n\n# Load test dataset\nprint(\"\\nLoading test dataset...\")\ntest_dataset = TestDataset(PATH_DATA_TEST, num_frames=NUM_FRAMES, image_size=IMG_SIZE)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\nprint(f\"Test samples: {len(test_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T08:27:44.649844Z","iopub.execute_input":"2026-01-18T08:27:44.650162Z","iopub.status.idle":"2026-01-18T08:28:27.118927Z","shell.execute_reply.started":"2026-01-18T08:27:44.650126Z","shell.execute_reply":"2026-01-18T08:28:27.118319Z"}},"outputs":[{"name":"stdout","text":"INFERENCE ON TEST SET\nLoading checkpoint from vit_base_best.pt...\nModel loaded (trained acc: 0.9726)\n\nLoading test dataset...\nTest samples: 510\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"print(\"\\nRunning inference...\")\npredictions = []\n\nwith torch.no_grad():\n    for videos, video_ids in tqdm(test_loader, desc=\"Inference\"):\n        videos = videos.to(DEVICE)\n        logits = model(videos)\n        preds = logits.argmax(dim=1)\n        \n        for video_id, pred_idx in zip(video_ids.cpu().numpy(), preds.cpu().numpy()):\n            pred_class = classes[pred_idx]\n            predictions.append((video_id, pred_class))\n\npredictions.sort(key=lambda x: x[0])\nprint(f\"\\nTotal predictions: {len(predictions)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T08:28:27.119644Z","iopub.execute_input":"2026-01-18T08:28:27.119880Z","iopub.status.idle":"2026-01-18T08:30:07.176386Z","shell.execute_reply.started":"2026-01-18T08:28:27.119853Z","shell.execute_reply":"2026-01-18T08:30:07.175481Z"}},"outputs":[{"name":"stdout","text":"\nRunning inference...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Inference:   0%|          | 0/64 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"595c676880014758a38983f2050d7fc5"}},"metadata":{}},{"name":"stdout","text":"\nTotal predictions: 510\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## 9. Evaluate with Ground Truth Labels","metadata":{}},{"cell_type":"code","source":"!gdown \"1Xv2CWOqdBj3kt0rkNJKRsodSIEd3-wX_\" -O test_labels.csv -q\nprint(\"Downloaded test_labels.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T08:30:07.177758Z","iopub.execute_input":"2026-01-18T08:30:07.178043Z","iopub.status.idle":"2026-01-18T08:30:09.531951Z","shell.execute_reply.started":"2026-01-18T08:30:07.178011Z","shell.execute_reply":"2026-01-18T08:30:09.531231Z"}},"outputs":[{"name":"stdout","text":"Downloaded test_labels.csv\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Load ground truth\ngt_df = pd.read_csv(\"test_labels.csv\")\ntest_labels = dict(zip(gt_df['id'].astype(str), gt_df['class']))\n\n# Match predictions with ground truth\ny_pred = []\ny_true = []\nfor video_id, pred_class in predictions:\n    video_id_str = str(video_id)\n    if video_id_str in test_labels:\n        y_pred.append(pred_class)\n        y_true.append(test_labels[video_id_str])\n\n# Calculate accuracy\naccuracy = accuracy_score(y_true, y_pred)\n\nprint(\"=\" * 50)\nprint(\"VIT-BASE TEST SET EVALUATION\")\nprint(\"=\" * 50)\nprint(f\"Total: {len(y_true)} | Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\nprint()\nprint(\"Comparison:\")\nprint(f\"  ViT-Small (baseline): 63.92%\")\nprint(f\"  ViT-Base (this):      {accuracy*100:.2f}%\")\nprint(f\"  Improvement:          {(accuracy - 0.6392)*100:+.2f}%\")\nprint()\nprint(classification_report(y_true, y_pred, zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T08:30:09.533266Z","iopub.execute_input":"2026-01-18T08:30:09.533627Z","iopub.status.idle":"2026-01-18T08:30:09.574502Z","shell.execute_reply.started":"2026-01-18T08:30:09.533600Z","shell.execute_reply":"2026-01-18T08:30:09.573932Z"}},"outputs":[{"name":"stdout","text":"==================================================\nVIT-BASE TEST SET EVALUATION\n==================================================\nTotal: 510 | Accuracy: 0.6804 (68.04%)\n\nComparison:\n  ViT-Small (baseline): 63.92%\n  ViT-Base (this):      68.04%\n  Improvement:          +4.12%\n\n                precision    recall  f1-score   support\n\n    brush_hair       1.00      0.70      0.82        10\n     cartwheel       0.73      0.80      0.76        10\n         catch       0.88      0.70      0.78        10\n          chew       0.89      0.80      0.84        10\n          clap       1.00      0.90      0.95        10\n         climb       1.00      0.90      0.95        10\n  climb_stairs       0.67      0.60      0.63        10\n          dive       0.57      0.80      0.67        10\n    draw_sword       1.00      0.90      0.95        10\n       dribble       0.60      0.90      0.72        10\n         drink       0.78      0.70      0.74        10\n           eat       0.38      0.30      0.33        10\n    fall_floor       0.35      0.70      0.47        10\n       fencing       0.91      1.00      0.95        10\n     flic_flac       1.00      0.60      0.75        10\n          golf       1.00      0.80      0.89        10\n     handstand       0.82      0.90      0.86        10\n           hit       0.75      0.30      0.43        10\n           hug       0.83      0.50      0.62        10\n          jump       0.00      0.00      0.00        10\n          kick       0.00      0.00      0.00        10\n     kick_ball       0.80      0.80      0.80        10\n          kiss       1.00      1.00      1.00        10\n         laugh       0.80      0.40      0.53        10\n          pick       0.56      0.50      0.53        10\n          pour       0.56      1.00      0.71        10\n        pullup       1.00      1.00      1.00        10\n         punch       1.00      0.40      0.57        10\n          push       1.00      0.80      0.89        10\n        pushup       0.91      1.00      0.95        10\n     ride_bike       1.00      1.00      1.00        10\n    ride_horse       0.77      1.00      0.87        10\n           run       0.22      0.40      0.29        10\n   shake_hands       0.56      0.90      0.69        10\n    shoot_ball       0.64      0.70      0.67        10\n     shoot_bow       0.90      0.90      0.90        10\n     shoot_gun       0.73      0.80      0.76        10\n           sit       0.40      0.40      0.40        10\n         situp       0.91      1.00      0.95        10\n         smile       0.62      0.80      0.70        10\n         smoke       0.70      0.70      0.70        10\n    somersault       0.69      0.90      0.78        10\n         stand       0.27      0.30      0.29        10\nswing_baseball       0.78      0.70      0.74        10\n         sword       0.89      0.80      0.84        10\nsword_exercise       0.70      0.70      0.70        10\n          talk       0.38      0.50      0.43        10\n         throw       0.38      0.30      0.33        10\n          turn       0.57      0.40      0.47        10\n          walk       0.33      0.50      0.40        10\n          wave       0.60      0.30      0.40        10\n\n      accuracy                           0.68       510\n     macro avg       0.70      0.68      0.67       510\n  weighted avg       0.70      0.68      0.67       510\n\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## 10. Save Submission","metadata":{}},{"cell_type":"code","source":"# Create submission file\nsubmission = pd.DataFrame(predictions, columns=['id', 'class'])\nsubmission.to_csv('submission_vit_base.csv', index=False)\nprint(f\"Saved submission_vit_base.csv ({len(submission)} rows)\")\nprint(submission.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T08:30:09.575205Z","iopub.execute_input":"2026-01-18T08:30:09.575511Z","iopub.status.idle":"2026-01-18T08:30:09.591746Z","shell.execute_reply.started":"2026-01-18T08:30:09.575495Z","shell.execute_reply":"2026-01-18T08:30:09.591082Z"}},"outputs":[{"name":"stdout","text":"Saved submission_vit_base.csv (510 rows)\n   id         class\n0   0  climb_stairs\n1   1     flic_flac\n2   2         climb\n3   3          pick\n4   4          wave\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}