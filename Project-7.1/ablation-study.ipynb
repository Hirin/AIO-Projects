{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VideoMAE Ablation Study: All Experiments\n",
    "\n",
    "**Goal**: Measure impact of each improvement from Paper baseline \u2192 Full Custom (0.87)\n",
    "\n",
    "| Exp | Name | Cumulative Changes |\n",
    "|-----|------|-------------------|\n",
    "| 1 | Paper Baseline | VideoMAE + paper settings |\n",
    "| 2 | + Multi-Segment TTA | Exp1 + 2 temporal \u00d7 3 spatial |\n",
    "| 3 | + Consistent Transform | Exp1 + same crop/flip all frames |\n",
    "| 4 | + Mixup | Exp3 + Mixup \u03b1=0.8 |\n",
    "| 5 | + Label Smoothing | Exp3 + \u03b5=0.1 |\n",
    "| 6 | + 2-Stage | Exp4 + Phase1\u2192Phase2 |\n",
    "| 7 | + Flip TTA | Exp6 + 6-view flip TTA |\n",
    "| 8 | Full Custom | = videoMAE.ipynb (target 0.87) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Setup & Shared Imports\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from transformers import VideoMAEForVideoClassification, VideoMAEImageProcessor\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(42)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# Results tracking\n",
    "RESULTS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Configuration\n",
    "\n",
    "PATH_DATA_TRAIN = Path('/kaggle/input/action-video/data/data_train')\n",
    "PATH_DATA_TEST = Path('/kaggle/input/action-video/data/test')\n",
    "\n",
    "MODEL_CKPT = \"MCG-NJU/videomae-base-finetuned-kinetics\"\n",
    "NUM_FRAMES = 16\n",
    "IMAGE_SIZE = 224\n",
    "RESIZE_SIZE = 256  # Resize short edge before crop\n",
    "BATCH_SIZE = 16\n",
    "GRAD_ACCUM_STEPS = 2\n",
    "EPOCHS = 4  # Quick test (use 30 for full)\n",
    "WEIGHT_DECAY = 0.05\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "# Paper LR scaling\n",
    "BASE_LR = 1e-3\n",
    "EFFECTIVE_BATCH = BATCH_SIZE * GRAD_ACCUM_STEPS\n",
    "LR = BASE_LR * EFFECTIVE_BATCH / 256\n",
    "\n",
    "processor = VideoMAEImageProcessor.from_pretrained(MODEL_CKPT)\n",
    "MEAN = processor.image_mean\n",
    "STD = processor.image_std\n",
    "\n",
    "print(f\"LR: {LR:.2e}, Effective Batch: {EFFECTIVE_BATCH}\")\n",
    "print(f\"Norm - Mean: {MEAN}, Std: {STD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Transform Classes\n",
    "\n",
    "class VideoTransformBaseline:\n",
    "    \"\"\"Baseline: Per-frame independent random transforms.\"\"\"\n",
    "    def __init__(self, is_train=True):\n",
    "        self.is_train = is_train\n",
    "    \n",
    "    def __call__(self, frames):\n",
    "        # frames: [T, C, H, W] tensor\n",
    "        if self.is_train:\n",
    "            h, w = frames.shape[-2:]\n",
    "            scale = random.uniform(0.8, 1.0)\n",
    "            new_h, new_w = int(h * scale), int(w * scale)\n",
    "            frames = TF.resize(frames, [new_h, new_w])\n",
    "            i = random.randint(0, max(0, new_h - IMAGE_SIZE))\n",
    "            j = random.randint(0, max(0, new_w - IMAGE_SIZE))\n",
    "            frames = TF.crop(frames, i, j, min(IMAGE_SIZE, new_h), min(IMAGE_SIZE, new_w))\n",
    "            frames = TF.resize(frames, [IMAGE_SIZE, IMAGE_SIZE])\n",
    "            if random.random() < 0.5:\n",
    "                frames = TF.hflip(frames)\n",
    "        else:\n",
    "            frames = TF.resize(frames, [IMAGE_SIZE, IMAGE_SIZE])\n",
    "        return torch.stack([TF.normalize(f, MEAN, STD) for f in frames])\n",
    "\n",
    "\n",
    "class VideoTransformConsistent:\n",
    "    \"\"\"Consistent: Same crop/flip params for ALL frames.\"\"\"\n",
    "    def __init__(self, is_train=True):\n",
    "        self.is_train = is_train\n",
    "    \n",
    "    def __call__(self, frames):\n",
    "        # frames: list of PIL Images\n",
    "        frames = [TF.resize(img, RESIZE_SIZE) for img in frames]\n",
    "        \n",
    "        if self.is_train:\n",
    "            # Get SAME random params for all frames\n",
    "            i, j, h, w = T.RandomResizedCrop.get_params(\n",
    "                frames[0], scale=(0.8, 1.0), ratio=(0.75, 1.33)\n",
    "            )\n",
    "            is_flip = random.random() > 0.5\n",
    "            \n",
    "            transformed = []\n",
    "            for img in frames:\n",
    "                img = TF.resized_crop(img, i, j, h, w, size=(IMAGE_SIZE, IMAGE_SIZE))\n",
    "                if is_flip:\n",
    "                    img = TF.hflip(img)\n",
    "                img = TF.to_tensor(img)\n",
    "                img = TF.normalize(img, MEAN, STD)\n",
    "                transformed.append(img)\n",
    "            return torch.stack(transformed)\n",
    "        else:\n",
    "            transformed = []\n",
    "            for img in frames:\n",
    "                img = TF.center_crop(img, IMAGE_SIZE)\n",
    "                img = TF.to_tensor(img)\n",
    "                img = TF.normalize(img, MEAN, STD)\n",
    "                transformed.append(img)\n",
    "            return torch.stack(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Dataset Classes\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, root, transform, use_pil=False):\n",
    "        self.root = Path(root)\n",
    "        self.transform = transform\n",
    "        self.use_pil = use_pil  # True for ConsistentTransform\n",
    "        \n",
    "        self.classes = sorted([d.name for d in self.root.iterdir() if d.is_dir()])\n",
    "        self.class_to_idx = {c: i for i, c in enumerate(self.classes)}\n",
    "        \n",
    "        self.samples = []\n",
    "        for cls in self.classes:\n",
    "            for vid in (self.root / cls).iterdir():\n",
    "                if vid.is_dir():\n",
    "                    self.samples.append((vid, self.class_to_idx[cls]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        vid_dir, label = self.samples[idx]\n",
    "        files = sorted(vid_dir.glob('*.jpg'))\n",
    "        total = len(files)\n",
    "        indices = torch.linspace(0, total-1, NUM_FRAMES).long()\n",
    "        \n",
    "        if self.use_pil:\n",
    "            frames = [Image.open(files[i]).convert('RGB') for i in indices]\n",
    "        else:\n",
    "            frames = [TF.to_tensor(Image.open(files[i]).convert('RGB')) for i in indices]\n",
    "            frames = torch.stack(frames)\n",
    "        \n",
    "        frames = self.transform(frames)\n",
    "        return frames, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Test Dataset Variants\n",
    "\n",
    "class TestDatasetSingle(Dataset):\n",
    "    \"\"\"Single center crop (no TTA).\"\"\"\n",
    "    def __init__(self, root):\n",
    "        self.root = Path(root)\n",
    "        self.samples = [(d, int(d.name)) for d in self.root.iterdir() if d.is_dir()]\n",
    "        self.samples.sort(key=lambda x: x[1])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        vid_dir, vid_id = self.samples[idx]\n",
    "        files = sorted(vid_dir.glob('*.jpg'))\n",
    "        indices = torch.linspace(0, len(files)-1, NUM_FRAMES).long()\n",
    "        \n",
    "        frames = []\n",
    "        for i in indices:\n",
    "            img = Image.open(files[i]).convert('RGB')\n",
    "            img = TF.resize(img, RESIZE_SIZE)\n",
    "            img = TF.center_crop(img, IMAGE_SIZE)\n",
    "            img = TF.to_tensor(img)\n",
    "            img = TF.normalize(img, MEAN, STD)\n",
    "            frames.append(img)\n",
    "        return torch.stack(frames), vid_id\n",
    "\n",
    "\n",
    "class TestDatasetMultiSegment(Dataset):\n",
    "    \"\"\"Paper-like TTA: N temporal segments \u00d7 3 spatial crops.\"\"\"\n",
    "    def __init__(self, root, num_segments=2):\n",
    "        self.root = Path(root)\n",
    "        self.num_segments = num_segments\n",
    "        self.samples = [(d, int(d.name)) for d in self.root.iterdir() if d.is_dir()]\n",
    "        self.samples.sort(key=lambda x: x[1])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        vid_dir, vid_id = self.samples[idx]\n",
    "        files = sorted(vid_dir.glob('*.jpg'))\n",
    "        total = len(files)\n",
    "        \n",
    "        views = []\n",
    "        for seg in range(self.num_segments):\n",
    "            # Different temporal segment\n",
    "            seg_len = total // self.num_segments\n",
    "            start = seg * seg_len\n",
    "            end = min(start + seg_len, total)\n",
    "            seg_indices = torch.linspace(start, end-1, NUM_FRAMES).long()\n",
    "            \n",
    "            frames = [Image.open(files[i]).convert('RGB') for i in seg_indices]\n",
    "            frames = [TF.resize(img, RESIZE_SIZE) for img in frames]\n",
    "            \n",
    "            w, h = frames[0].size  # PIL: (width, height)\n",
    "            # 3 spatial crops: left, center, right\n",
    "            crop_positions = [\n",
    "                (0, 0),  # top-left\n",
    "                ((h - IMAGE_SIZE) // 2, (w - IMAGE_SIZE) // 2),  # center\n",
    "                (h - IMAGE_SIZE, w - IMAGE_SIZE),  # bottom-right\n",
    "            ]\n",
    "            \n",
    "            for top, left in crop_positions:\n",
    "                top = max(0, min(top, h - IMAGE_SIZE))\n",
    "                left = max(0, min(left, w - IMAGE_SIZE))\n",
    "                view_frames = []\n",
    "                for img in frames:\n",
    "                    cropped = TF.crop(img, top, left, IMAGE_SIZE, IMAGE_SIZE)\n",
    "                    t = TF.to_tensor(cropped)\n",
    "                    t = TF.normalize(t, MEAN, STD)\n",
    "                    view_frames.append(t)\n",
    "                views.append(torch.stack(view_frames))\n",
    "        \n",
    "        return torch.stack(views), vid_id  # [num_segments*3, T, C, H, W]\n",
    "\n",
    "\n",
    "class TestDatasetFlipTTA(Dataset):\n",
    "    \"\"\"Custom TTA: 3 spatial crops + 3 flipped = 6 views.\"\"\"\n",
    "    def __init__(self, root):\n",
    "        self.root = Path(root)\n",
    "        self.samples = [(d, int(d.name)) for d in self.root.iterdir() if d.is_dir()]\n",
    "        self.samples.sort(key=lambda x: x[1])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        vid_dir, vid_id = self.samples[idx]\n",
    "        files = sorted(vid_dir.glob('*.jpg'))\n",
    "        indices = torch.linspace(0, len(files)-1, NUM_FRAMES).long()\n",
    "        \n",
    "        frames = [Image.open(files[i]).convert('RGB') for i in indices]\n",
    "        frames = [TF.resize(img, RESIZE_SIZE) for img in frames]\n",
    "        \n",
    "        w, h = frames[0].size\n",
    "        crop_positions = [\n",
    "            ((h - IMAGE_SIZE) // 2, (w - IMAGE_SIZE) // 2),  # center\n",
    "            (0, (w - IMAGE_SIZE) // 2),  # top-center\n",
    "            (h - IMAGE_SIZE, (w - IMAGE_SIZE) // 2),  # bottom-center\n",
    "        ]\n",
    "        \n",
    "        views = []\n",
    "        for top, left in crop_positions:\n",
    "            top = max(0, min(top, h - IMAGE_SIZE))\n",
    "            left = max(0, min(left, w - IMAGE_SIZE))\n",
    "            view_frames = []\n",
    "            for img in frames:\n",
    "                cropped = TF.crop(img, top, left, IMAGE_SIZE, IMAGE_SIZE)\n",
    "                t = TF.to_tensor(cropped)\n",
    "                t = TF.normalize(t, MEAN, STD)\n",
    "                view_frames.append(t)\n",
    "            views.append(torch.stack(view_frames))\n",
    "        \n",
    "        # Flipped versions\n",
    "        for top, left in crop_positions:\n",
    "            top = max(0, min(top, h - IMAGE_SIZE))\n",
    "            left = max(0, min(left, w - IMAGE_SIZE))\n",
    "            view_frames = []\n",
    "            for img in frames:\n",
    "                cropped = TF.crop(img, top, left, IMAGE_SIZE, IMAGE_SIZE)\n",
    "                cropped = TF.hflip(cropped)\n",
    "                t = TF.to_tensor(cropped)\n",
    "                t = TF.normalize(t, MEAN, STD)\n",
    "                view_frames.append(t)\n",
    "            views.append(torch.stack(view_frames))\n",
    "        \n",
    "        return torch.stack(views), vid_id  # [6, T, C, H, W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Mixup Collate Function\n",
    "\n",
    "class MixupCollate:\n",
    "    def __init__(self, num_classes, alpha=0.8):\n",
    "        self.num_classes = num_classes\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        inputs, targets = torch.utils.data.default_collate(batch)\n",
    "        batch_size = inputs.size(0)\n",
    "        \n",
    "        lam = np.random.beta(self.alpha, self.alpha)\n",
    "        index = torch.randperm(batch_size)\n",
    "        \n",
    "        inputs = lam * inputs + (1 - lam) * inputs[index]\n",
    "        targets_onehot = F.one_hot(targets, self.num_classes).float()\n",
    "        targets = lam * targets_onehot + (1 - lam) * targets_onehot[index]\n",
    "        \n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. Training & Evaluation Functions\n",
    "\n",
    "def train_epoch(model, loader, optimizer, scheduler, scaler, use_mixup=False, label_smoothing=0.0):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for batch_idx, (videos, targets) in enumerate(tqdm(loader, leave=False)):\n",
    "        videos = videos.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "        \n",
    "        with torch.amp.autocast(device_type='cuda'):\n",
    "            outputs = model(videos)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            if use_mixup:\n",
    "                log_probs = F.log_softmax(logits, dim=1)\n",
    "                loss = -torch.sum(targets * log_probs, dim=1).mean()\n",
    "                true_labels = targets.argmax(dim=1)\n",
    "            else:\n",
    "                loss = F.cross_entropy(logits, targets, label_smoothing=label_smoothing)\n",
    "                true_labels = targets\n",
    "        \n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == true_labels).sum().item()\n",
    "        total += true_labels.size(0)\n",
    "        total_loss += loss.item() * true_labels.size(0)\n",
    "        \n",
    "        loss = loss / GRAD_ACCUM_STEPS\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (batch_idx + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "    \n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, loader, multi_view=False, id2label=None):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, vid_ids in tqdm(loader, leave=False, desc=\"Eval\"):\n",
    "            if multi_view:\n",
    "                B, V, T, C, H, W = data.shape\n",
    "                data = data.view(B * V, T, C, H, W).to(DEVICE)\n",
    "                outputs = model(data)\n",
    "                logits = outputs.logits.view(B, V, -1).mean(dim=1)\n",
    "            else:\n",
    "                data = data.to(DEVICE)\n",
    "                outputs = model(data)\n",
    "                logits = outputs.logits\n",
    "            \n",
    "            preds = logits.argmax(dim=1)\n",
    "            for vid, pred in zip(vid_ids.tolist(), preds.tolist()):\n",
    "                predictions.append((vid, id2label[pred]))\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. Load Test Labels\n",
    "\n",
    "!gdown \"1Xv2CWOqdBj3kt0rkNJKRsodSIEd3-wX_\" -O test_labels.csv -q\n",
    "gt_df = pd.read_csv(\"test_labels.csv\")\n",
    "GT_LABELS = dict(zip(gt_df['id'].astype(str), gt_df['class']))\n",
    "\n",
    "def calc_accuracy(predictions):\n",
    "    y_pred, y_true = [], []\n",
    "    for vid_id, pred_cls in predictions:\n",
    "        if str(vid_id) in GT_LABELS:\n",
    "            y_pred.append(pred_cls)\n",
    "            y_true.append(GT_LABELS[str(vid_id)])\n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Loaded {len(GT_LABELS)} test labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9. Experiment Runner\n",
    "\n",
    "def run_experiment(\n",
    "    exp_name, train_dataset, test_dataset,\n",
    "    epochs=EPOCHS, use_mixup=False, label_smoothing=0.0,\n",
    "    multi_view=False, two_stage=False, mixup_collate=None\n",
    "):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EXPERIMENT: {exp_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    seed_everything(42)  # Reset seed for reproducibility\n",
    "    \n",
    "    # Create fresh model\n",
    "    label2id = train_dataset.class_to_idx\n",
    "    id2label = {v: k for k, v in label2id.items()}\n",
    "    \n",
    "    model = VideoMAEForVideoClassification.from_pretrained(\n",
    "        MODEL_CKPT, label2id=label2id, id2label=id2label,\n",
    "        ignore_mismatched_sizes=True, num_frames=NUM_FRAMES\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "        num_workers=2, pin_memory=True, drop_last=True,\n",
    "        collate_fn=mixup_collate\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=4 if multi_view else BATCH_SIZE,\n",
    "        shuffle=False, num_workers=2\n",
    "    )\n",
    "    \n",
    "    # Optimizer & Scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scaler = torch.amp.GradScaler()\n",
    "    num_steps = len(train_loader) * epochs // GRAD_ACCUM_STEPS\n",
    "    num_warmup = int(num_steps * WARMUP_RATIO)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup, num_steps)\n",
    "    \n",
    "    # Phase 1 Training\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        loss, acc = train_epoch(\n",
    "            model, train_loader, optimizer, scheduler, scaler,\n",
    "            use_mixup=use_mixup, label_smoothing=label_smoothing\n",
    "        )\n",
    "        print(f\"  Epoch {epoch+1}/{epochs}: Loss={loss:.4f}, Acc={acc:.4f}\")\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), f'{exp_name}_best.pt')\n",
    "    \n",
    "    # Phase 2 (if 2-stage)\n",
    "    if two_stage:\n",
    "        print(\"  --> Phase 2: Fine-tuning with Label Smoothing...\")\n",
    "        model.load_state_dict(torch.load(f'{exp_name}_best.pt'))\n",
    "        \n",
    "        # Phase 2 settings: low LR, no mixup, label smoothing\n",
    "        p2_epochs = 3\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6, weight_decay=WEIGHT_DECAY)\n",
    "        num_steps_p2 = len(train_loader) * p2_epochs // GRAD_ACCUM_STEPS\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, 0, num_steps_p2)\n",
    "        \n",
    "        p2_loader = DataLoader(\n",
    "            train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "            num_workers=2, pin_memory=True, drop_last=True\n",
    "        )\n",
    "        \n",
    "        for epoch in range(p2_epochs):\n",
    "            loss, acc = train_epoch(\n",
    "                model, p2_loader, optimizer, scheduler, scaler,\n",
    "                use_mixup=False, label_smoothing=0.1\n",
    "            )\n",
    "            print(f\"  P2 Epoch {epoch+1}/{p2_epochs}: Loss={loss:.4f}, Acc={acc:.4f}\")\n",
    "            best_acc = max(best_acc, acc)\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(f'{exp_name}_best.pt'))\n",
    "    \n",
    "    # Evaluate\n",
    "    predictions = evaluate(model, test_loader, multi_view=multi_view, id2label=id2label)\n",
    "    test_acc = calc_accuracy(predictions)\n",
    "    \n",
    "    print(f\"\\n  >>> TEST ACCURACY: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "    RESULTS.append({'exp': exp_name, 'train_acc': best_acc, 'test_acc': test_acc})\n",
    "    \n",
    "    # Cleanup\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ========================================\n",
    "## PHASE 1: PAPER BASELINE EXPERIMENTS\n",
    "## ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exp 1: Paper Baseline (VideoMAE + paper settings, single view)\n",
    "\n",
    "train_ds_baseline = VideoDataset(\n",
    "    PATH_DATA_TRAIN, VideoTransformBaseline(is_train=True), use_pil=False\n",
    ")\n",
    "test_ds_single = TestDatasetSingle(PATH_DATA_TEST)\n",
    "\n",
    "run_experiment(\"Exp1_Paper_Baseline\", train_ds_baseline, test_ds_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exp 2: Paper + Multi-Segment TTA\n",
    "\n",
    "test_ds_multi = TestDatasetMultiSegment(PATH_DATA_TEST, num_segments=2)\n",
    "\n",
    "run_experiment(\n",
    "    \"Exp2_MultiSegment_TTA\", train_ds_baseline, test_ds_multi,\n",
    "    multi_view=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ========================================\n",
    "## PHASE 2: CUSTOM IMPROVEMENTS (ABLATION)\n",
    "## ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exp 3: Consistent Spatial Transforms\n",
    "\n",
    "train_ds_consistent = VideoDataset(\n",
    "    PATH_DATA_TRAIN, VideoTransformConsistent(is_train=True), use_pil=True\n",
    ")\n",
    "\n",
    "run_experiment(\"Exp3_Consistent_Transform\", train_ds_consistent, test_ds_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exp 4: Consistent + Mixup\n",
    "\n",
    "mixup_collate = MixupCollate(num_classes=len(train_ds_consistent.classes), alpha=0.8)\n",
    "\n",
    "run_experiment(\n",
    "    \"Exp4_Mixup\", train_ds_consistent, test_ds_single,\n",
    "    use_mixup=True, mixup_collate=mixup_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exp 5: Consistent + Label Smoothing (no mixup)\n",
    "\n",
    "run_experiment(\n",
    "    \"Exp5_LabelSmoothing\", train_ds_consistent, test_ds_single,\n",
    "    label_smoothing=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exp 6: Consistent + Mixup + 2-Stage\n",
    "\n",
    "run_experiment(\n",
    "    \"Exp6_2Stage\", train_ds_consistent, test_ds_single,\n",
    "    use_mixup=True, mixup_collate=mixup_collate, two_stage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exp 7: Full Custom + Flip TTA (= videoMAE.ipynb)\n",
    "\n",
    "test_ds_flip = TestDatasetFlipTTA(PATH_DATA_TEST)\n",
    "\n",
    "run_experiment(\n",
    "    \"Exp7_FlipTTA_Full\", train_ds_consistent, test_ds_flip,\n",
    "    use_mixup=True, mixup_collate=mixup_collate, two_stage=True, multi_view=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exp 8: Summary\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXP 8: Full Custom Config (= videoMAE.ipynb)\")\n",
    "print(\"=\"*60)\n",
    "print(\"Same as Exp 7 but with 40 epochs (30+10) to reach 0.87\")\n",
    "print(f\"Current Exp 7 result with {EPOCHS} epochs: {RESULTS[-1]['test_acc']:.4f}\")\n",
    "print(\"\\nTo reproduce 0.87, run with EPOCHS=30 (Phase 1) + 10 (Phase 2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ========================================\n",
    "## FINAL RESULTS SUMMARY\n",
    "## ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL EXPERIMENTS RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results_df = pd.DataFrame(RESULTS)\n",
    "baseline_acc = results_df['test_acc'].iloc[0]\n",
    "results_df['delta'] = (results_df['test_acc'] - baseline_acc) * 100\n",
    "results_df['delta'] = results_df['delta'].apply(lambda x: f\"+{x:.2f}%\" if x > 0 else f\"{x:.2f}%\")\n",
    "\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nBaseline: {baseline_acc:.4f} ({baseline_acc*100:.2f}%)\")\n",
    "print(f\"Best: {results_df['test_acc'].max():.4f} ({results_df['test_acc'].max()*100:.2f}%)\")\n",
    "print(f\"Improvement: +{(results_df['test_acc'].max() - baseline_acc)*100:.2f}%\")\n",
    "\n",
    "results_df.to_csv('ablation_results.csv', index=False)\n",
    "print(\"\\nResults saved to ablation_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}