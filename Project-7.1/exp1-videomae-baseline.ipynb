{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Experiment 1: VideoMAE Baseline (Paper-like Settings)\n",
                "\n",
                "**Objective**: Replace ViT with VideoMAE following paper settings.\n",
                "\n",
                "**Changes from Baseline**:\n",
                "- Model: `vit_small_patch16_224` → `VideoMAE-base-finetuned-kinetics`\n",
                "- Normalization: `[0.5,0.5,0.5]` → VideoMAE processor stats\n",
                "- **LR**: Linear scaling rule (paper)\n",
                "- **Scheduler**: Cosine with warmup (paper)\n",
                "- Rest: Keep baseline data pipeline, 4 epochs for quick test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## 1. Setup & Imports\n",
                "\n",
                "import os\n",
                "import random\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from pathlib import Path\n",
                "from PIL import Image\n",
                "from tqdm.auto import tqdm\n",
                "import torchvision.transforms.functional as TF\n",
                "from torchvision.transforms import InterpolationMode\n",
                "from transformers import VideoMAEForVideoClassification, VideoMAEImageProcessor\n",
                "from transformers import get_cosine_schedule_with_warmup\n",
                "\n",
                "# Reproducibility\n",
                "def seed_everything(seed=42):\n",
                "    random.seed(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    torch.cuda.manual_seed_all(seed)\n",
                "    torch.backends.cudnn.deterministic = True\n",
                "\n",
                "seed_everything(42)\n",
                "\n",
                "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {DEVICE}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## 2. Configuration (Paper-like Settings)\n",
                "\n",
                "# Paths (Kaggle)\n",
                "PATH_DATA_TRAIN = Path('/kaggle/input/action-video/data/data_train')\n",
                "PATH_DATA_TEST = Path('/kaggle/input/action-video/data/test')\n",
                "\n",
                "# Model\n",
                "MODEL_CKPT = \"MCG-NJU/videomae-base-finetuned-kinetics\"\n",
                "\n",
                "# Training params\n",
                "NUM_FRAMES = 16\n",
                "FRAME_STRIDE = 2\n",
                "IMAGE_SIZE = 224\n",
                "BATCH_SIZE = 8\n",
                "GRAD_ACCUM_STEPS = 4  # Effective batch = 8 * 4 = 32\n",
                "EPOCHS = 4  # Quick test (paper uses 50)\n",
                "\n",
                "# Paper-like LR settings\n",
                "BASE_LR = 1e-3  # Paper base LR\n",
                "EFFECTIVE_BATCH = BATCH_SIZE * GRAD_ACCUM_STEPS  # 32\n",
                "LR = BASE_LR * EFFECTIVE_BATCH / 256  # Linear scaling: 1e-3 * 32/256 = 1.25e-4\n",
                "\n",
                "WEIGHT_DECAY = 0.05\n",
                "WARMUP_RATIO = 0.1  # 10% warmup (paper uses ~5 epochs for 50 epochs)\n",
                "\n",
                "# Get normalization stats from processor\n",
                "processor = VideoMAEImageProcessor.from_pretrained(MODEL_CKPT)\n",
                "MEAN = processor.image_mean\n",
                "STD = processor.image_std\n",
                "\n",
                "print(f\"Paper-like Settings:\")\n",
                "print(f\"  Effective Batch Size: {EFFECTIVE_BATCH}\")\n",
                "print(f\"  LR (scaled): {LR:.2e}\")\n",
                "print(f\"  Warmup Ratio: {WARMUP_RATIO}\")\n",
                "print(f\"  Weight Decay: {WEIGHT_DECAY}\")\n",
                "print(f\"  Normalization - Mean: {MEAN}, Std: {STD}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## 3. Dataset & Transforms (Same as Baseline)\n",
                "\n",
                "class VideoTransform:\n",
                "    \"\"\"Same as baseline but with VideoMAE normalization.\"\"\"\n",
                "    def __init__(self, image_size=224, is_train=True):\n",
                "        self.image_size = image_size\n",
                "        self.is_train = is_train\n",
                "        self.mean = MEAN\n",
                "        self.std = STD\n",
                "    \n",
                "    def __call__(self, frames):\n",
                "        # frames: [T, C, H, W]\n",
                "        if self.is_train:\n",
                "            h, w = frames.shape[-2:]\n",
                "            scale = random.uniform(0.8, 1.0)\n",
                "            new_h, new_w = int(h * scale), int(w * scale)\n",
                "            frames = TF.resize(frames, [new_h, new_w], interpolation=InterpolationMode.BILINEAR)\n",
                "            i = random.randint(0, max(0, new_h - self.image_size))\n",
                "            j = random.randint(0, max(0, new_w - self.image_size))\n",
                "            frames = TF.crop(frames, i, j, min(self.image_size, new_h), min(self.image_size, new_w))\n",
                "            frames = TF.resize(frames, [self.image_size, self.image_size], interpolation=InterpolationMode.BILINEAR)\n",
                "            if random.random() < 0.5:\n",
                "                frames = TF.hflip(frames)\n",
                "        else:\n",
                "            frames = TF.resize(frames, [self.image_size, self.image_size], interpolation=InterpolationMode.BILINEAR)\n",
                "        \n",
                "        normalized = [TF.normalize(frame, self.mean, self.std) for frame in frames]\n",
                "        return torch.stack(normalized)\n",
                "\n",
                "\n",
                "class VideoDataset(Dataset):\n",
                "    \"\"\"Same as baseline dataset.\"\"\"\n",
                "    def __init__(self, root, transform=None, num_frames=16, frame_stride=2):\n",
                "        self.root = Path(root)\n",
                "        self.transform = transform\n",
                "        self.num_frames = num_frames\n",
                "        self.frame_stride = frame_stride\n",
                "        \n",
                "        self.classes = sorted([d.name for d in self.root.iterdir() if d.is_dir()])\n",
                "        self.class_to_idx = {c: i for i, c in enumerate(self.classes)}\n",
                "        \n",
                "        self.samples = []\n",
                "        for class_name in self.classes:\n",
                "            class_dir = self.root / class_name\n",
                "            for video_dir in class_dir.iterdir():\n",
                "                if video_dir.is_dir():\n",
                "                    self.samples.append((video_dir, self.class_to_idx[class_name]))\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.samples)\n",
                "    \n",
                "    def _select_indices(self, total):\n",
                "        if total <= 0:\n",
                "            raise ValueError(\"No frames\")\n",
                "        if total == 1:\n",
                "            return torch.zeros(self.num_frames, dtype=torch.long)\n",
                "        steps = max(self.num_frames * self.frame_stride, self.num_frames)\n",
                "        grid = torch.linspace(0, total - 1, steps=steps)\n",
                "        idxs = grid[::self.frame_stride].long()\n",
                "        if idxs.numel() < self.num_frames:\n",
                "            pad = idxs.new_full((self.num_frames - idxs.numel(),), idxs[-1].item())\n",
                "            idxs = torch.cat([idxs, pad], dim=0)\n",
                "        return idxs[:self.num_frames]\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        video_dir, label = self.samples[idx]\n",
                "        frame_files = sorted(video_dir.glob('*.jpg'))\n",
                "        \n",
                "        if len(frame_files) == 0:\n",
                "            raise ValueError(f\"No frames in {video_dir}\")\n",
                "        \n",
                "        indices = self._select_indices(len(frame_files))\n",
                "        frames = []\n",
                "        for i in indices:\n",
                "            img = Image.open(frame_files[i]).convert('RGB')\n",
                "            frames.append(TF.to_tensor(img))\n",
                "        \n",
                "        frames = torch.stack(frames)  # [T, C, H, W]\n",
                "        \n",
                "        if self.transform:\n",
                "            frames = self.transform(frames)\n",
                "        \n",
                "        return frames, label"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## 4. Create Datasets & DataLoaders\n",
                "\n",
                "train_transform = VideoTransform(image_size=IMAGE_SIZE, is_train=True)\n",
                "test_transform = VideoTransform(image_size=IMAGE_SIZE, is_train=False)\n",
                "\n",
                "train_dataset = VideoDataset(PATH_DATA_TRAIN, transform=train_transform, \n",
                "                              num_frames=NUM_FRAMES, frame_stride=FRAME_STRIDE)\n",
                "\n",
                "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
                "                          num_workers=2, pin_memory=True, drop_last=True)\n",
                "\n",
                "print(f\"Classes: {len(train_dataset.classes)}\")\n",
                "print(f\"Training samples: {len(train_dataset)}\")\n",
                "print(f\"Batches per epoch: {len(train_loader)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## 5. Load VideoMAE Model\n",
                "\n",
                "# Create label mappings\n",
                "label2id = train_dataset.class_to_idx\n",
                "id2label = {v: k for k, v in label2id.items()}\n",
                "\n",
                "# Load pretrained VideoMAE\n",
                "model = VideoMAEForVideoClassification.from_pretrained(\n",
                "    MODEL_CKPT,\n",
                "    label2id=label2id,\n",
                "    id2label=id2label,\n",
                "    ignore_mismatched_sizes=True,  # Head: 400 -> 51 classes\n",
                "    num_frames=NUM_FRAMES\n",
                ")\n",
                "model = model.to(DEVICE)\n",
                "\n",
                "# Count parameters\n",
                "total_params = sum(p.numel() for p in model.parameters())\n",
                "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "print(f\"Total params: {total_params:,}\")\n",
                "print(f\"Trainable params: {trainable_params:,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## 6. Training Loop (with Scheduler)\n",
                "\n",
                "def train_one_epoch(model, loader, optimizer, scheduler, scaler, device, grad_accum_steps=1):\n",
                "    model.train()\n",
                "    total_loss = 0.0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    optimizer.zero_grad()\n",
                "    progress = tqdm(loader, desc=\"Training\", leave=False)\n",
                "    \n",
                "    for batch_idx, (videos, labels) in enumerate(progress):\n",
                "        videos = videos.to(device)  # [B, T, C, H, W]\n",
                "        labels = labels.to(device)\n",
                "        \n",
                "        with torch.amp.autocast(device_type='cuda', enabled=True):\n",
                "            outputs = model(videos)\n",
                "            logits = outputs.logits\n",
                "            loss = F.cross_entropy(logits, labels)\n",
                "        \n",
                "        preds = logits.argmax(dim=1)\n",
                "        correct += (preds == labels).sum().item()\n",
                "        total += labels.size(0)\n",
                "        total_loss += loss.item() * labels.size(0)\n",
                "        \n",
                "        loss = loss / grad_accum_steps\n",
                "        scaler.scale(loss).backward()\n",
                "        \n",
                "        should_step = ((batch_idx + 1) % grad_accum_steps == 0) or (batch_idx + 1 == len(loader))\n",
                "        if should_step:\n",
                "            scaler.step(optimizer)\n",
                "            scaler.update()\n",
                "            optimizer.zero_grad()\n",
                "            scheduler.step()  # Step scheduler after each optimizer step\n",
                "        \n",
                "        current_lr = scheduler.get_last_lr()[0]\n",
                "        progress.set_postfix({'loss': total_loss/total, 'acc': correct/total, 'lr': f'{current_lr:.2e}'})\n",
                "    \n",
                "    return total_loss / total, correct / total"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## 7. Train Model (with Cosine Schedule + Warmup)\n",
                "\n",
                "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
                "scaler = torch.amp.GradScaler(enabled=True)\n",
                "\n",
                "# Calculate scheduler steps\n",
                "num_training_steps = len(train_loader) * EPOCHS // GRAD_ACCUM_STEPS\n",
                "num_warmup_steps = int(num_training_steps * WARMUP_RATIO)\n",
                "\n",
                "scheduler = get_cosine_schedule_with_warmup(\n",
                "    optimizer, \n",
                "    num_warmup_steps=num_warmup_steps, \n",
                "    num_training_steps=num_training_steps\n",
                ")\n",
                "\n",
                "print(f\"Training steps: {num_training_steps}\")\n",
                "print(f\"Warmup steps: {num_warmup_steps}\")\n",
                "\n",
                "best_acc = 0.0\n",
                "\n",
                "for epoch in range(EPOCHS):\n",
                "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
                "    loss, acc = train_one_epoch(model, train_loader, optimizer, scheduler, scaler, DEVICE, GRAD_ACCUM_STEPS)\n",
                "    print(f\"  Loss: {loss:.4f} | Acc: {acc:.4f}\")\n",
                "    \n",
                "    if acc > best_acc:\n",
                "        best_acc = acc\n",
                "        model.save_pretrained('./videomae_exp1_best')\n",
                "        print(f\"  >>> Saved Best (Acc: {best_acc:.4f})\")\n",
                "\n",
                "print(f\"\\nTraining complete. Best acc: {best_acc:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## 8. Test Dataset\n",
                "\n",
                "class TestDataset(Dataset):\n",
                "    def __init__(self, root, transform=None, num_frames=16, frame_stride=2):\n",
                "        self.root = Path(root)\n",
                "        self.transform = transform\n",
                "        self.num_frames = num_frames\n",
                "        self.frame_stride = frame_stride\n",
                "        \n",
                "        self.samples = []\n",
                "        for video_dir in self.root.iterdir():\n",
                "            if video_dir.is_dir():\n",
                "                video_id = int(video_dir.name)\n",
                "                self.samples.append((video_dir, video_id))\n",
                "        self.samples.sort(key=lambda x: x[1])\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.samples)\n",
                "    \n",
                "    def _select_indices(self, total):\n",
                "        if total <= 0:\n",
                "            raise ValueError(\"No frames\")\n",
                "        if total == 1:\n",
                "            return torch.zeros(self.num_frames, dtype=torch.long)\n",
                "        steps = max(self.num_frames * self.frame_stride, self.num_frames)\n",
                "        grid = torch.linspace(0, total - 1, steps=steps)\n",
                "        idxs = grid[::self.frame_stride].long()\n",
                "        if idxs.numel() < self.num_frames:\n",
                "            pad = idxs.new_full((self.num_frames - idxs.numel(),), idxs[-1].item())\n",
                "            idxs = torch.cat([idxs, pad], dim=0)\n",
                "        return idxs[:self.num_frames]\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        video_dir, video_id = self.samples[idx]\n",
                "        frame_files = sorted(video_dir.glob('*.jpg'))\n",
                "        \n",
                "        if len(frame_files) == 0:\n",
                "            raise ValueError(f\"No frames in {video_dir}\")\n",
                "        \n",
                "        indices = self._select_indices(len(frame_files))\n",
                "        frames = []\n",
                "        for i in indices:\n",
                "            img = Image.open(frame_files[i]).convert('RGB')\n",
                "            frames.append(TF.to_tensor(img))\n",
                "        \n",
                "        frames = torch.stack(frames)\n",
                "        \n",
                "        if self.transform:\n",
                "            frames = self.transform(frames)\n",
                "        \n",
                "        return frames, video_id"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## 9. Inference\n",
                "\n",
                "# Load best model\n",
                "model = VideoMAEForVideoClassification.from_pretrained(\n",
                "    './videomae_exp1_best',\n",
                "    num_frames=NUM_FRAMES\n",
                ")\n",
                "model = model.to(DEVICE)\n",
                "model.eval()\n",
                "\n",
                "# Create test loader\n",
                "test_dataset = TestDataset(PATH_DATA_TEST, transform=test_transform,\n",
                "                           num_frames=NUM_FRAMES, frame_stride=FRAME_STRIDE)\n",
                "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
                "\n",
                "print(f\"Test samples: {len(test_dataset)}\")\n",
                "\n",
                "# Run inference\n",
                "predictions = []\n",
                "with torch.no_grad():\n",
                "    for videos, video_ids in tqdm(test_loader, desc=\"Inference\"):\n",
                "        videos = videos.to(DEVICE)\n",
                "        outputs = model(videos)\n",
                "        preds = outputs.logits.argmax(dim=1)\n",
                "        \n",
                "        for vid, pred in zip(video_ids.tolist(), preds.tolist()):\n",
                "            pred_class = id2label[pred]\n",
                "            predictions.append((vid, pred_class))\n",
                "\n",
                "predictions.sort(key=lambda x: x[0])\n",
                "print(f\"\\nPredictions: {len(predictions)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## 10. Evaluate on Test Set\n",
                "\n",
                "# Download test labels from Drive\n",
                "!gdown \"1Xv2CWOqdBj3kt0rkNJKRsodSIEd3-wX_\" -O test_labels.csv -q\n",
                "\n",
                "import pandas as pd\n",
                "from sklearn.metrics import accuracy_score, classification_report\n",
                "\n",
                "# Load ground truth\n",
                "gt_df = pd.read_csv(\"test_labels.csv\")\n",
                "test_labels = dict(zip(gt_df['id'].astype(str), gt_df['class']))\n",
                "\n",
                "# Match predictions with ground truth\n",
                "y_pred = []\n",
                "y_true = []\n",
                "for video_id, pred_class in predictions:\n",
                "    video_id_str = str(video_id)\n",
                "    if video_id_str in test_labels:\n",
                "        y_pred.append(pred_class)\n",
                "        y_true.append(test_labels[video_id_str])\n",
                "\n",
                "# Calculate accuracy\n",
                "accuracy = accuracy_score(y_true, y_pred)\n",
                "\n",
                "print(\"=\" * 50)\n",
                "print(\"EXP 1: VideoMAE Baseline (Paper Settings) - TEST RESULTS\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"Total: {len(y_true)} | Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
                "print()\n",
                "print(classification_report(y_true, y_pred, zero_division=0))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## 11. Save Submission\n",
                "\n",
                "with open('submission_exp1.csv', 'w') as f:\n",
                "    f.write('id,class\\n')\n",
                "    for video_id, pred_class in predictions:\n",
                "        f.write(f'{video_id},{pred_class}\\n')\n",
                "\n",
                "print(\"Submission saved to: submission_exp1.csv\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}