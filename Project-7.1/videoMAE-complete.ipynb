{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# VideoMAE Phase 3 - Complete Training + Evaluation\n",
                "\n",
                "**Features:**\n",
                "1. VideoMAE-Base (Kinetics-400 pretrained)\n",
                "2. 2-Stage Training: Mixup \u2192 Label Smoothing\n",
                "3. Test Set Evaluation with Ground Truth\n",
                "4. Training Curves Plotting\n",
                "5. History CSV Export"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!uv pip install -q transformers accelerate evaluate gdown"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import DataLoader, Dataset\n",
                "import torchvision.transforms as T\n",
                "import torchvision.transforms.functional as TF\n",
                "from transformers import VideoMAEForVideoClassification, VideoMAEImageProcessor\n",
                "from transformers import get_cosine_schedule_with_warmup\n",
                "from pathlib import Path\n",
                "from PIL import Image\n",
                "from tqdm.auto import tqdm\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.metrics import accuracy_score\n",
                "import random\n",
                "import os\n",
                "import gc\n",
                "\n",
                "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Using device: {DEVICE}')\n",
                "\n",
                "# Paths\n",
                "PATH_DATA_TRAIN = '/kaggle/input/action-video/data/data_train'\n",
                "PATH_DATA_TEST = '/kaggle/input/action-video/data/test'"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model Config\n",
                "MODEL_CKPT = 'MCG-NJU/videomae-base-finetuned-kinetics'\n",
                "NUM_FRAMES = 16\n",
                "IMG_SIZE = 224\n",
                "RESIZE_SIZE = 256\n",
                "\n",
                "# Phase 1 Config (Mixup)\n",
                "EPOCHS_P1 = 10\n",
                "LR_P1 = 5e-5\n",
                "\n",
                "# Phase 2 Config (Label Smoothing)\n",
                "EPOCHS_P2 = 10\n",
                "LR_P2 = 1e-6\n",
                "LABEL_SMOOTHING = 0.1\n",
                "\n",
                "# Common Config\n",
                "BATCH_SIZE = 20\n",
                "ACCUM_STEPS = 4\n",
                "WEIGHT_DECAY = 0.05\n",
                "WARMUP_RATIO = 0.1\n",
                "\n",
                "# Augmentation\n",
                "MIXUP_ALPHA = 0.8\n",
                "MIXUP_PROB = 1.0\n",
                "\n",
                "# Normalization\n",
                "MEAN = [0.485, 0.456, 0.406]\n",
                "STD = [0.229, 0.224, 0.225]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.5. Offline Data Augmentation\n",
                "\n",
                "**Strategy**: Balance class distribution by generating 6 variations:\n",
                "1. Original\n",
                "2. Horizontal Flip\n",
                "3. Rotate \u00b110\u00b0\n",
                "4. Crop/Zoom (90%)\n",
                "5. Flip + Brightness\n",
                "6. Rotate + Crop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "import shutil\n",
                "from collections import Counter\n",
                "\n",
                "def get_class_distribution(train_path):\n",
                "    \"\"\"Count samples per class\"\"\"\n",
                "    train_path = Path(train_path)\n",
                "    class_counts = {}\n",
                "    for cls_dir in train_path.iterdir():\n",
                "        if cls_dir.is_dir():\n",
                "            video_count = len([d for d in cls_dir.iterdir() if d.is_dir()])\n",
                "            class_counts[cls_dir.name] = video_count\n",
                "    return class_counts\n",
                "\n",
                "def augment_video_frames(src_dir, dst_dir, transform_type):\n",
                "    \"\"\"Apply augmentation to all frames in a video directory\"\"\"\n",
                "    dst_dir.mkdir(parents=True, exist_ok=True)\n",
                "    \n",
                "    for frame_path in sorted(src_dir.glob('*.jpg')):\n",
                "        img = Image.open(frame_path)\n",
                "        \n",
                "        if transform_type == 'flip':\n",
                "            img = TF.hflip(img)\n",
                "        elif transform_type == 'rotate_neg':\n",
                "            img = TF.rotate(img, -10, fill=0)\n",
                "        elif transform_type == 'rotate_pos':\n",
                "            img = TF.rotate(img, 10, fill=0)\n",
                "        elif transform_type == 'crop':\n",
                "            w, h = img.size\n",
                "            crop_size = int(min(w, h) * 0.9)\n",
                "            left = (w - crop_size) // 2\n",
                "            top = (h - crop_size) // 2\n",
                "            img = TF.crop(img, top, left, crop_size, crop_size)\n",
                "            img = TF.resize(img, (h, w))\n",
                "        elif transform_type == 'flip_bright':\n",
                "            img = TF.hflip(img)\n",
                "            img = TF.adjust_brightness(img, 1.2)\n",
                "        elif transform_type == 'rotate_crop':\n",
                "            img = TF.rotate(img, -8, fill=0)\n",
                "            w, h = img.size\n",
                "            crop_size = int(min(w, h) * 0.92)\n",
                "            left = (w - crop_size) // 2\n",
                "            top = (h - crop_size) // 2\n",
                "            img = TF.crop(img, top, left, crop_size, crop_size)\n",
                "            img = TF.resize(img, (h, w))\n",
                "        \n",
                "        img.save(dst_dir / frame_path.name, quality=95)\n",
                "\n",
                "def balance_dataset(train_path, output_path, target_per_class=None):\n",
                "    \"\"\"Balance dataset by augmenting minority classes\"\"\"\n",
                "    train_path = Path(train_path)\n",
                "    output_path = Path(output_path)\n",
                "    \n",
                "    # Get current distribution\n",
                "    class_counts = get_class_distribution(train_path)\n",
                "    max_count = max(class_counts.values())\n",
                "    target = target_per_class or max_count\n",
                "    \n",
                "    print(f'Target samples per class: {target}')\n",
                "    print(f'Max class: {max_count}, Min class: {min(class_counts.values())}')\n",
                "    \n",
                "    # Augmentation types\n",
                "    aug_types = ['flip', 'rotate_neg', 'rotate_pos', 'crop', 'flip_bright', 'rotate_crop']\n",
                "    \n",
                "    # Copy original + augment\n",
                "    total_created = 0\n",
                "    for cls_name, count in tqdm(class_counts.items(), desc='Balancing classes'):\n",
                "        cls_src = train_path / cls_name\n",
                "        cls_dst = output_path / cls_name\n",
                "        cls_dst.mkdir(parents=True, exist_ok=True)\n",
                "        \n",
                "        videos = sorted([d for d in cls_src.iterdir() if d.is_dir()])\n",
                "        \n",
                "        # Copy originals\n",
                "        for v in videos:\n",
                "            dst = cls_dst / v.name\n",
                "            if not dst.exists():\n",
                "                shutil.copytree(v, dst)\n",
                "        \n",
                "        # Calculate how many augmented samples needed\n",
                "        needed = target - count\n",
                "        if needed <= 0:\n",
                "            continue\n",
                "        \n",
                "        # Generate augmented samples\n",
                "        aug_idx = 0\n",
                "        created = 0\n",
                "        while created < needed:\n",
                "            for v in videos:\n",
                "                if created >= needed:\n",
                "                    break\n",
                "                aug_type = aug_types[aug_idx % len(aug_types)]\n",
                "                aug_name = f'{v.name}_aug_{aug_type}_{aug_idx // len(aug_types)}'\n",
                "                dst = cls_dst / aug_name\n",
                "                if not dst.exists():\n",
                "                    augment_video_frames(v, dst, aug_type)\n",
                "                    created += 1\n",
                "                    total_created += 1\n",
                "            aug_idx += 1\n",
                "    \n",
                "    print(f'\\n\u2713 Created {total_created} augmented videos')\n",
                "    return output_path\n",
                "\n",
                "print('Augmentation functions defined')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Analyze BEFORE augmentation\n",
                "print('=' * 60)\n",
                "print('BEFORE AUGMENTATION')\n",
                "print('=' * 60)\n",
                "\n",
                "before_counts = get_class_distribution(PATH_DATA_TRAIN)\n",
                "before_df = pd.DataFrame([\n",
                "    {'class': k, 'count': v} for k, v in before_counts.items()\n",
                "]).sort_values('count', ascending=False)\n",
                "\n",
                "print(f'Total classes: {len(before_counts)}')\n",
                "print(f'Total samples: {sum(before_counts.values())}')\n",
                "print(f'Max: {max(before_counts.values())} ({max(before_counts, key=before_counts.get)})')\n",
                "print(f'Min: {min(before_counts.values())} ({min(before_counts, key=before_counts.get)})')\n",
                "print(f'Imbalance ratio: {max(before_counts.values()) / min(before_counts.values()):.2f}x')\n",
                "\n",
                "# Plot\n",
                "plt.figure(figsize=(16, 5))\n",
                "plt.bar(range(len(before_df)), before_df['count'].values, color='steelblue', alpha=0.7)\n",
                "plt.xlabel('Class (sorted by count)')\n",
                "plt.ylabel('Number of samples')\n",
                "plt.title('BEFORE Augmentation: Class Distribution')\n",
                "plt.xticks(range(len(before_df)), before_df['class'].values, rotation=90, fontsize=7)\n",
                "plt.tight_layout()\n",
                "plt.savefig('distribution_before.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Run augmentation\n",
                "PATH_DATA_AUGMENTED = '/kaggle/working/data_train_augmented'\n",
                "\n",
                "# Balance to max class count\n",
                "balanced_path = balance_dataset(PATH_DATA_TRAIN, PATH_DATA_AUGMENTED)\n",
                "\n",
                "# Update train path for rest of notebook\n",
                "PATH_DATA_TRAIN = str(balanced_path)\n",
                "print(f'\\n\u2713 Updated PATH_DATA_TRAIN to: {PATH_DATA_TRAIN}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Analyze AFTER augmentation\n",
                "print('=' * 60)\n",
                "print('AFTER AUGMENTATION')\n",
                "print('=' * 60)\n",
                "\n",
                "after_counts = get_class_distribution(PATH_DATA_TRAIN)\n",
                "after_df = pd.DataFrame([\n",
                "    {'class': k, 'count': v} for k, v in after_counts.items()\n",
                "]).sort_values('count', ascending=False)\n",
                "\n",
                "print(f'Total classes: {len(after_counts)}')\n",
                "print(f'Total samples: {sum(after_counts.values())}')\n",
                "print(f'Max: {max(after_counts.values())} ({max(after_counts, key=after_counts.get)})')\n",
                "print(f'Min: {min(after_counts.values())} ({min(after_counts, key=after_counts.get)})')\n",
                "print(f'Imbalance ratio: {max(after_counts.values()) / min(after_counts.values()):.2f}x')\n",
                "\n",
                "# Plot\n",
                "plt.figure(figsize=(16, 5))\n",
                "plt.bar(range(len(after_df)), after_df['count'].values, color='darkgreen', alpha=0.7)\n",
                "plt.xlabel('Class (sorted by count)')\n",
                "plt.ylabel('Number of samples')\n",
                "plt.title('AFTER Augmentation: Class Distribution (Balanced)')\n",
                "plt.xticks(range(len(after_df)), after_df['class'].values, rotation=90, fontsize=7)\n",
                "plt.tight_layout()\n",
                "plt.savefig('distribution_after.png', dpi=150)\n",
                "plt.show()\n",
                "\n",
                "# Summary comparison\n",
                "print('\\n' + '=' * 60)\n",
                "print('SUMMARY COMPARISON')\n",
                "print('=' * 60)\n",
                "print(f'Before: {sum(before_counts.values())} samples, ratio: {max(before_counts.values())/min(before_counts.values()):.2f}x')\n",
                "print(f'After:  {sum(after_counts.values())} samples, ratio: {max(after_counts.values())/min(after_counts.values()):.2f}x')\n",
                "print(f'Added:  {sum(after_counts.values()) - sum(before_counts.values())} augmented samples')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Demo: Show augmented samples\n",
                "def show_augmentation_demo(train_path, class_name='smile', num_samples=3):\n",
                "    \"\"\"Show original vs augmented samples\"\"\"\n",
                "    train_path = Path(train_path)\n",
                "    cls_dir = train_path / class_name\n",
                "    \n",
                "    if not cls_dir.exists():\n",
                "        print(f'Class {class_name} not found')\n",
                "        return\n",
                "    \n",
                "    # Find one original and its augmented versions\n",
                "    videos = sorted([d for d in cls_dir.iterdir() if d.is_dir()])\n",
                "    originals = [v for v in videos if '_aug_' not in v.name][:num_samples]\n",
                "    \n",
                "    fig, axes = plt.subplots(num_samples, 6, figsize=(18, 3*num_samples))\n",
                "    \n",
                "    aug_labels = ['Original', 'Flip', 'Rotate-', 'Crop', 'Flip+Bright', 'Rot+Crop']\n",
                "    \n",
                "    for row, orig in enumerate(originals):\n",
                "        # Get first frame of each version\n",
                "        for col, label in enumerate(aug_labels):\n",
                "            if col == 0:\n",
                "                video_dir = orig\n",
                "            else:\n",
                "                aug_suffix = ['flip', 'rotate_neg', 'crop', 'flip_bright', 'rotate_crop'][col-1]\n",
                "                aug_name = f'{orig.name}_aug_{aug_suffix}_0'\n",
                "                video_dir = cls_dir / aug_name\n",
                "            \n",
                "            if video_dir.exists():\n",
                "                frame = sorted(video_dir.glob('*.jpg'))[0]\n",
                "                img = Image.open(frame)\n",
                "                axes[row, col].imshow(img)\n",
                "            else:\n",
                "                axes[row, col].text(0.5, 0.5, 'N/A', ha='center', va='center')\n",
                "            \n",
                "            axes[row, col].axis('off')\n",
                "            if row == 0:\n",
                "                axes[row, col].set_title(label, fontsize=10)\n",
                "    \n",
                "    plt.suptitle(f'Augmentation Demo: {class_name}', fontsize=14, fontweight='bold')\n",
                "    plt.tight_layout()\n",
                "    plt.savefig('augmentation_demo.png', dpi=150, bbox_inches='tight')\n",
                "    plt.show()\n",
                "\n",
                "# Show demo for a minority class\n",
                "minority_class = min(before_counts, key=before_counts.get)\n",
                "print(f'Showing augmentation demo for minority class: {minority_class}')\n",
                "show_augmentation_demo(PATH_DATA_TRAIN, minority_class, num_samples=2)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Dataset Classes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MixupCollate:\n",
                "    def __init__(self, num_classes, alpha=0.8, prob=1.0):\n",
                "        self.num_classes = num_classes\n",
                "        self.alpha = alpha\n",
                "        self.prob = prob\n",
                "\n",
                "    def __call__(self, batch):\n",
                "        inputs, targets = torch.utils.data.default_collate(batch)\n",
                "        if np.random.rand() > self.prob:\n",
                "            return inputs, F.one_hot(targets, num_classes=self.num_classes).float()\n",
                "        batch_size = inputs.size(0)\n",
                "        index = torch.randperm(batch_size)\n",
                "        lam = np.random.beta(self.alpha, self.alpha)\n",
                "        inputs = lam * inputs + (1 - lam) * inputs[index, :]\n",
                "        targets_one_hot = F.one_hot(targets, num_classes=self.num_classes).float()\n",
                "        targets = lam * targets_one_hot + (1 - lam) * targets_one_hot[index, :]\n",
                "        return inputs, targets\n",
                "\n",
                "class VideoDataset(Dataset):\n",
                "    def __init__(self, root, num_frames=16, is_train=True):\n",
                "        self.root = Path(root)\n",
                "        self.num_frames = num_frames\n",
                "        self.is_train = is_train\n",
                "        self.classes = sorted([d.name for d in self.root.iterdir() if d.is_dir()])\n",
                "        self.class_to_idx = {c: i for i, c in enumerate(self.classes)}\n",
                "        self.samples = []\n",
                "        for cls in self.classes:\n",
                "            cls_dir = self.root / cls\n",
                "            for video_dir in sorted([d for d in cls_dir.iterdir() if d.is_dir()]):\n",
                "                self.samples.append((video_dir, self.class_to_idx[cls]))\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.samples)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        video_dir, label = self.samples[idx]\n",
                "        frame_paths = sorted(video_dir.glob('*.jpg'))\n",
                "        indices = np.linspace(0, len(frame_paths) - 1, self.num_frames, dtype=int)\n",
                "        frames = [TF.resize(Image.open(frame_paths[i]).convert('RGB'), RESIZE_SIZE) for i in indices]\n",
                "        \n",
                "        if self.is_train:\n",
                "            i, j, h, w = T.RandomResizedCrop.get_params(frames[0], (0.8, 1.0), (0.75, 1.33))\n",
                "            do_flip = random.random() > 0.5\n",
                "            processed = []\n",
                "            for img in frames:\n",
                "                img = TF.resized_crop(img, i, j, h, w, (IMG_SIZE, IMG_SIZE))\n",
                "                if do_flip:\n",
                "                    img = TF.hflip(img)\n",
                "                img = TF.normalize(TF.to_tensor(img), MEAN, STD)\n",
                "                processed.append(img)\n",
                "        else:\n",
                "            processed = [TF.normalize(TF.to_tensor(TF.center_crop(img, IMG_SIZE)), MEAN, STD) for img in frames]\n",
                "        \n",
                "        return torch.stack(processed), label\n",
                "\n",
                "class TestDataset(Dataset):\n",
                "    \"\"\"Test dataset - loads videos by ID.\"\"\"\n",
                "    def __init__(self, root, num_frames=16):\n",
                "        self.root = Path(root)\n",
                "        self.num_frames = num_frames\n",
                "        self.samples = sorted([(d, int(d.name)) for d in self.root.iterdir() if d.is_dir()], key=lambda x: x[1])\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.samples)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        video_dir, video_id = self.samples[idx]\n",
                "        frame_paths = sorted(video_dir.glob('*.jpg'))\n",
                "        indices = np.linspace(0, len(frame_paths) - 1, self.num_frames, dtype=int)\n",
                "        frames = [TF.resize(Image.open(frame_paths[i]).convert('RGB'), RESIZE_SIZE) for i in indices]\n",
                "        processed = [TF.normalize(TF.to_tensor(TF.center_crop(img, IMG_SIZE)), MEAN, STD) for img in frames]\n",
                "        return torch.stack(processed), video_id"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.5. Focal Loss for Imbalanced Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def focal_loss(logits, targets, alpha=0.25, gamma=2.0):\n",
                "    \"\"\"\n",
                "    Focal Loss for handling class imbalance.\n",
                "    \n",
                "    Args:\n",
                "        logits: Model predictions (N, C)\n",
                "        targets: Ground truth labels (N,)\n",
                "        alpha: Weight for rare classes [0-1], default=0.25\n",
                "        gamma: Focusing parameter [0-5], default=2.0\n",
                "               Higher gamma = more focus on hard examples\n",
                "    \n",
                "    Reference: https://arxiv.org/abs/1708.02002\n",
                "    \"\"\"\n",
                "    ce_loss = F.cross_entropy(logits, targets, reduction='none')\n",
                "    pt = torch.exp(-ce_loss)  # Probability of correct class\n",
                "    focal = alpha * (1 - pt) ** gamma * ce_loss\n",
                "    return focal.mean()\n",
                "\n",
                "print('Focal Loss function defined.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load Dataset & Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download test labels\n",
                "!gdown \"1Xv2CWOqdBj3kt0rkNJKRsodSIEd3-wX_\" -O test_labels.csv -q\n",
                "\n",
                "# Load datasets\n",
                "train_dataset = VideoDataset(PATH_DATA_TRAIN, NUM_FRAMES, is_train=True)\n",
                "test_dataset = TestDataset(PATH_DATA_TEST, NUM_FRAMES)\n",
                "\n",
                "# Ground truth\n",
                "gt_df = pd.read_csv('test_labels.csv')\n",
                "gt_dict = dict(zip(gt_df['id'].astype(str), gt_df['class']))\n",
                "\n",
                "print(f'Train samples: {len(train_dataset)}')\n",
                "print(f'Test samples: {len(test_dataset)}')\n",
                "print(f'Classes: {len(train_dataset.classes)}')\n",
                "\n",
                "# DataLoaders - NO MixupCollate needed with Focal Loss\n",
                "train_loader_p1 = DataLoader(train_dataset, BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n",
                "train_loader_p2 = DataLoader(train_dataset, BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n",
                "test_loader = DataLoader(test_dataset, BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
                "\n",
                "print('DataLoaders created (using Focal Loss, no Mixup).')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load model\n",
                "model = VideoMAEForVideoClassification.from_pretrained(\n",
                "    MODEL_CKPT,\n",
                "    num_labels=len(train_dataset.classes),\n",
                "    ignore_mismatched_sizes=True,\n",
                "    num_frames=NUM_FRAMES\n",
                ").to(DEVICE)\n",
                "print('Model loaded.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Training & Evaluation Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_epoch(model, loader, optimizer, scheduler, scaler, use_focal=True, label_smoothing=0.0):\n",
                "    model.train()\n",
                "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
                "    pbar = tqdm(loader, desc='Training', leave=False)\n",
                "    optimizer.zero_grad()\n",
                "    \n",
                "    for step, (inputs, targets) in enumerate(pbar):\n",
                "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
                "        \n",
                "        with torch.amp.autocast('cuda'):\n",
                "            logits = model(inputs).logits\n",
                "            if use_focal:\n",
                "                # Phase 1: Focal Loss for imbalance\n",
                "                loss = focal_loss(logits, targets, alpha=0.25, gamma=2.0)\n",
                "            else:\n",
                "                # Phase 2: Label Smoothing for refinement\n",
                "                loss = F.cross_entropy(logits, targets, label_smoothing=label_smoothing)\n",
                "        \n",
                "        total_correct += (logits.argmax(1) == targets).sum().item()\n",
                "        total_samples += inputs.size(0)\n",
                "        \n",
                "        scaler.scale(loss / ACCUM_STEPS).backward()\n",
                "        \n",
                "        if (step + 1) % ACCUM_STEPS == 0:\n",
                "            scaler.unscale_(optimizer)\n",
                "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
                "            scaler.step(optimizer)\n",
                "            scaler.update()\n",
                "            optimizer.zero_grad()\n",
                "            scheduler.step()\n",
                "        \n",
                "        total_loss += loss.item()\n",
                "        pbar.set_postfix({'loss': f'{total_loss/(step+1):.4f}', 'acc': f'{total_correct/total_samples:.4f}'})\n",
                "    \n",
                "    return total_loss / len(loader), total_correct / total_samples\n",
                "\n",
                "@torch.no_grad()\n",
                "def evaluate(model, loader, classes, gt_dict):\n",
                "    model.eval()\n",
                "    predictions = []\n",
                "    for videos, video_ids in tqdm(loader, desc='Evaluating', leave=False):\n",
                "        videos = videos.to(DEVICE)\n",
                "        preds = model(videos).logits.argmax(1).cpu().tolist()\n",
                "        predictions.extend(zip(video_ids.tolist(), preds))\n",
                "    \n",
                "    y_true = [gt_dict[str(vid)] for vid, _ in predictions]\n",
                "    y_pred = [classes[p] for _, p in predictions]\n",
                "    return accuracy_score(y_true, y_pred)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize\n",
                "history = []\n",
                "best_acc = 0.0\n",
                "scaler = torch.amp.GradScaler()\n",
                "\n",
                "# Phase 1: Focal Loss for Imbalance Handling\n",
                "print('=' * 50)\n",
                "print(f'PHASE 1: Focal Loss Training (Epochs: {EPOCHS_P1}, LR: {LR_P1})')\n",
                "print('=' * 50)\n",
                "\n",
                "optimizer = torch.optim.AdamW(model.parameters(), lr=LR_P1, weight_decay=WEIGHT_DECAY)\n",
                "total_steps = len(train_loader_p1) * EPOCHS_P1 // ACCUM_STEPS\n",
                "scheduler = get_cosine_schedule_with_warmup(optimizer, int(total_steps * WARMUP_RATIO), total_steps)\n",
                "\n",
                "for epoch in range(1, EPOCHS_P1 + 1):\n",
                "    loss, train_acc = train_epoch(model, train_loader_p1, optimizer, scheduler, scaler, use_focal=True)\n",
                "    test_acc = evaluate(model, test_loader, train_dataset.classes, gt_dict)\n",
                "    \n",
                "    history.append({'epoch': epoch, 'phase': 1, 'loss': loss, 'train_acc': train_acc, 'test_acc': test_acc})\n",
                "    \n",
                "    status = '>>> BEST' if test_acc > best_acc else ''\n",
                "    if test_acc > best_acc:\n",
                "        best_acc = test_acc\n",
                "        torch.save(model.state_dict(), 'best_p1.pt')\n",
                "    print(f'Ep {epoch}/{EPOCHS_P1}: L={loss:.4f} TrAcc={train_acc:.4f} TeAcc={test_acc:.4f} {status}')\n",
                "    \n",
                "    gc.collect()\n",
                "    torch.cuda.empty_cache()\n",
                "\n",
                "# Phase 2: Label Smoothing\n",
                "print('\\n' + '=' * 50)\n",
                "print(f'PHASE 2: Label Smoothing (Epochs: {EPOCHS_P2}, LR: {LR_P2})')\n",
                "print('=' * 50)\n",
                "\n",
                "model.load_state_dict(torch.load('best_p1.pt'))\n",
                "scaler = torch.amp.GradScaler()  # Reset scaler for Phase 2\n",
                "optimizer = torch.optim.AdamW(model.parameters(), lr=LR_P2, weight_decay=WEIGHT_DECAY)\n",
                "total_steps = len(train_loader_p2) * EPOCHS_P2 // ACCUM_STEPS\n",
                "scheduler = get_cosine_schedule_with_warmup(optimizer, int(total_steps * WARMUP_RATIO), total_steps)\n",
                "\n",
                "for epoch in range(1, EPOCHS_P2 + 1):\n",
                "    loss, train_acc = train_epoch(model, train_loader_p2, optimizer, scheduler, scaler, use_focal=False, label_smoothing=LABEL_SMOOTHING)\n",
                "    test_acc = evaluate(model, test_loader, train_dataset.classes, gt_dict)\n",
                "    \n",
                "    history.append({'epoch': EPOCHS_P1 + epoch, 'phase': 2, 'loss': loss, 'train_acc': train_acc, 'test_acc': test_acc})\n",
                "    \n",
                "    status = '>>> BEST' if test_acc > best_acc else ''\n",
                "    if test_acc > best_acc:\n",
                "        best_acc = test_acc\n",
                "        torch.save(model.state_dict(), 'best_final.pt')\n",
                "    print(f'P2 Ep {epoch}/{EPOCHS_P2}: L={loss:.4f} TrAcc={train_acc:.4f} TeAcc={test_acc:.4f} {status}')\n",
                "    \n",
                "    gc.collect()\n",
                "    torch.cuda.empty_cache()\n",
                "\n",
                "# Save history\n",
                "df_history = pd.DataFrame(history)\n",
                "df_history.to_csv('training_history.csv', index=False)\n",
                "print(f'\\nTraining Complete! Best Test Acc: {best_acc:.4f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Plot Training Curves"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_csv('training_history.csv')\n",
                "\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "\n",
                "# Test Accuracy\n",
                "axes[0].plot(df['epoch'], df['test_acc'], 'b-o', markersize=4, label='Test Acc')\n",
                "axes[0].axvline(x=EPOCHS_P1, color='gray', linestyle='--', alpha=0.5, label='P1\u2192P2')\n",
                "axes[0].set_title('Test Accuracy')\n",
                "axes[0].set_xlabel('Epoch')\n",
                "axes[0].set_ylabel('Accuracy')\n",
                "axes[0].legend()\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# Train Accuracy\n",
                "axes[1].plot(df['epoch'], df['train_acc'], 'g-s', markersize=4, label='Train Acc')\n",
                "axes[1].axvline(x=EPOCHS_P1, color='gray', linestyle='--', alpha=0.5, label='P1\u2192P2')\n",
                "axes[1].set_title('Train Accuracy')\n",
                "axes[1].set_xlabel('Epoch')\n",
                "axes[1].set_ylabel('Accuracy')\n",
                "axes[1].legend()\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "# Loss\n",
                "axes[2].plot(df['epoch'], df['loss'], 'r-^', markersize=4, label='Loss')\n",
                "axes[2].axvline(x=EPOCHS_P1, color='gray', linestyle='--', alpha=0.5, label='P1\u2192P2')\n",
                "axes[2].set_title('Loss')\n",
                "axes[2].set_xlabel('Epoch')\n",
                "axes[2].set_ylabel('Loss')\n",
                "axes[2].legend()\n",
                "axes[2].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('training_curves.png', dpi=150)\n",
                "plt.show()\n",
                "\n",
                "# Print summary\n",
                "print('\\n' + '=' * 50)\n",
                "print('TRAINING SUMMARY')\n",
                "print('=' * 50)\n",
                "print(df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Per-Class Accuracy Analysis\n",
                "\n",
                "Analyze model performance on each class after training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Load best model and evaluate per-class\n",
                "from sklearn.metrics import classification_report, confusion_matrix\n",
                "import seaborn as sns\n",
                "\n",
                "# Load best checkpoint\n",
                "if os.path.exists('best_final.pt'):\n",
                "    model.load_state_dict(torch.load('best_final.pt'))\n",
                "    print('Loaded best_final.pt')\n",
                "elif os.path.exists('best_p1.pt'):\n",
                "    model.load_state_dict(torch.load('best_p1.pt'))\n",
                "    print('Loaded best_p1.pt')\n",
                "model.eval()\n",
                "\n",
                "# Get predictions\n",
                "all_preds = []\n",
                "all_true = []\n",
                "\n",
                "with torch.no_grad():\n",
                "    for videos, video_ids in tqdm(test_loader, desc='Evaluating per-class'):\n",
                "        videos = videos.to(DEVICE)\n",
                "        preds = model(videos).logits.argmax(1).cpu().tolist()\n",
                "        \n",
                "        for vid, pred in zip(video_ids.tolist(), preds):\n",
                "            true_label = gt_dict[str(vid)]\n",
                "            all_true.append(true_label)\n",
                "            all_preds.append(train_dataset.classes[pred])\n",
                "\n",
                "# Overall accuracy\n",
                "overall_acc = accuracy_score(all_true, all_preds)\n",
                "print(f'\\n{\"=\"*60}')\n",
                "print(f'OVERALL TEST ACCURACY: {overall_acc:.4f} ({overall_acc*100:.2f}%)')\n",
                "print(f'{\"=\"*60}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Full classification report\n",
                "print('\\nPER-CLASS CLASSIFICATION REPORT:')\n",
                "print('=' * 80)\n",
                "report = classification_report(all_true, all_preds, target_names=train_dataset.classes, \n",
                "                                digits=4, zero_division=0)\n",
                "print(report)\n",
                "\n",
                "# Save report\n",
                "with open('per_class_report.txt', 'w') as f:\n",
                "    f.write(f'Overall Accuracy: {overall_acc:.4f}\\n\\n')\n",
                "    f.write(report)\n",
                "print('\\n\u2713 Saved to per_class_report.txt')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Extract per-class accuracy and visualize\n",
                "report_dict = classification_report(all_true, all_preds, target_names=train_dataset.classes, \n",
                "                                     output_dict=True, zero_division=0)\n",
                "\n",
                "# Extract recall (accuracy) per class\n",
                "class_accs = [(cls, report_dict[cls]['recall']) for cls in train_dataset.classes]\n",
                "class_accs_sorted = sorted(class_accs, key=lambda x: x[1], reverse=True)\n",
                "\n",
                "cls_names = [c[0] for c in class_accs_sorted]\n",
                "cls_accs_vals = [c[1] * 100 for c in class_accs_sorted]\n",
                "\n",
                "# Plot\n",
                "plt.figure(figsize=(16, 6))\n",
                "colors = ['darkgreen' if acc >= 90 else 'orange' if acc >= 70 else 'darkred' for acc in cls_accs_vals]\n",
                "bars = plt.bar(range(len(cls_names)), cls_accs_vals, color=colors, alpha=0.7)\n",
                "plt.axhline(y=overall_acc*100, color='blue', linestyle='--', linewidth=2, label=f'Overall ({overall_acc*100:.2f}%)')\n",
                "plt.xlabel('Action Category', fontsize=12)\n",
                "plt.ylabel('Accuracy (%)', fontsize=12)\n",
                "plt.title('Per-Class Test Accuracy (Sorted by Performance)', fontsize=14, fontweight='bold')\n",
                "plt.xticks(range(len(cls_names)), cls_names, rotation=90, ha='right', fontsize=8)\n",
                "plt.ylim(0, 105)\n",
                "plt.legend()\n",
                "plt.grid(axis='y', alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.savefig('per_class_accuracy.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Top and Bottom performing classes\n",
                "print('\\n' + '=' * 60)\n",
                "print('\ud83c\udfc6 TOP-5 BEST PERFORMING CLASSES:')\n",
                "print('=' * 60)\n",
                "for i in range(min(5, len(cls_names))):\n",
                "    print(f'{i+1:2d}. {cls_names[i]:20s} {cls_accs_vals[i]:6.2f}%')\n",
                "\n",
                "print('\\n' + '=' * 60)\n",
                "print('\u26a0\ufe0f  TOP-5 WORST PERFORMING CLASSES:')\n",
                "print('=' * 60)\n",
                "for i in range(max(0, len(cls_names)-5), len(cls_names)):\n",
                "    print(f'{len(cls_names)-i:2d}. {cls_names[i]:20s} {cls_accs_vals[i]:6.2f}%')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Top confusion pairs\n",
                "cm = confusion_matrix(all_true, all_preds, labels=train_dataset.classes)\n",
                "\n",
                "# Find top confusions (off-diagonal)\n",
                "confusions = []\n",
                "for i in range(len(train_dataset.classes)):\n",
                "    for j in range(len(train_dataset.classes)):\n",
                "        if i != j and cm[i, j] > 0:\n",
                "            confusions.append((train_dataset.classes[i], train_dataset.classes[j], cm[i, j]))\n",
                "\n",
                "confusions.sort(key=lambda x: x[2], reverse=True)\n",
                "\n",
                "print('\\n' + '=' * 70)\n",
                "print('TOP-10 CONFUSION PAIRS (True \u2192 Predicted)')\n",
                "print('=' * 70)\n",
                "for i, (true_cls, pred_cls, count) in enumerate(confusions[:10]):\n",
                "    print(f'{i+1:2d}. {true_cls:20s} \u2192 {pred_cls:20s} ({int(count):2d} errors)')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Final summary\n",
                "print('\\n' + '=' * 70)\n",
                "print('FINAL TRAINING SUMMARY')\n",
                "print('=' * 70)\n",
                "print(f'Overall Test Accuracy: {overall_acc:.4f} ({overall_acc*100:.2f}%)')\n",
                "print(f'Best performing class: {cls_names[0]} ({cls_accs_vals[0]:.2f}%)')\n",
                "print(f'Worst performing class: {cls_names[-1]} ({cls_accs_vals[-1]:.2f}%)')\n",
                "print(f'Classes with 100% accuracy: {sum(1 for acc in cls_accs_vals if acc >= 99.9)}')\n",
                "print(f'Classes with <70% accuracy: {sum(1 for acc in cls_accs_vals if acc < 70)}')\n",
                "\n",
                "# Save summary\n",
                "summary = {\n",
                "    'overall_accuracy': overall_acc,\n",
                "    'best_class': cls_names[0],\n",
                "    'best_acc': cls_accs_vals[0] / 100,\n",
                "    'worst_class': cls_names[-1],\n",
                "    'worst_acc': cls_accs_vals[-1] / 100,\n",
                "}\n",
                "pd.DataFrame([summary]).to_csv('analysis_summary.csv', index=False)\n",
                "print('\\n\u2713 Saved analysis_summary.csv')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}