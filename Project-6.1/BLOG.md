# D·ª± B√°o Gi√° C·ªï Phi·∫øu FPT: LTSF-Linear + HMM Regime-Switching

# I. Gi·ªõi thi·ªáu

ƒê√¢y l√† b√†i vi·∫øt chia s·∫ª gi·∫£i ph√°p c·ªßa nh√≥m trong cu·ªôc thi **[AIO-2025: LTSF-Linear Forecasting Challenge](https://www.kaggle.com/competitions/aio-2025-linear-forecasting-challenge)** tr√™n Kaggle.

**M·ª•c ti√™u cu·ªôc thi:** D·ª± b√°o gi√° ƒë√≥ng c·ª≠a c·ªï phi·∫øu FPT trong **100 ng√†y ti·∫øp theo**. Thay v√¨ d·ª±a v√†o c√°c m√¥ h√¨nh deep learning ph·ª©c t·∫°p, th·ª≠ th√°ch khuy·∫øn kh√≠ch ng∆∞·ªùi tham gia kh√°m ph√° s·ª©c m·∫°nh c·ªßa c√°c **m√¥ h√¨nh tuy·∫øn t√≠nh** nh∆∞ Linear, NLinear v√† DLinear khi √°p d·ª•ng v√†o d·ªØ li·ªáu t√†i ch√≠nh th·ª±c t·∫ø.

## Th√°ch th·ª©c ch√≠nh

| Th√°ch th·ª©c | M√¥ t·∫£ |
|------------|-------|
| **Long-term Forecasting** | D·ª± b√°o 100 ng√†y, kh√¥ng ph·∫£i 1-7 ng√†y nh∆∞ th√¥ng th∆∞·ªùng |
| **Distribution Shift** | Gi√° c·ªï phi·∫øu thay ƒë·ªïi range theo th·ªùi gian |
| **Market Regimes** | Th·ªã tr∆∞·ªùng c√≥ c√°c tr·∫°ng th√°i ·∫©n: ·ªïn ƒë·ªãnh, bi·∫øn ƒë·ªông, chuy·ªÉn ƒë·ªïi |
| **Data Constraint** | Ch·ªâ ƒë∆∞·ª£c d√πng m·ªói Data train, kh√¥ng c√≥ x√†i data ngo√†i |

## Gi·∫£i ph√°p c·ªßa nh√≥m

Nh√≥m k·∫øt h·ª£p 3 k·ªπ thu·∫≠t ch√≠nh:

1. **RevIN (Reversible Instance Normalization)**: X·ª≠ l√Ω distribution shift b·∫±ng c√°ch normalize ƒë·∫ßu v√†o v√† denormalize ƒë·∫ßu ra

2. **HMM Regime Detection**: S·ª≠ d·ª•ng Hidden Markov Model ƒë·ªÉ ph√°t hi·ªán tr·∫°ng th√°i th·ªã tr∆∞·ªùng (Stable / Transition / Volatile)

3. **Regime-Specific Models**: Train model ri√™ng cho m·ªói regime, d·ª± b√°o d·ª±a tr√™n ƒëi·ªÅu ki·ªán th·ªã tr∆∞·ªùng hi·ªán t·∫°i

## K·∫øt qu·∫£ ƒë·∫°t ƒë∆∞·ª£c hi·ªán t·∫°i

| Rank | Method | Hidden MSE | Ghi ch√∫ |
|------|--------|-----------|---------|
| 1 | Univariate DLinear | **34** | Best overall |
| 2 | Univariate Linear | 36 | |
| 3 | Multivariate DLinear | 51 | Best v·ªõi HMM |


---

# II. C√°c th√°ch th·ª©c

## 1. Long-term Forecasting

D·ª± b√°o 100 ng√†y l√† m·ªôt th√°ch th·ª©c l·ªõn so v·ªõi c√°c b√†i to√°n d·ª± b√°o ng·∫Øn h·∫°n (1-7 ng√†y). C√≥ hai ph∆∞∆°ng ph√°p ch√≠nh:

### Direct Forecasting (Multi-output)
- Train **nhi·ªÅu model ri√™ng bi·ªát**, m·ªói model d·ª± b√°o m·ªôt b∆∞·ªõc th·ªùi gian c·ª• th·ªÉ
- Model 1 d·ª± b√°o T+1, Model 2 d·ª± b√°o T+2, ..., Model 100 d·ª± b√°o T+100
- **∆Øu ƒëi·ªÉm:** Kh√¥ng t√≠ch l≈©y error
- **Nh∆∞·ª£c ƒëi·ªÉm:** C·∫ßn train nhi·ªÅu model, kh√¥ng capture ƒë∆∞·ª£c dependency gi·ªØa c√°c b∆∞·ªõc

### Recursive Forecasting (Autoregressive)
- Train **m·ªôt model duy nh·∫•t** d·ª± b√°o b∆∞·ªõc ti·∫øp theo
- D√πng prediction l√†m input ƒë·ªÉ d·ª± b√°o ti·∫øp theo
- **∆Øu ƒëi·ªÉm:** M·ªôt model, capture ƒë∆∞·ª£c dependency
- **Nh∆∞·ª£c ƒëi·ªÉm:** Error t√≠ch l≈©y theo th·ªùi gian

<p align="center">
  <img src="images/direct_vs_recursive.png" alt="Direct vs Recursive Forecasting" width="680">
  <br><em>H√¨nh 1. So s√°nh Direct (tr√°i) v√† Recursive (ph·∫£i) Forecasting. (Ngu·ªìn: AI Viet Nam)</em>
</p>

**Trong project n√†y:** Nh√≥m m√¨nh s·ª≠ d·ª•ng **Direct Forecasting** - model d·ª± b√°o tr·ª±c ti·∫øp 100 ng√†y m·ªôt l·∫ßn m√† kh√¥ng c·∫ßn recursive.

## 2. Distribution Shift

**Distribution shift** l√† hi·ªán t∆∞·ª£ng ph√¢n ph·ªëi d·ªØ li·ªáu thay ƒë·ªïi theo th·ªùi gian. Trong d·ªØ li·ªáu FPT, ƒëi·ªÅu n√†y th·ªÉ hi·ªán r√µ r√†ng:

<p align="center">
  <img src="images/distribution_shift_fpt.png" alt="Distribution Shift FPT" width="680">
  <br><em>H√¨nh 2. Distribution Shift trong d·ªØ li·ªáu FPT: Ph√¢n ph·ªëi gi√° 2020-2021 ho√†n to√†n kh√°c v·ªõi 2023-2024.</em>
</p>

Nh∆∞ h√¨nh tr√™n cho th·∫•y:
- **Period 1 (2020-2021):** Gi√° dao ƒë·ªông trong kho·∫£ng th·∫•p
- **Period 2 (2023-2024):** Gi√° ƒë√£ tƒÉng l√™n m·ª©c cao h∆°n nhi·ªÅu

M·∫∑c d√π **pattern bi·∫øn ƒë·ªông v·∫´n t∆∞∆°ng t·ª±**, nh∆∞ng **scale ƒë√£ thay ƒë·ªïi ho√†n to√†n**. Model h·ªçc tr√™n d·ªØ li·ªáu c≈© s·∫Ω d·ª± b√°o sai scale n·∫øu kh√¥ng x·ª≠ l√Ω.

**Gi·∫£i ph√°p:** RevIN (Reversible Instance Normalization).

## 3. Market Regimes

Th·ªã tr∆∞·ªùng t√†i ch√≠nh kh√¥ng ho·∫°t ƒë·ªông theo m·ªôt quy lu·∫≠t duy nh·∫•t. Thay v√†o ƒë√≥, n√≥ chuy·ªÉn ƒë·ªïi gi·ªØa c√°c **tr·∫°ng th√°i (regimes)** kh√°c nhau:

- **Bull Market**: Xu h∆∞·ªõng tƒÉng m·∫°nh, volatility th·∫•p
- **Bear Market**: Xu h∆∞·ªõng gi·∫£m, volatility cao
- **Sideways/Consolidation**: ƒêi ngang, kh√¥ng c√≥ xu h∆∞·ªõng r√µ r√†ng
- **Transition**: Giai ƒëo·∫°n chuy·ªÉn ƒë·ªïi gi·ªØa c√°c regime

<p align="center">
  <img src="images/market_regime_spx.png" alt="Market Regime Analysis" width="680">
  <br><em>H√¨nh 3. Ph√¢n t√≠ch Market Regime tr√™n S&P 500 (Ngu·ªìn: <a href="https://www.wallstreetcourier.com/spotlights/mrnl_sp-500-outlook-analyzing-the-current-market-regime-of-sp-500-spx/">Wall Street Courier</a>)</em>
</p>

**V·∫•n ƒë·ªÅ:** M·ªôt model duy nh·∫•t kh√≥ c√≥ th·ªÉ h·ªçc ƒë∆∞·ª£c t·∫•t c·∫£ patterns t·ª´ c√°c regime kh√°c nhau. D·ªØ li·ªáu t·ª´ Bull Market c√≥ th·ªÉ "nhi·ªÖu" vi·ªác h·ªçc pattern c·ªßa Bear Market v√† ng∆∞·ª£c l·∫°i.

**Gi·∫£i ph√°p:** HMM Regime-Switching - ph√°t hi·ªán regime v√† train model chuy√™n bi·ªát cho t·ª´ng regime.

## 4. Data Constraint - N√†y th√¨ kh√¥ng ch·ªãu c≈©ng ph·∫£i ch·ªãu ü§£
---

# III. Gi·∫£i ph√°p k·ªπ thu·∫≠t

## 1. RevIN - Reversible Instance Normalization

### 1.1 √ù t∆∞·ªüng

RevIN l√† k·ªπ thu·∫≠t normalize d·ªØ li·ªáu **c√≥ th·ªÉ ƒë·∫£o ng∆∞·ª£c**, ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·∫∑c bi·ªát cho time series v·ªõi distribution shift. √ù t∆∞·ªüng ch√≠nh:

1. **Normalize input**: Chu·∫©n h√≥a chu·ªói ƒë·∫ßu v√†o v·ªÅ mean=0, std=1
2. **Model h·ªçc**: Model h·ªçc patterns tr√™n d·ªØ li·ªáu ƒë√£ chu·∫©n h√≥a
3. **Denormalize output**: Kh√¥i ph·ª•c l·∫°i scale g·ªëc cho d·ª± b√°o

<p align="center">
  <img src="images/fig1.gif" alt="RevIN Animation" width="500">
  <br><em>H√¨nh 4. T√°c d·ª•ng c·ªßa RevIN. (Ngu·ªìn: <a href="https://github.com/ts-kim/RevIN/">RevIN GitHub</a>)</em>
</p>

### 1.2 Thu·∫≠t to√°n

<p align="center">
  <img src="images/revin_algorithm.png" alt="RevIN Algorithm" width="600">
  <br><em>H√¨nh 5. Thu·∫≠t to√°n RevIN chi ti·∫øt. (Ngu·ªìn: <a href="https://github.com/ts-kim/RevIN/">RevIN GitHub</a>)</em>
</p>

### 1.3 Code Implementation

```python
class RevIN(nn.Module):
    def __init__(self, num_features, eps=1e-5, affine=True):
        super().__init__()
        self.eps = eps
        self.affine = affine
        if affine:
            # B∆∞·ªõc 4: Learnable parameters Œ≥ v√† Œ≤
            self.gamma = nn.Parameter(torch.ones(num_features))
            self.beta = nn.Parameter(torch.zeros(num_features))

    def forward(self, x, mode):
        if mode == 'norm':
            # B∆∞·ªõc 1: Compute instance mean
            self.mean = x.mean(dim=1, keepdim=True).detach()
            # B∆∞·ªõc 2: Compute instance variance  
            self.std = torch.sqrt(x.var(dim=1, keepdim=True) + self.eps).detach()
            # B∆∞·ªõc 3: Normalize
            x = (x - self.mean) / self.std
            # B∆∞·ªõc 4: Scale and shift v·ªõi Œ≥, Œ≤
            if self.affine:
                x = x * self.gamma + self.beta
            return x
            
        elif mode == 'denorm':
            # B∆∞·ªõc 6: Reverse scale and shift
            if self.affine:
                x = (x - self.beta) / (self.gamma + self.eps)
            # B∆∞·ªõc 7: Denormalize v·ªÅ scale g·ªëc
            x = x * self.std + self.mean
            return x
```


### 1.4 Apply v√†o d·ªØ li·ªáu FPT

<p align="center">
  <img src="images/revin_fpt.png" alt="RevIN on FPT" width="680">
  <br><em>H√¨nh 6. √Åp d·ª•ng RevIN v√†o d·ªØ li·ªáu FPT.</em>
</p>

**Ph√¢n t√≠ch:**
- **G√≥c tr√™n tr√°i (Original):** Gi√° FPT c√≥ range thay ƒë·ªïi t·ª´ ~50 (2020) l√™n ~130 (2024)
- **G√≥c tr√™n ph·∫£i (After RevIN):** Sau normalize, gi√° dao ƒë·ªông quanh 0 v·ªõi std ‚âà 1
- **G√≥c d∆∞·ªõi tr√°i (Distribution Original):** Ph√¢n ph·ªëi l·ªách ph·∫£i, nhi·ªÅu peaks kh√°c nhau
- **G√≥c d∆∞·ªõi ph·∫£i (Distribution After):** Ph√¢n ph·ªëi g·∫ßn chu·∫©n h∆°n, t·∫≠p trung quanh 0

**L·ª£i √≠ch:** Model kh√¥ng c√≤n b·ªã ·∫£nh h∆∞·ªüng b·ªüi s·ª± thay ƒë·ªïi scale theo th·ªùi gian.

---

## 2. HMM Regime Detection

### 2.1 Hidden Markov Model

<p align="center">
  <img src="images/hmm_diagram.png" alt="HMM Diagram" width="500">
  <br><em>H√¨nh 7. Minh h·ªça Hidden Markov Model v·ªõi 3 hidden states v√† 2 observations. (Ngu·ªìn: <a href="https://www.youtube.com/watch?v=RWkHJnFj5rY">YouTube</a>)</em>
</p>

HMM l√† m√¥ h√¨nh x√°c su·∫•t trong ƒë√≥:
- **Hidden states (Regimes):** Tr·∫°ng th√°i ·∫©n m√† ta kh√¥ng quan s√°t tr·ª±c ti·∫øp (v√≠ d·ª•: m∆∞a, m√¢y, n·∫Øng)
- **Observations:** C√°c features ta ƒëo ƒë∆∞·ª£c (v√≠ d·ª•: bu·ªìn, vui)
- **Transition Matrix:** X√°c su·∫•t chuy·ªÉn gi·ªØa c√°c tr·∫°ng th√°i (c√°c s·ªë 0.2, 0.3, 0.4, ...)

Trong b·ªëi c·∫£nh th·ªã tr∆∞·ªùng ch·ª©ng kho√°n:
- **Hidden states:** Bull Market, Bear Market, Sideways
- **Observations:** Returns, Volatility, Trend

### 2.2 Features cho HMM

ƒê·ªÉ HMM c√≥ th·ªÉ ph√°t hi·ªán regime, ta c·∫ßn cung c·∫•p c√°c features ph·∫£n √°nh "h√†nh vi" th·ªã tr∆∞·ªùng:

| Feature | C√¥ng th·ª©c | √ù nghƒ©a |
|---------|-----------|---------|
| **Returns** | $R_t = \frac{Close_t - Close_{t-1}}{Close_{t-1}}$ | T·ª∑ su·∫•t sinh l·ªùi ng√†y, cho bi·∫øt th·ªã tr∆∞·ªùng tƒÉng hay gi·∫£m |
| **Volatility** | $Vol_t = std(R_{t-9}, ..., R_t)$ | ƒê·ªô bi·∫øn ƒë·ªông 10 ng√†y, cao = th·ªã tr∆∞·ªùng b·∫•t ·ªïn |
| **Trend** | $Trend_t = \frac{MA_{10}(t) - MA_{10}(t-1)}{MA_{10}(t-1)}$ | Xu h∆∞·ªõng trung b√¨nh ƒë·ªông, cho bi·∫øt trend tƒÉng/gi·∫£m |

```python
# T√≠nh to√°n features
df['returns'] = df['close'].pct_change().fillna(0)
df['volatility'] = df['returns'].rolling(window=10).std().fillna(0)
df['trend'] = df['close'].rolling(window=10).mean().pct_change().fillna(0)
```

<p align="center">
  <img src="images/hmm_features.png" alt="HMM Features" width="680">
  <br><em>H√¨nh 8. Visualization c√°c features cho HMM tr√™n d·ªØ li·ªáu FPT.</em>
</p>

**Nh·∫≠n x√©t t·ª´ h√¨nh:**
- **Returns:** Dao ƒë·ªông quanh 0, c√≥ c√°c spike l·ªõn v√†o th·ªùi ƒëi·ªÉm bi·∫øn ƒë·ªông m·∫°nh
- **Volatility:** TƒÉng cao v√†o c√°c giai ƒëo·∫°n b·∫•t ·ªïn (2020, 2022), th·∫•p khi th·ªã tr∆∞·ªùng ·ªïn ƒë·ªãnh
- **Trend:** Cho th·∫•y xu h∆∞·ªõng tƒÉng/gi·∫£m r√µ r√†ng h∆°n so v·ªõi returns

### 2.3 Regime Window

**Regime Window** l√† s·ªë ng√†y ƒë·∫ßu ti√™n b·ªã b·ªè qua khi detect regimes:

```python
class RegimeDetector:
    def __init__(self, n_components=3, window=30):
        self.window = window
        
    def fit(self, df):
        # B·ªè qua `window` ng√†y ƒë·∫ßu
        features = df[['returns', 'volatility', 'trend']].iloc[self.window:].values
        self.model.fit(features)
```

**T·∫°i sao c·∫ßn Regime Window?**
- C√°c features nh∆∞ `volatility` v√† `trend` c·∫ßn rolling window ƒë·ªÉ t√≠nh to√°n
- Nh·ªØng ng√†y ƒë·∫ßu ti√™n c√≥ gi√° tr·ªã NaN ho·∫∑c kh√¥ng ·ªïn ƒë·ªãnh
- `window=30` ‚Üí b·ªè 30 ng√†y ƒë·∫ßu, ƒë·∫£m b·∫£o features ƒë√£ ·ªïn ƒë·ªãnh

**Gi√° tr·ªã th∆∞·ªùng d√πng:** 30, 60

<p align="center">
  <img src="images/regime_window.png" alt="Regime Window" width="680">
  <br><em>H√¨nh 9. Regime Window: B·ªè qua 30 ng√†y ƒë·∫ßu khi features ch∆∞a ·ªïn ƒë·ªãnh.</em>
</p>

### 2.4 Ch·ªçn s·ªë l∆∞·ª£ng Regimes

C√¢u h·ªèi: N√™n d√πng bao nhi√™u regimes? 3? 4? 5?

#### N = 3 Regimes

<p align="center">
  <img src="images/hmm_3_regimes.png" alt="HMM 3 Regimes" width="680">
  <br><em>H√¨nh 10. HMM v·ªõi 3 Regimes tr√™n d·ªØ li·ªáu FPT.</em>
</p>

**Ph√¢n t√≠ch:**
- **Regime 0 (Xanh l√°):** Th∆∞·ªùng xu·∫•t hi·ªán khi th·ªã tr∆∞·ªùng ·ªïn ƒë·ªãnh, volatility th·∫•p
- **Regime 1 (V√†ng):** Giai ƒëo·∫°n chuy·ªÉn ƒë·ªïi, th∆∞·ªùng th·∫•y tr∆∞·ªõc khi th·ªã tr∆∞·ªùng ƒë·ªïi h∆∞·ªõng
- **Regime 2 (ƒê·ªè):** Th·ªã tr∆∞·ªùng bi·∫øn ƒë·ªông m·∫°nh, c√≥ th·ªÉ l√† crash ho·∫∑c rally m·∫°nh

**Nh·∫≠n x√©t:** Ph√¢n chia 3 regime kh√° r√µ r√†ng, m·ªói regime c√≥ ƒë·ªß samples ƒë·ªÉ train model.

#### N = 4 Regimes

<p align="center">
  <img src="images/hmm_4_regimes.png" alt="HMM 4 Regimes" width="680">
  <br><em>H√¨nh 11. HMM v·ªõi 4 Regimes tr√™n d·ªØ li·ªáu FPT.</em>
</p>

**Ph√¢n t√≠ch:**
- Ph√¢n chia chi ti·∫øt h∆°n v·ªõi 4 tr·∫°ng th√°i
- M·ªôt s·ªë regime c√≥ th·ªÉ c√≥ √≠t samples, g√¢y kh√≥ khƒÉn cho training
- C√≥ th·ªÉ b·∫Øt ƒë∆∞·ª£c nhi·ªÅu chi ti·∫øt h∆°n, nh∆∞ng c≈©ng d·ªÖ overfit

**Trade-off:**

| N Regimes | ∆Øu ƒëi·ªÉm | Nh∆∞·ª£c ƒëi·ªÉm |
|-----------|---------|------------|
| **N = 2** | ƒê∆°n gi·∫£n, nhi·ªÅu samples/regime | Qu√° th√¥, b·ªè s√≥t chi ti·∫øt |
| **N = 3** | C√¢n b·∫±ng, ph·ªï bi·∫øn | C√≥ th·ªÉ kh√¥ng ƒë·ªß chi ti·∫øt |
| **N = 4+** | Chi ti·∫øt h∆°n | √çt samples/regime, d·ªÖ overfit |

**Trong project n√†y:** Nh√≥m ch·ªçn **N = 3** v√¨:
1. ƒê·ªß chi ti·∫øt ƒë·ªÉ ph√¢n bi·ªát bull/bear/transition
2. M·ªói regime c√≥ ƒë·ªß samples ƒë·ªÉ train

### 2.5 L∆∞u √Ω quan tr·ªçng

> **‚ö†Ô∏è HMM kh√¥ng th·ªÉ predict regime cho future!**
> 
> HMM ch·ªâ c√≥ th·ªÉ predict regime d·ª±a tr√™n observations (returns, volatility, trend). V·ªõi 100 ng√†y t∆∞∆°ng lai, ta ch∆∞a c√≥ observations ‚Üí kh√¥ng th·ªÉ predict regime.
>
> **Gi·∫£i ph√°p:** Gi·∫£ ƒë·ªãnh regime hi·ªán t·∫°i (`regimes[-1]`) ti·∫øp t·ª•c trong 100 ng√†y forecast.

---

## 3. LTSF-Linear Models

### 3.1 RLinear (Linear + RevIN)

<p align="center">
  <img src="images/RLinear.png" alt="RLinear Architecture" width="500">
  <br><em>H√¨nh 12. Ki·∫øn tr√∫c Linear + RevIN: RevIN ‚Üí Linear ‚Üí Denormalize.</em>
</p>

**Ki·∫øn tr√∫c:**
1. **RevIN Normalize**: Chu·∫©n h√≥a input v·ªÅ mean=0, std=1
2. **Linear Layer**: M·ªôt l·ªõp fully-connected √°nh x·∫° t·ª´ `seq_len` ‚Üí `pred_len`
3. **Denormalize**: Kh√¥i ph·ª•c scale g·ªëc cho output

**C√¥ng th·ª©c:**
$$\hat{y} = W \cdot x_{norm} + b$$
$$y = \hat{y} \cdot \sigma + \mu$$

Trong ƒë√≥ $W \in \mathbb{R}^{pred\_len \times seq\_len}$.

```python
class Linear(nn.Module):
    def __init__(self, seq_len, pred_len, num_features):
        super().__init__()
        self.revin = RevIN(num_features)
        self.linear = nn.Linear(seq_len, pred_len)
        
    def forward(self, x):
        # RevIN normalize
        x = self.revin(x, 'norm')
        
        # Linear projection
        out = self.linear(x[:, :, 0])  # Univariate: ch·ªâ d√πng close
        
        # RevIN denormalize
        out = self.revin(out.unsqueeze(-1), 'denorm').squeeze(-1)
        return out
```

### 3.2 RDLinear (DLinear + RevIN)

<p align="center">
  <img src="images/RDLinear.png" alt="RDLinear Architecture" width="600">
  <br><em>H√¨nh 13. Ki·∫øn tr√∫c DLinear + RevIN: Decomposition th√†nh Trend + Seasonal.</em>
</p>

**√ù t∆∞·ªüng:** T√°ch chu·ªói th·ªùi gian th√†nh 2 th√†nh ph·∫ßn:
- **Trend**: Xu h∆∞·ªõng d√†i h·∫°n (t√≠nh b·∫±ng Moving Average)
- **Seasonal**: Bi·∫øn ƒë·ªông ng·∫Øn h·∫°n (ph·∫ßn c√≤n l·∫°i)

**C√¥ng th·ª©c:**
$$x_{trend} = \text{MovingAvg}(x, kernel)$$
$$x_{seasonal} = x - x_{trend}$$
$$\hat{y} = W_t \cdot x_{trend} + W_s \cdot x_{seasonal}$$

```python
class DLinear(nn.Module):
    def __init__(self, seq_len, pred_len, num_features, kernel_size=25):
        super().__init__()
        self.revin = RevIN(num_features)
        self.moving_avg = nn.AvgPool1d(kernel_size, stride=1, padding=kernel_size//2)
        self.linear_trend = nn.Linear(seq_len, pred_len)
        self.linear_seasonal = nn.Linear(seq_len, pred_len)
        
    def forward(self, x):
        # RevIN normalize
        x = self.revin(x, 'norm')
        x_in = x[:, :, 0]  # Univariate
        
        # Decomposition
        trend = self.moving_avg(x_in.unsqueeze(1)).squeeze(1)
        seasonal = x_in - trend
        
        # Separate linear projections
        out = self.linear_trend(trend) + self.linear_seasonal(seasonal)
        
        # RevIN denormalize
        out = self.revin(out.unsqueeze(-1), 'denorm').squeeze(-1)
        return out
```

**T·∫°i sao DLinear t·ªët h∆°n?**
- Trend v√† Seasonal c√≥ patterns kh√°c nhau ‚Üí c·∫ßn weights kh√°c nhau
- Linear ƒë∆°n ph·∫£i h·ªçc c·∫£ 2 patterns c√πng l√∫c ‚Üí kh√≥ h∆°n

---

## 4. Regime-Specific Training

### 4.1 √ù t∆∞·ªüng

Thay v√¨ train **m·ªôt model duy nh·∫•t** tr√™n to√†n b·ªô d·ªØ li·ªáu, ta:
1. D√πng **HMM ƒë·ªÉ ph√¢n c·ª•m** d·ªØ li·ªáu th√†nh c√°c regimes (tr·∫°ng th√°i th·ªã tr∆∞·ªùng ·∫©n)
2. **Train m·ªôt model ri√™ng** tr√™n d·ªØ li·ªáu c·ªßa t·ª´ng regime
3. Khi forecast: x√°c ƒë·ªãnh **regime hi·ªán t·∫°i** ‚Üí ch·ªçn model ƒë√≥ ‚Üí predict

### 4.2 Code

```python
# === TRAINING ===
# 1. Fit HMM tr√™n TRAIN data
hmm = GaussianHMM(n_components=3)
hmm.fit(train_features)  # features = [returns, volatility, trend]

# 2. Predict regimes cho TRAIN + VAL
regimes = hmm.predict(trainval_features)

# 3. Train model ri√™ng cho m·ªói regime
models = {}
for r in [0, 1, 2]:
    mask = (regimes == r)
    X_r, y_r = X_trainval[mask], y_trainval[mask]
    
    models[r] = DLinear(seq_len, pred_len, num_features)
    train(models[r], X_r, y_r)

# === PREDICTION ===
# 4. L·∫•y regime cu·ªëi c√πng
current_regime = regimes[-1]

# 5. D√πng model t∆∞∆°ng ·ª©ng ƒë·ªÉ predict
prediction = models[current_regime](last_sequence)
```

### 4.3 T·∫°i sao hi·ªáu qu·∫£?

| C√°ch ti·∫øp c·∫≠n | V·∫•n ƒë·ªÅ |
|---------------|--------|
| **1 model cho t·∫•t c·∫£** | Ph·∫£i h·ªçc c√πng l√∫c pattern c·ªßa bull, bear, sideways ‚Üí confused |
| **Model ri√™ng theo regime** | M·ªói model ch·ªâ t·∫≠p trung h·ªçc pattern c·ªßa 1 regime ‚Üí specialized |

**V√≠ d·ª•:**
- **Regime 0 (Stable):** Model h·ªçc pattern ·ªïn ƒë·ªãnh, volatility th·∫•p
- **Regime 1 (Transition):** Model h·ªçc c√°c d·∫•u hi·ªáu ƒë·ªïi h∆∞·ªõng
- **Regime 2 (Volatile):** Model h·ªçc c√°ch x·ª≠ l√Ω bi·∫øn ƒë·ªông m·∫°nh

---

# IV. Lu·ªìng x·ª≠ l√Ω

## T·ªïng quan Pipeline

```mermaid
flowchart TB
    subgraph STEP1["STEP 1: DATA LOADING"]
        A[("üóÉÔ∏è Raw Data<br/>1149 x 6")]
    end
    
    subgraph STEP2["STEP 2: DATA SPLITTING"]
        direction LR
        B1["üîµ TRAIN<br/>839 days"]
        B2["üü° VAL<br/>210 days"]
        B3["üî¥ TEST<br/>100 days"]
    end
    
    subgraph STEP3["STEP 3: FEATURE ENGINEERING"]
        direction LR
        C1["üìä Log Transform"]
        C2["üìà Spread Features"]
        C3["üìâ HMM Features"]
    end
    
    subgraph STEP4["STEP 4: HMM REGIME DETECTION"]
        D1["fit on TRAIN"]
        D2["predict on TRAIN+VAL"]
        D3["Regime Labels: 0, 1, 2"]
    end
    
    subgraph STEP5["STEP 5: MODEL TRAINING"]
        direction LR
        E1["Model 0<br/>Stable"]
        E2["Model 1<br/>Transition"]
        E3["Model 2<br/>Volatile"]
    end
    
    subgraph STEP6["STEP 6: EVALUATION"]
        F1["Grid Search on VAL"]
        F2["Final Eval on TEST"]
    end
    
    subgraph STEP7["STEP 7: PRODUCTION"]
        G1["Retrain on 95% full data"]
        G2["Select model by regimes[-1]"]
    end
    
    subgraph OUTPUT["FINAL: SUBMISSION"]
        H[("üìÑ submission.csv<br/>100 days forecast")]
    end
    
    A --> B1 & B2 & B3
    B1 & B2 --> C1 & C2 & C3
    B3 -.->|"for comparison"| F2
    C1 & C2 & C3 --> D1
    D1 --> D2 --> D3
    D3 --> E1 & E2 & E3
    E1 & E2 & E3 --> F1
    F1 --> F2
    F2 --> G1 --> G2 --> H
```

---

## 1. Data Loading

Load d·ªØ li·ªáu OHLCV (Open, High, Low, Close, Volume) t·ª´ file CSV:

```python
df = pd.read_csv('data/FPT_train.csv')
# Columns: time, open, high, low, close, volume
# 1149 rows (days)
```

---

## 2. Feature Engineering

```mermaid
flowchart LR
    A["Raw Data<br/>OHLCV"] --> B["Log Transform"]
    A --> C["Spread Features"]
    B --> D["close_log<br/>volume_log"]
    C --> E["HL_Spread<br/>OC_Spread"]
    D & E --> F["HMM Features"]
    F --> G["returns<br/>volatility<br/>trend"]
```

### 2.1 Log Transform

√Åp d·ª•ng log transform cho `close` v√† `volume` ƒë·ªÉ ·ªïn ƒë·ªãnh ph∆∞∆°ng sai:

```python
df['close_log'] = np.log(df['close'])
df['volume_log'] = np.log(df['volume'] + 1)
```

**T·∫°i sao?** D·ªØ li·ªáu t√†i ch√≠nh th∆∞·ªùng c√≥ ph√¢n ph·ªëi l·ªách ph·∫£i. Log transform gi√∫p:
- ·ªîn ƒë·ªãnh ph∆∞∆°ng sai
- D·ªÖ h·ªçc pattern h∆°n

### 2.2 Spread Features

<p align="center">
  <img src="images/spread_features.png" alt="Spread Features" width="680">
  <br><em>H√¨nh 14. Visualization c√°c Spread Features tr√™n d·ªØ li·ªáu FPT.</em>
</p>

**HL_Spread (High-Low Spread):**
$$HL\_Spread = \frac{High - Low}{Low} \times 100\%$$

- ƒêo **ƒë·ªô bi·∫øn ƒë·ªông trong ng√†y**
- Cao ‚Üí th·ªã tr∆∞·ªùng bi·∫øn ƒë·ªông m·∫°nh
- Th·∫•p ‚Üí th·ªã tr∆∞·ªùng ·ªïn ƒë·ªãnh

**OC_Spread (Open-Close Spread):**
$$OC\_Spread = \frac{Close - Open}{Open} \times 100\%$$

- ƒêo **xu h∆∞·ªõng trong ng√†y**
- D∆∞∆°ng (xanh) ‚Üí ng√†y tƒÉng
- √Çm (ƒë·ªè) ‚Üí ng√†y gi·∫£m

```python
df['HL_Spread'] = (df['high'] - df['low']) / df['low']
df['OC_Spread'] = (df['close'] - df['open']) / df['open']
```

### 2.3 HMM Features

Nh∆∞ ƒë√£ tr√¨nh b√†y ·ªü ph·∫ßn III.2:

```python
df['returns'] = df['close'].pct_change()
df['volatility'] = df['returns'].rolling(window=10).std()
df['trend'] = df['close'].rolling(window=10).mean().pct_change()
```

---

## 3. Data Splitting

```mermaid
pie title Data Split (1149 days)
    "TRAIN (839 - 73%)" : 839
    "VAL (210 - 18%)" : 210
    "TEST (100 - 9%)" : 100
```

| Split | Days | M·ª•c ƒë√≠ch |
|-------|------|----------|
| **TRAIN** | 839 (73%) | Train model |
| **VAL** | 210 (18%) | Early stopping, tuning |
| **TEST** | 100 (9%) | ƒê√°nh gi√° cu·ªëi c√πng |

---

## 4. HMM Regime Detection

```mermaid
flowchart TB
    HMM["GaussianHMM<br/>n_components=3"]
    
    HMM --> |"fit()"| FIT["Learn patterns<br/>from TRAIN only"]
    HMM --> |"predict()"| PRED["Label each day<br/>in TRAIN+VAL"]
    PRED --> LABELS["Regime Labels<br/>[0,1,2,0,1,1,2,...]"]
    LABELS --> LAST["regimes[-1]<br/>= Current Regime"]
    
    LAST --> |"Regime g√¨?"| SELECT["Ch·ªçn model<br/>t∆∞∆°ng ·ª©ng"]
```

> ‚ö†Ô∏è **L∆ØU √ù QUAN TR·ªåNG: Tr√°nh Data Leakage**
> 
> - **fit()** CH·ªà tr√™n TRAIN ‚Üí ƒë·ªÉ h·ªçc patterns
> - **predict()** tr√™n TRAIN+VAL ‚Üí ƒë·ªÉ c√≥ regime labels cho c·∫£ 2
> - KH√îNG predict ƒë∆∞·ª£c tr√™n TEST v√¨ ch∆∞a c√≥ data!

```python
# Fit HMM CH·ªà tr√™n TRAIN
hmm = RegimeDetector(n_components=3)
hmm.fit(train_df)

# Predict tr√™n TRAIN + VAL
regimes = hmm.predict(trainval_df)
```

---

## 5. Model Training (Per Regime)

```mermaid
flowchart TB
    LABELS["Regime Labels"] --> R0 & R1 & R2
    
    subgraph R0["Regime 0 - Stable"]
        D0["Data Regime 0"] --> M0["Model 0<br/>DLinear"]
    end
    
    subgraph R1["Regime 1 - Transition"]
        D1["Data Regime 1"] --> M1["Model 1<br/>DLinear"]
    end
    
    subgraph R2["Regime 2 - Volatile"]
        D2["Data Regime 2"] --> M2["Model 2<br/>DLinear"]
    end
```

V·ªõi m·ªói regime, train m·ªôt model ri√™ng:

```python
for r in [0, 1, 2]:
    mask = (regimes == r)
    X_r, y_r = X_trainval[mask], y_trainval[mask]
    models[r] = DLinear(seq_len, pred_len, num_features)
    train(models[r], X_r, y_r)
```

---

## 6. Evaluation on TEST

ƒê√°nh gi√° model tr√™n TEST set ƒë·ªÉ ki·ªÉm tra:

```python
# L·∫•y regime cu·ªëi c·ªßa TRAINVAL
test_regime = regimes[-1]

# D√πng model t∆∞∆°ng ·ª©ng ƒë·ªÉ predict
predictions = models[test_regime](X_test)

# T√≠nh MSE
test_mse = ((predictions - y_test) ** 2).mean()
```

**M·ª•c ƒë√≠ch:** ƒê·∫£m b·∫£o pipeline ho·∫°t ƒë·ªông t·ªët tr∆∞·ªõc khi submit.

---

## 7. Production & Submission

```mermaid
flowchart LR
    A["Best Config"] --> B["Retrain on 95%"]
    B --> C["Get regimes[-1]"]
    C --> D{"Current<br/>Regime?"}
    D --> |"0"| M0["Model 0"]
    D --> |"1"| M1["Model 1"]
    D --> |"2"| M2["Model 2"]
    M0 & M1 & M2 --> E["Predict 100 days"]
    E --> F["Inverse Transform"]
    F --> G[("submission.csv")]
```

### 7.1 Retrain for Production

Sau khi ƒë√£ validate xong, retrain tr√™n **95% to√†n b·ªô data**:

```python
# Chia l·∫°i data: 95% train, 5% kept for regime detection
production_df = full_df.iloc[:int(len(full_df) * 0.95)]

# Fit HMM l·∫°i tr√™n production data
hmm.fit(production_df)
regimes = hmm.predict(production_df)
```

### 7.2 Select Model by Current Regime

```python
# Regime cu·ªëi c√πng = "t√¢m l√Ω th·ªã tr∆∞·ªùng" hi·ªán t·∫°i
current_regime = regimes[-1]
print(f"Current market regime: {current_regime}")
```

### 7.3 Final Prediction & Submission

```python
# Ch·ªçn model t∆∞∆°ng ·ª©ng v·ªõi current regime
final_model = models[current_regime]

# Predict 100 ng√†y
last_sequence = get_last_sequence(production_df)
predictions = final_model(last_sequence)

# Inverse transform (n·∫øu d√πng log)
predictions = np.exp(predictions)

# T·∫°o submission
submission = pd.DataFrame({
    'row_id': range(len(predictions)),
    'close': predictions
})
submission.to_csv('submission.csv', index=False)
```

---

# V. K·∫øt qu·∫£ ƒë√°nh gi√°

> **M·∫πo:** D·ªØ li·ªáu FPT c√≥ th·ªÉ c√†o ƒë∆∞·ª£c t·ª´ th∆∞ vi·ªán **Vnstock**, n√™n nh√≥m ƒë√£ c√†o hidden test v·ªÅ ƒë·ªÉ ƒë√°nh gi√° chi ti·∫øt h∆°n.

## B·∫£ng k·∫øt qu·∫£

| # | Model | Config | Hidden MSE | Train MSE | Nh·∫≠n x√©t |
|---|-------|--------|------------|-----------|----------|
| 1 | **Univariate** | DLinear \| Seq480 | **34.55** | 4118 | ü•á T·ªët nh·∫•t |
| 2 | Univariate | Linear \| Seq480 | 39.33 | 4188 | ü•à |
| 3 | Multivariate | DLinear \| Seq60 | 56.35 | 550 | HMM gi√∫p √≠ch |
| 4 | Multivariate | Linear \| Seq60 | 64.64 | 633 | HMM gi√∫p √≠ch |
| 5 | Univariate | DLinear \| Seq60 | 203.53 | 179 | Overfitting |
| 6 | Univariate | Linear \| Seq60 | 205.47 | 182 | Overfitting |
| 7 | Multivariate | DLinear \| Seq60 | 249.19 | 193 | NoHMM - k√©m |
| 8 | Multivariate | Linear \| Seq60 | 253.08 | 195 | NoHMM - k√©m |
| ... | ... | ... | ... | ... | ... |

<p align="center">
  <img src="images/top4_predictions.png" alt="Top 4 Predictions" width="700">
  <br><em>H√¨nh 15. So s√°nh Top 4 predictions v·ªõi actual values tr√™n hidden test.</em>
</p>

## Ph√¢n t√≠ch chi ti·∫øt

### Top 1-2: Univariate + Seq480 

```
Univariate DLinear Seq480: TrainMSE=4118, HiddenMSE=34.55
Univariate Linear Seq480:  TrainMSE=4188, HiddenMSE=39.33
```

**T·∫°i sao trainMSE cao nh∆∞ng hiddenMSE l·∫°i th·∫•p?**

1. **Seq480 = 480 ng√†y input = ~2 nƒÉm d·ªØ li·ªáu**
   - Model nh√¨n th·∫•y trend d√†i h·∫°n
   - √çt b·ªã ·∫£nh h∆∞·ªüng b·ªüi nhi·ªÖu ng·∫Øn h·∫°n
   
2. **Univariate ch·ªâ d√πng `close`**
   - Kh√¥ng b·ªã nhi·ªÖu t·ª´ c√°c features kh√°c


3. **TrainMSE cao = kh√¥ng overfitting**
   - Model h·ªçc pattern t·ªïng qu√°t thay v√¨ nh·ªõ training data
   - Generalize t·ªët h∆°n tr√™n hidden test

### Top 3-4: Multivariate + HMM + Seq60

```
Multivariate DLinear Seq60 + HMM: TrainMSE=550, HiddenMSE=56.35
Multivariate Linear Seq60 + HMM:  TrainMSE=633, HiddenMSE=64.64
```

**T·∫°i sao multivariate v·ªõi HMM l·∫°i kh√° t·ªët?**

1. **HMM gi√∫p ph√¢n c·ª•m data theo regime**
   - M·ªói model ch·ªâ h·ªçc pattern c·ªßa 1 regime
   - Gi·∫£m conflict gi·ªØa c√°c patterns kh√°c nhau

2. **Multivariate + HMM = combination t·ªët**
   - Spread features gi√∫p HMM detect regime t·ªët h∆°n
   - Model nh·∫≠n th√™m th√¥ng tin t·ª´ nhi·ªÅu features

### Univariate + Seq60 (5-6)

```
Univariate DLinear Seq60: TrainMSE=179, HiddenMSE=203.53
Univariate Linear Seq60:  TrainMSE=182, HiddenMSE=205.47
```

**D·∫•u hi·ªáu overfitting r√µ r√†ng:**

| TrainMSE | HiddenMSE | Ratio |
|----------|-----------|-------|
| 179 | 203.53 | 1.14x |
| 182 | 205.47 | 1.13x |

- **Seq60 = ch·ªâ 60 ng√†y input = ~3 th√°ng**
- Model h·ªçc ƒë∆∞·ª£c patterns ng·∫Øn h·∫°n r·∫•t t·ªët (trainMSE th·∫•p)
- Nh∆∞ng patterns ƒë√≥ kh√¥ng generalize (hiddenMSE cao)

### Multivariate NoHMM (7-8)

```
Multivariate DLinear Seq60 NoHMM: TrainMSE=193, HiddenMSE=249.19
Multivariate Linear Seq60 NoHMM:  TrainMSE=195, HiddenMSE=253.08
```

**V·∫•n ƒë·ªÅ:**
- Kh√¥ng c√≥ HMM ‚Üí model ph·∫£i h·ªçc c√πng l√∫c t·∫•t c·∫£ regimes
- Multivariate th√™m noise t·ª´ c√°c features
- K·∫øt qu·∫£: performance k√©m h∆°n univariate

## K·∫øt lu·∫≠n

| Insight | Gi·∫£i th√≠ch |
|---------|------------|
| **Univariate > Multivariate** | √çt noise h∆°n, t·∫≠p trung v√†o target |
| **Seq480 > Seq60** | Nh√¨n trend d√†i h·∫°n, tr√°nh overfitting |
| **DLinear > Linear** | Trend-Seasonal decomposition gi√∫p √≠ch |
| **HMM gi√∫p Multivariate** | Ph√¢n c·ª•m data gi·∫£m conflict |
| **TrainMSE cao ‚â† x·∫•u** | C√≥ th·ªÉ l√† d·∫•u hi·ªáu c·ªßa generalization t·ªët |

---

# Bonus: K·∫øt qu·∫£ tr√™n VIC

Nh√≥m c≈©ng √°p d·ª•ng pipeline t∆∞∆°ng t·ª± cho c·ªï phi·∫øu **VIC (Vingroup)**:

## D·ªØ li·ªáu VIC

<p align="center">
  <img src="images/vic_train_vs_hidden.png" alt="VIC Train vs Hidden Test" width="680">
  <br><em>H√¨nh 16. D·ªØ li·ªáu VIC: Train (xanh) vs Hidden Test (cam).</em>
</p>

**ƒê·∫∑c ƒëi·ªÉm VIC kh√°c FPT:**
- Downtrend d√†i t·ª´ 2019-2023 (~120 ‚Üí ~40)
- Hidden test c√≥ rally m·∫°nh (~40 ‚Üí ~120)
- **Th√°ch th·ª©c l·ªõn:** Model train tr√™n downtrend, ph·∫£i predict uptrend!

## So s√°nh c√°c predictions

<p align="center">
  <img src="images/vic_predictions.png" alt="VIC Predictions" width="680">
  <br><em>H√¨nh 17. So s√°nh predictions c·ªßa c√°c models tr√™n VIC hidden test.</em>
</p>

**Nh·∫≠n x√©t:**
- T·∫•t c·∫£ models ƒë·ªÅu **underestimate** rally m·∫°nh c·ªßa VIC
- ƒêi·ªÅu n√†y h·ª£p l√Ω v√¨:
  - Model ch·ªâ th·∫•y downtrend trong training data
  - Kh√¥ng c√≥ th√¥ng tin g√¨ v·ªÅ catalyst (news, events) g√¢y rally
  - **Regime shift** t·ª´ bearish ‚Üí bullish kh√¥ng ƒë∆∞·ª£c capture

**B√†i h·ªçc:**
> LTSF-Linear (v√† c√°c technical models n√≥i chung) ch·ªâ c√≥ th·ªÉ d·ª± ƒëo√°n d·ª±a tr√™n **historical patterns**. Khi c√≥ **regime change** m·∫°nh (fundamental shifts), models s·∫Ω kh√≥ predict ch√≠nh x√°c.

---

# VI. K·∫øt lu·∫≠n

## T√≥m t·∫Øt

Trong project n√†y, nh√≥m ƒë√£:

1. **√Åp d·ª•ng LTSF-Linear** cho b√†i to√°n d·ª± ƒëo√°n gi√° c·ªï phi·∫øu
2. **S·ª≠ d·ª•ng RevIN** ƒë·ªÉ x·ª≠ l√Ω distribution shift
3. **K·∫øt h·ª£p HMM Regime-Switching** ƒë·ªÉ ph√¢n c·ª•m market states
4. **Grid Search** ƒë·ªÉ t√¨m config t·ªët nh·∫•t

## Findings ch√≠nh

| Finding | Gi·∫£i th√≠ch |
|---------|------------|
| **Univariate DLinear Seq480 = Best** | ƒê∆°n gi·∫£n, nh√¨n trend d√†i h·∫°n |
| **HMM gi√∫p Multivariate** | Ph√¢n c·ª•m gi·∫£m conflict |
| **TrainMSE kh√¥ng ph·∫£i t·∫•t c·∫£** | C·∫ßn ƒë√°nh gi√° tr√™n unseen data |

## H·∫°n ch·∫ø

- **Regime assumption:** Gi·∫£ ƒë·ªãnh regime cu·ªëi c√πng ti·∫øp t·ª•c trong 100 ng√†y
- **No external factors:** Kh√¥ng c√≥ news, events, macro data
- **Linear models:** C√≥ th·ªÉ miss non-linear patterns

## H∆∞·ªõng ph√°t tri·ªÉn

1. Th√™m **external features** (sentiment, news)
2. Th·ª≠ **ensemble** multiple regimes
3. Combine v·ªõi **Transformer-based** models

---

**üéâ C·∫£m ∆°n b·∫°n ƒë√£ ƒë·ªçc!**

