# ğŸ“ˆ Dá»± ÄoÃ¡n GiÃ¡ Cá»• Phiáº¿u FPT - LTSF-Linear vá»›i Advanced Grid Search

![Project Banner](https://img.shields.io/badge/Project-Time%20Series%20Forecasting-blue?style=for-the-badge&logo=python)
![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)
![Status](https://img.shields.io/badge/Status-Active-success?style=for-the-badge)

## ğŸ“– Tá»•ng Quan (Overview)

Dá»± Ã¡n nÃ y táº­p trung vÃ o viá»‡c dá»± Ä‘oÃ¡n giÃ¡ Ä‘Ã³ng cá»­a cá»§a cá»• phiáº¿u **FPT** trong **100 ngÃ y tiáº¿p theo** sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t **Long-Term Time Series Forecasting (LTSF)** tiÃªn tiáº¿n. ChÃºng tÃ´i Ã¡p dá»¥ng cÃ¡c mÃ´ hÃ¬nh há» **Linear** (Linear, DLinear, NLinear) káº¿t há»£p vá»›i **Reversible Instance Normalization (RevIN)** vÃ  **Hidden Markov Model (HMM)** Ä‘á»ƒ xá»­ lÃ½ sá»± thay Ä‘á»•i phÃ¢n phá»‘i dá»¯ liá»‡u (distribution shift) vÃ  thÃ­ch nghi vá»›i cÃ¡c cháº¿ Ä‘á»™ thá»‹ trÆ°á»ng (market regimes) khÃ¡c nhau.

Má»¥c tiÃªu chÃ­nh lÃ  tÃ¬m ra cáº¥u hÃ¬nh mÃ´ hÃ¬nh tá»‘i Æ°u thÃ´ng qua **Grid Search** toÃ n diá»‡n trÃªn khÃ´ng gian siÃªu tham sá»‘ rá»™ng lá»›n.

> [!TIP]
> **Chiáº¿n lÆ°á»£c Kaggle**: Táº­n dá»¥ng giá»›i háº¡n tá»‘i Ä‘a **500 submissions** cá»§a cuá»™c thi, chÃºng tÃ´i thá»±c hiá»‡n chiáº¿n lÆ°á»£c "VÃ©t cáº¡n thÃ´ng minh" (Smart Exhaustive Search). Thay vÃ¬ chá»‰ chá»n má»™t vÃ i mÃ´ hÃ¬nh tá»‘t nháº¥t, chÃºng tÃ´i táº¡o ra hÃ ng loáº¡t biáº¿n thá»ƒ Ä‘á»ƒ bao phá»§ má»i kháº£ nÄƒng, tá»« Ä‘Ã³ tÃ¬m ra "Global Optima" thá»±c sá»± cho bÃ i toÃ¡n dá»± bÃ¡o nÃ y.

## ğŸš€ TÃ­nh NÄƒng ChÃ­nh (Key Features)

*   **Äa dáº¡ng MÃ´ hÃ¬nh (Models)**: Há»— trá»£ 3 biáº¿n thá»ƒ hiá»‡n Ä‘áº¡i cá»§a Linear models:
    *   **Linear**: Máº¡ng nÆ¡-ron má»™t lá»›p Ä‘Æ¡n giáº£n nhÆ°ng hiá»‡u quáº£.
    *   **DLinear**: Decomposition Linear - tÃ¡ch chuá»—i thá»i gian thÃ nh Trend (xu hÆ°á»›ng) vÃ  Seasonality (mÃ¹a vá»¥).
    *   **NLinear**: Normalization Linear - chuáº©n hÃ³a Ä‘áº§u vÃ o báº±ng cÃ¡ch trá»« Ä‘i giÃ¡ trá»‹ cuá»‘i cÃ¹ng.
*   **Biáº¿n thá»ƒ (Variants)**:
    *   **Univariate**: Chá»‰ sá»­ dá»¥ng chuá»—i giÃ¡ Ä‘Ã³ng cá»­a (`close_log`).
    *   **Multivariate**: Sá»­ dá»¥ng thÃªm khá»‘i lÆ°á»£ng (`volume_log`) vÃ  cÃ¡c chá»‰ sá»‘ spread (`HL_Spread`, `OC_Spread`).
*   **Ká»¹ thuáº­t NÃ¢ng cao**:
    *   **RevIN (Reversible Instance Normalization)**: Giáº£i quyáº¿t váº¥n Ä‘á» distribution shift báº±ng cÃ¡ch chuáº©n hÃ³a Ä‘áº§u vÃ o vÃ  giáº£i chuáº©n hÃ³a Ä‘áº§u ra, giÃºp mÃ´ hÃ¬nh há»c tá»‘t hÆ¡n trÃªn dá»¯ liá»‡u khÃ´ng dá»«ng (non-stationary).
    *   **HMM (Hidden Markov Model)**: PhÃ¡t hiá»‡n cÃ¡c tráº¡ng thÃ¡i áº©n cá»§a thá»‹ trÆ°á»ng (vÃ­ dá»¥: TÄƒng trÆ°á»Ÿng, Suy thoÃ¡i, Äi ngang) dá»±a trÃªn `returns`, `volatility`, vÃ  `trend`. MÃ´ hÃ¬nh sáº½ tá»± Ä‘á»™ng chuyá»ƒn Ä‘á»•i chiáº¿n lÆ°á»£c dá»± Ä‘oÃ¡n tÃ¹y theo tráº¡ng thÃ¡i hiá»‡n táº¡i.
*   **Tá»‘i Æ°u hÃ³a (Optimization)**:
    *   **Grid Search**: Tá»± Ä‘á»™ng thá»­ nghiá»‡m hÃ ng trÄƒm tá»• há»£p tham sá»‘ (Sequence Length, Model Type, HMM Configs).
    *   **Early Stopping**: NgÄƒn cháº·n overfitting.
    *   **Learning Rate Scheduler**: Äiá»u chá»‰nh tá»‘c Ä‘á»™ há»c Ä‘á»™ng.

## ğŸ”„ Quy TrÃ¬nh Xá»­ LÃ½ (Pipeline)

Biá»ƒu Ä‘á»“ dÆ°á»›i Ä‘Ã¢y mÃ´ táº£ chi tiáº¿t luá»“ng xá»­ lÃ½ dá»¯ liá»‡u vÃ  huáº¥n luyá»‡n mÃ´ hÃ¬nh trong `FPT_LTSF_GridSearch.ipynb`:

```mermaid
graph TD
    %% Nodes
    Start([Start])
    LoadData[Load Data: FPT_train.csv]
    
    subgraph Preprocessing [Preprocessing & Feature Engineering]
        LogTransform[Log Transformation: Close, Volume]
        SpreadFeat[Spread Features: High-Low, Open-Close]
        HMMFeat[HMM Features: Returns, Volatility, Trend]
    end
    
    subgraph GridSearch [Grid Search Loop]
        Config[Select Hyperparameters: Model, Variant, Seq_Len, RevIN, HMM]
        
        subgraph HMM_Logic [HMM Logic]
            CheckHMM{Use HMM?}
            TrainHMM[Train GaussianHMM]
            DetectRegime[Detect Regimes: 0, 1, 2...]
            SplitRegime[Split Data by Regime]
        end
        
        subgraph Model_Architecture [Model Architecture]
            CheckModel{Model Type?}
            
            CheckModel -- Linear/DLinear --> CheckRevIN{Use RevIN?}
            CheckRevIN -- Yes --> AddRevIN[Add RevIN Layer]
            CheckRevIN -- No --> RawInput[Raw Input]
            
            CheckModel -- NLinear --> InternalNorm["Internal Normalization (Last Value Subtraction)"]
            
            AddRevIN --> InitModel[Initialize Model]
            RawInput --> InitModel
            InternalNorm --> InitModel
        end
        
        subgraph Model_Training [Model Training]
            TrainLoop[Training Loop with Early Stopping]
            Validate[Evaluate on Validation Set]
        end
        
        subgraph Retrain_Submission [Retrain & Submission]
            ReSplit[Re-split Data: Train on 95% Data]
            Retrain[Retrain Model on Larger Set]
            Predict[Generate Predictions]
            SaveResult[Save Submission & Logs]
        end
    end
    
    End([End])

    %% Edges
    Start --> LoadData
    LoadData --> LogTransform
    LogTransform --> SpreadFeat
    SpreadFeat --> HMMFeat
    HMMFeat --> Config
    
    Config --> CheckHMM
    CheckHMM -- Yes --> TrainHMM
    TrainHMM --> DetectRegime
    DetectRegime --> SplitRegime
    SplitRegime --> CheckModel
    
    CheckHMM -- No --> CheckModel
    
    InitModel --> TrainLoop
    TrainLoop --> Validate
    Validate --> ReSplit
    ReSplit --> Retrain
    Retrain --> Predict
    Predict --> SaveResult
    SaveResult --> Config
    
    Config -- All Combinations Done --> End
    
    %% Styling
    style Start fill:#f9f,stroke:#333,stroke-width:2px
    style End fill:#f9f,stroke:#333,stroke-width:2px
    style GridSearch fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    style Preprocessing fill:#fff3e0,stroke:#e65100,stroke-width:2px
    style Retrain_Submission fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
    style Model_Architecture fill:#fff9c4,stroke:#fbc02d,stroke-width:2px
```

## ğŸ“‚ Cáº¥u TrÃºc Dá»± Ãn (Project Structure)

```
Project-6.1/
â”œâ”€â”€ FPT_LTSF_GridSearch.ipynb       # Notebook chÃ­nh: Grid Search toÃ n diá»‡n, EDA, vÃ  Training
â”œâ”€â”€ testing_increas_pred_val.py     # Script: Tinh chá»‰nh Ä‘á»™ dÃ i dá»± Ä‘oÃ¡n (Pred Len Tuning) vÃ  thá»­ nghiá»‡m HMM
â”œâ”€â”€ data/
â”‚   â””â”€â”€ FPT_train.csv               # Dá»¯ liá»‡u lá»‹ch sá»­ giÃ¡ cá»• phiáº¿u FPT
â”œâ”€â”€ submissions/                    # ThÆ° má»¥c chá»©a káº¿t quáº£ dá»± Ä‘oÃ¡n (file .csv)
â”œâ”€â”€ results/                        # ThÆ° má»¥c chá»©a logs vÃ  báº£ng tá»•ng há»£p káº¿t quáº£
â””â”€â”€ README.md                       # TÃ i liá»‡u hÆ°á»›ng dáº«n chi tiáº¿t (File nÃ y)
```

### Chi tiáº¿t cÃ¡c file:
*   **`FPT_LTSF_GridSearch.ipynb`**: ÄÃ¢y lÃ  "bá»™ nÃ£o" cá»§a dá»± Ã¡n. NÃ³ thá»±c hiá»‡n:
    1.  Táº£i vÃ  tiá»n xá»­ lÃ½ dá»¯ liá»‡u (Log transform, Feature engineering).
    2.  Äá»‹nh nghÄ©a cÃ¡c lá»›p mÃ´ hÃ¬nh (Linear, DLinear, NLinear) vÃ  RevIN.
    3.  Cháº¡y vÃ²ng láº·p Grid Search qua cÃ¡c tham sá»‘: `Seq_Len` (7, 15, ..., 480), `Model`, `Variant`, `HMM`.
    4.  LÆ°u káº¿t quáº£ tá»‘t nháº¥t vÃ o thÆ° má»¥c `submissions/`.
*   **`testing_increas_pred_val.py`**: Má»™t script Python Ä‘á»™c láº­p dÃ¹ng Ä‘á»ƒ kiá»ƒm thá»­ chuyÃªn sÃ¢u hÆ¡n vá» Ä‘á»™ dÃ i dá»± Ä‘oÃ¡n (`Pred_Len`) vÃ  tinh chá»‰nh cÃ¡c tham sá»‘ HMM (sá»‘ lÆ°á»£ng regime, window size).

## ğŸ› ï¸ CÃ i Äáº·t & YÃªu Cáº§u (Installation)

Dá»± Ã¡n yÃªu cáº§u **Python 3.7+** vÃ  cÃ¡c thÆ° viá»‡n sau:

```bash
pip install torch pandas numpy scikit-learn hmmlearn matplotlib seaborn tqdm
```

### CÃ¡c thÆ° viá»‡n chÃ­nh:
*   `torch`: Framework Deep Learning.
*   `hmmlearn`: ThÆ° viá»‡n cho Hidden Markov Models.
*   `pandas`, `numpy`: Xá»­ lÃ½ dá»¯ liá»‡u.
*   `matplotlib`, `seaborn`: Trá»±c quan hÃ³a dá»¯ liá»‡u.

## ğŸ“– HÆ°á»›ng Dáº«n Sá»­ Dá»¥ng (Usage)

### 1. Cháº¡y Grid Search (Notebook)
Má»Ÿ file `FPT_LTSF_GridSearch.ipynb` báº±ng Jupyter Notebook hoáº·c VS Code.
Cháº¡y láº§n lÆ°á»£t cÃ¡c cell Ä‘á»ƒ:
*   Thá»±c hiá»‡n EDA (KhÃ¡m phÃ¡ dá»¯ liá»‡u).
*   Huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh.
*   Xem biá»ƒu Ä‘á»“ phÃ¢n loáº¡i Regime thá»‹ trÆ°á»ng.
*   Káº¿t quáº£ dá»± Ä‘oÃ¡n sáº½ Ä‘Æ°á»£c lÆ°u tá»± Ä‘á»™ng.

### 2. Cháº¡y Tinh chá»‰nh Pred Len (Script)
Cháº¡y lá»‡nh sau trong terminal:

```bash
python testing_increas_pred_val.py
```
Script nÃ y sáº½:
*   Thá»­ nghiá»‡m cÃ¡c Ä‘á»™ dÃ i dá»± Ä‘oÃ¡n khÃ¡c nhau: `[100, 120, 150, 200, 220]`.
*   Sá»­ dá»¥ng cáº¥u hÃ¬nh tá»‘t nháº¥t tÃ¬m Ä‘Æ°á»£c tá»« Grid Search.
*   In ra MSE (Mean Squared Error) trÃªn táº­p validation.
*   LÆ°u file submission vÃ o `submissions/pred_len_tuning/`.

## ğŸ§  Chi Tiáº¿t PhÆ°Æ¡ng PhÃ¡p (Methodology)

### Feature Engineering
Dá»¯ liá»‡u gá»‘c Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i vÃ  táº¡o Ä‘áº·c trÆ°ng má»›i Ä‘á»ƒ tÄƒng tÃ­nh á»•n Ä‘á»‹nh vÃ  kháº£ nÄƒng há»c cá»§a mÃ´ hÃ¬nh:

1.  **Log Transformation**:
    *   CÃ´ng thá»©c: $x' = \ln(x + 1)$
    *   **CÃ´ng dá»¥ng**: Giáº£m Ä‘á»™ lá»‡ch (skewness) cá»§a phÃ¢n phá»‘i giÃ¡, giÃºp dá»¯ liá»‡u trá»Ÿ nÃªn "gáº§n chuáº©n" (Gaussian-like) hÆ¡n, á»•n Ä‘á»‹nh phÆ°Æ¡ng sai cho mÃ´ hÃ¬nh Linear.

2.  **Spread Features** (Biáº¿n Ä‘á»™ng trong ngÃ y):
    *   **HL_Spread (High-Low)**:
        *   CÃ´ng thá»©c: $S_{HL} = \ln(High) - \ln(Low)$
        *   **CÃ´ng dá»¥ng**: Äo lÆ°á»ng **biáº¿n Ä‘á»™ng ná»™i ngÃ y** (Intraday Volatility). GiÃ¡ trá»‹ cÃ ng lá»›n cho tháº¥y thá»‹ trÆ°á»ng cÃ ng giáº±ng co máº¡nh.
    *   **OC_Spread (Open-Close)**:
        *   CÃ´ng thá»©c: $S_{OC} = \ln(Close) - \ln(Open)$
        *   **CÃ´ng dá»¥ng**: Äo lÆ°á»ng **Ä‘á»™ng lá»±c giÃ¡** (Price Momentum) vÃ  hÆ°á»›ng di chuyá»ƒn trong ngÃ y (DÆ°Æ¡ng = TÄƒng, Ã‚m = Giáº£m).

3.  **HMM Features** (Äáº§u vÃ o cho Regime Detection):
    *   **Returns**:
        *   CÃ´ng thá»©c: $R_t = \ln(Close_t) - \ln(Close_{t-1})$
        *   **CÃ´ng dá»¥ng**: Tá»· suáº¥t sinh lá»i logarit, Ä‘áº·c trÆ°ng cÆ¡ báº£n nháº¥t cá»§a chuá»—i thá»i gian tÃ i chÃ­nh, cÃ³ tÃ­nh dá»«ng (stationary) cao hÆ¡n giÃ¡ gá»‘c.
    *   **Volatility**:
        *   CÃ´ng thá»©c: $Vol_t = Std(R_{t-k}:R_t)$ (Äá»™ lá»‡ch chuáº©n lÄƒn)
        *   **CÃ´ng dá»¥ng**: Äo lÆ°á»ng rá»§i ro thá»‹ trÆ°á»ng. Volatility cao thÆ°á»ng bÃ¡o hiá»‡u cÃ¡c giai Ä‘oáº¡n báº¥t á»•n hoáº·c Ä‘áº£o chiá»u.
    *   **Trend**:
        *   CÃ´ng thá»©c: $Trend_t = MA(Close_t) - Close_t$ (hoáº·c cÃ¡c biáº¿n thá»ƒ tÆ°Æ¡ng tá»±)
        *   **CÃ´ng dá»¥ng**: XÃ¡c Ä‘á»‹nh xu hÆ°á»›ng chá»§ Ä‘áº¡o (Uptrend/Downtrend) Ä‘á»ƒ HMM phÃ¢n loáº¡i tráº¡ng thÃ¡i.

### Hidden Markov Model (HMM)
ChÃºng tÃ´i sá»­ dá»¥ng `GaussianHMM` Ä‘á»ƒ chia dá»¯ liá»‡u lá»‹ch sá»­ thÃ nh cÃ¡c "Regime" (vÃ­ dá»¥: 3 regimes).
*   **Training**: Má»—i regime sáº½ cÃ³ má»™t mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n riÃªng biá»‡t Ä‘Æ°á»£c huáº¥n luyá»‡n chuyÃªn sÃ¢u trÃªn dá»¯ liá»‡u thuá»™c regime Ä‘Ã³.
*   **Inference**: Khi dá»± Ä‘oÃ¡n, há»‡ thá»‘ng xÃ¡c Ä‘á»‹nh regime hiá»‡n táº¡i vÃ  chá»n mÃ´ hÃ¬nh tÆ°Æ¡ng á»©ng Ä‘á»ƒ Ä‘Æ°a ra káº¿t quáº£ chÃ­nh xÃ¡c nháº¥t.

## ğŸ“Š Káº¿t Quáº£ (Results)

Káº¿t quáº£ dá»± Ä‘oÃ¡n (file `.csv`) bao gá»“m cá»™t `id` (ngÃ y dá»± Ä‘oÃ¡n) vÃ  `close` (giÃ¡ dá»± Ä‘oÃ¡n Ä‘Ã£ Ä‘Æ°á»£c inverse transform vá» thang Ä‘o gá»‘c).
TÃªn file káº¿t quáº£ chá»©a Ä‘áº§y Ä‘á»§ thÃ´ng tin cáº¥u hÃ¬nh, vÃ­ dá»¥:
`Sub_Multivariate_DLinear_RevIN_MSE_HMM3W60_Seq60_Pred100_ValMSE1234.csv`

## ğŸ“ˆ PhÃ¢n TÃ­ch & ÄÃ¡nh GiÃ¡ (Analysis & Insights)

Dá»±a trÃªn cÃ¡c thá»­ nghiá»‡m má»Ÿ rá»™ng (xem chi tiáº¿t trong `FPT_LTSF_Comparison.ipynb` vÃ  `testing_increas_pred_val.py`), chÃºng tÃ´i rÃºt ra má»™t sá»‘ káº¿t luáº­n quan trá»ng:

1.  **Loss Function Tuning (Huber vs MSE)**:
    *   Viá»‡c thay tháº¿ hÃ m máº¥t mÃ¡t máº·c Ä‘á»‹nh (MSE) báº±ng **Huber Loss** (Ä‘á»ƒ giáº£m tÃ¡c Ä‘á»™ng cá»§a outliers) **khÃ´ng mang láº¡i sá»± cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ** vá» Ä‘á»™ chÃ­nh xÃ¡c (RMSE/MAE tÆ°Æ¡ng Ä‘Æ°Æ¡ng).
    *   Äiá»u nÃ y cho tháº¥y dá»¯ liá»‡u FPT Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ tá»‘t bá»Ÿi RevIN vÃ  khÃ´ng chá»©a quÃ¡ nhiá»u nhiá»…u ngoáº¡i lai cá»±c Ä‘oan áº£nh hÆ°á»Ÿng Ä‘áº¿n quÃ¡ trÃ¬nh huáº¥n luyá»‡n.

2.  **Äá»™ DÃ i Dá»± ÄoÃ¡n (Prediction Length)**:
    *   Viá»‡c tÄƒng `pred_len` (tá»« 100 lÃªn 120, 150...) **khÃ´ng táº¡o ra tÃ¡c Ä‘á»™ng lá»›n** Ä‘áº¿n hiá»‡u suáº¥t mÃ´ hÃ¬nh.
    *   MÃ´ hÃ¬nh váº«n duy trÃ¬ Ä‘Æ°á»£c xu hÆ°á»›ng dá»± Ä‘oÃ¡n khÃ¡ tá»‘t ngay cáº£ á»Ÿ cÃ¡c chÃ¢n trá»i dá»± bÃ¡o xa hÆ¡n, chá»©ng tá» tÃ­nh á»•n Ä‘á»‹nh cá»§a kiáº¿n trÃºc Linear.

3.  **So SÃ¡nh MSE (Validation vs Hidden Test)**:
    *   **Quan sÃ¡t**: CÃ³ sá»± chÃªnh lá»‡ch ráº¥t lá»›n giá»¯a `ValMSE` (thÆ°á»ng > 4000) vÃ  `MSE` trÃªn táº­p Hidden Test (~34).
    *   **LÃ½ do**: `ValMSE` trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n Ä‘Æ°á»£c tÃ­nh trÃªn dá»¯ liá»‡u Ä‘Ã£ qua xá»­ lÃ½ (Log transform/RevIN), trong khi `MSE` trÃªn Hidden Test Ä‘Æ°á»£c tÃ­nh trÃªn giÃ¡ trá»‹ thá»±c táº¿ (Original Scale).
    *   **Ã nghÄ©a**: KhÃ´ng nÃªn so sÃ¡nh trá»±c tiáº¿p giÃ¡ trá»‹ Loss khi training vá»›i sai sá»‘ thá»±c táº¿. Tuy nhiÃªn, thá»© háº¡ng mÃ´ hÃ¬nh trÃªn táº­p Validation thÆ°á»ng tÆ°Æ¡ng Ä‘á»“ng vá»›i káº¿t quáº£ trÃªn táº­p Test, cho tháº¥y mÃ´ hÃ¬nh há»c Ä‘Æ°á»£c cÃ¡c Ä‘áº·c trÆ°ng quan trá»ng mÃ  khÃ´ng bá»‹ overfitting quÃ¡ má»©c.

4.  **Káº¿t Luáº­n**:
    *   MÃ´ hÃ¬nh **Linear cÆ¡ báº£n** káº¿t há»£p vá»›i **RevIN** vÃ  **MSE Loss** lÃ  cáº¥u hÃ¬nh cÃ¢n báº±ng nháº¥t giá»¯a hiá»‡u nÄƒng vÃ  chi phÃ­ tÃ­nh toÃ¡n.
    *   CÃ¡c ká»¹ thuáº­t phá»©c táº¡p hÆ¡n (nhÆ° thay Ä‘á»•i Loss function hay tÄƒng quÃ¡ nhiá»u tham sá»‘) chÆ°a cáº§n thiáº¿t cho bÃ i toÃ¡n cá»¥ thá»ƒ nÃ y.

---
*Dá»± Ã¡n Ä‘Æ°á»£c thá»±c hiá»‡n nháº±m má»¥c Ä‘Ã­ch nghiÃªn cá»©u vÃ  há»c táº­p vá» Time Series Forecasting.*
