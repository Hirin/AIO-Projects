{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D·ª± ƒêo√°n Gi√° C·ªï Phi·∫øu FPT - LTSF-Linear v·ªõi Advanced Grid Search\n",
    "\n",
    "## M·ª•c ti√™u:\n",
    "- D·ª± ƒëo√°n gi√° ƒë√≥ng c·ª≠a FPT cho 100 ng√†y ti·∫øp theo\n",
    "- Grid Search to√†n di·ªán tr√™n:\n",
    "  - **Models**: Linear, DLinear, NLinear\n",
    "  - **Variants**: Univariate (1 feature) vs Multivariate (nhi·ªÅu features)\n",
    "  - **Normalization**: RevIN vs No RevIN\n",
    "  - **Sequence Lengths**: 7, 15, 30, 60, 120, 480\n",
    "  - **Regime Switching**: HMM v·ªõi 3-4 regimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries v√† Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "!pip install hmmlearn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import timedelta\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device available: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    \"\"\"Set random seed for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration - Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "DATA_PATH = 'data/FPT_train.csv'\n",
    "SUBMISSION_DIR = 'submissions/default'\n",
    "PRED_LEN = 100  # Predict 100 days ahead\n",
    "TARGET_COL = 'close'\n",
    "\n",
    "# Grid Search Space\n",
    "MODEL_TYPES = ['GLinear']\n",
    "VARIANTS = ['Univariate', 'Multivariate']\n",
    "# USE_REVIN_OPTIONS removed - GLinear always uses RevIN\n",
    "SEQ_LENS = [7, 15, 30, 60, 120, 480]\n",
    "USE_HMM_OPTIONS = [False, True]\n",
    "N_REGIMES_OPTIONS = [3, 4]\n",
    "REGIME_WINDOWS = [30, 60]\n",
    "\n",
    "# Training Hyperparams\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1000\n",
    "PATIENCE = 15\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(SUBMISSION_DIR, exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  - Prediction Length: {PRED_LEN} days\")\n",
    "print(f\"  - Model Types: {MODEL_TYPES}\")\n",
    "print(f\"  - Variants: {VARIANTS}\")\n",
    "print(f\"  - Sequence Lengths: {SEQ_LENS}\")\n",
    "print(f\"  - HMM Regimes: {N_REGIMES_OPTIONS}\")\n",
    "\n",
    "# Calculate total experiments\n",
    "\n",
    "# 1. Count No-HMM experiments\n",
    "# Variants * Seq_Lens * Models\n",
    "total_no_hmm = len(VARIANTS) * len(SEQ_LENS) * len(MODEL_TYPES)\n",
    "\n",
    "# 2. Count HMM experiments\n",
    "# HMM only runs on seq_len < 120\n",
    "short_seq_lens_count = len([s for s in SEQ_LENS if s < 120])\n",
    "# Variants * Short_Seq_Lens * Models * HMM_Configs\n",
    "hmm_configs_count = len(N_REGIMES_OPTIONS) * len(REGIME_WINDOWS)\n",
    "total_hmm = len(VARIANTS) * short_seq_lens_count * len(MODEL_TYPES) * hmm_configs_count\n",
    "\n",
    "total_experiments = total_no_hmm + total_hmm\n",
    "\n",
    "print(f\"\\nCalculation Logic:\")\n",
    "print(f\"  - No HMM Experiments: {len(VARIANTS)} * {len(SEQ_LENS)} * {len(MODEL_TYPES)} = {total_no_hmm}\")\n",
    "print(f\"  - HMM Experiments: {len(VARIANTS)} * {short_seq_lens_count} * {len(MODEL_TYPES)} * {hmm_configs_count} = {total_hmm}\")\n",
    "print(f\"\\nEstimated total experiments: {total_experiments}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data v√† EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df = df.sort_values('time').reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Date range: {df['time'].min().date()} to {df['time'].max().date()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "display(df.head())\n",
    "\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(f\"\\nBasic statistics:\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize price history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "fig.suptitle('FPT Stock Price - EDA', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Price history\n",
    "axes[0, 0].plot(df['time'], df['close'], color='blue', linewidth=1)\n",
    "axes[0, 0].set_title('Close Price History')\n",
    "axes[0, 0].set_xlabel('Date')\n",
    "axes[0, 0].set_ylabel('Price')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Volume\n",
    "axes[0, 1].bar(df['time'], df['volume'], color='green', alpha=0.5, width=1)\n",
    "axes[0, 1].set_title('Volume')\n",
    "axes[0, 1].set_xlabel('Date')\n",
    "axes[0, 1].set_ylabel('Volume')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Daily returns distribution\n",
    "returns = df['close'].pct_change().dropna()\n",
    "axes[1, 0].hist(returns, bins=50, color='purple', alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_title('Daily Returns Distribution')\n",
    "axes[1, 0].set_xlabel('Return')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].axvline(x=0, color='red', linestyle='--')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Rolling volatility\n",
    "rolling_vol = returns.rolling(window=30).std() * np.sqrt(252)\n",
    "axes[1, 1].plot(df['time'].iloc[1:], rolling_vol, color='red', linewidth=1)\n",
    "axes[1, 1].set_title('30-day Rolling Volatility (Annualized)')\n",
    "axes[1, 1].set_xlabel('Date')\n",
    "axes[1, 1].set_ylabel('Volatility')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering & Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FEATURE ENGINEERING ===\n",
    "\n",
    "# Log Transform (stabilize variance)\n",
    "for col in ['open', 'high', 'low', 'close', 'volume']:\n",
    "    df[f'{col}_log'] = np.log1p(df[col])\n",
    "\n",
    "# Spread Features\n",
    "df['HL_Spread'] = df['high_log'] - df['low_log']  # Intraday range\n",
    "df['OC_Spread'] = df['close_log'] - df['open_log']  # Open-Close spread\n",
    "\n",
    "# Returns & Volatility for HMM Regime Detection\n",
    "df['returns'] = df['close'].pct_change().fillna(0)\n",
    "df['volatility'] = df['returns'].rolling(window=10).std().fillna(0)\n",
    "df['trend'] = df['close'].rolling(window=10).mean().pct_change().fillna(0)\n",
    "\n",
    "print(\"Features created:\")\n",
    "print(f\"  - Log transforms: open_log, high_log, low_log, close_log, volume_log\")\n",
    "print(f\"  - Spread features: HL_Spread, OC_Spread\")\n",
    "print(f\"  - HMM features: returns, volatility, trend\")\n",
    "\n",
    "print(f\"\\nDataset shape after feature engineering: {df.shape}\")\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing = df.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(\"Missing values:\")\n",
    "    print(missing[missing > 0])\n",
    "else:\n",
    "    print(\"‚úì No missing values!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset v√† Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for time series forecasting\"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "def create_sliding_window(data, seq_len, pred_len, target_col_idx, feature_cols_idx):\n",
    "    \"\"\"\n",
    "    Create sliding window sequences for time series forecasting.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_len - pred_len + 1):\n",
    "        X.append(data[i : i + seq_len, feature_cols_idx])\n",
    "        y.append(data[i + seq_len : i + seq_len + pred_len, target_col_idx])\n",
    "    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)\n",
    "\n",
    "print(\"Dataset classes defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Regime Detector (Hidden Markov Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegimeDetector:\n",
    "    \"\"\"\n",
    "    Detect market regimes using Gaussian HMM.\n",
    "    Uses returns, volatility, and trend as features.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components=3, window=30):\n",
    "        self.n_components = n_components\n",
    "        self.window = window\n",
    "        self.model = GaussianHMM(\n",
    "            n_components=n_components, \n",
    "            covariance_type=\"full\", \n",
    "            n_iter=100, \n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "    def fit_predict(self, df):\n",
    "        \"\"\"Fit HMM and predict regimes\"\"\"\n",
    "        features = df[['returns', 'volatility', 'trend']].iloc[self.window:].values\n",
    "        scaler = StandardScaler()\n",
    "        features_scaled = scaler.fit_transform(features)\n",
    "        \n",
    "        self.model.fit(features_scaled)\n",
    "        states = self.model.predict(features_scaled)\n",
    "        \n",
    "        full_states = np.concatenate([np.zeros(self.window) - 1, states])\n",
    "        return full_states\n",
    "\n",
    "print(\"RegimeDetector class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize HMM Regimes\n",
    "detector = RegimeDetector(n_components=3, window=30)\n",
    "regimes = detector.fit_predict(df)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10), sharex=True)\n",
    "\n",
    "colors = ['green', 'yellow', 'red']\n",
    "for regime in range(3):\n",
    "    mask = regimes == regime\n",
    "    axes[0].scatter(df['time'][mask], df['close'][mask], \n",
    "                   c=colors[regime], label=f'Regime {regime}', alpha=0.6, s=10)\n",
    "\n",
    "axes[0].set_title('FPT Close Price by Market Regime (HMM)', fontsize=14)\n",
    "axes[0].set_ylabel('Price')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(df['time'], regimes, drawstyle='steps', color='blue', linewidth=1)\n",
    "axes[1].set_title('Regime Timeline', fontsize=14)\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Regime')\n",
    "axes[1].set_yticks([0, 1, 2])\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRegime Distribution:\")\n",
    "for r in range(3):\n",
    "    count = (regimes == r).sum()\n",
    "    pct = count / len(regimes) * 100\n",
    "    print(f\"  Regime {r}: {count} days ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82847c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize HMM Regimes (4 States)\n",
    "detector = RegimeDetector(n_components=4, window=30)\n",
    "regimes = detector.fit_predict(df)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10), sharex=True)\n",
    "\n",
    "# 4 m√†u cho 4 regimes\n",
    "colors = ['green', 'yellow', 'orange', 'red'] \n",
    "\n",
    "for regime in range(4):\n",
    "    mask = regimes == regime\n",
    "    # D√πng try-except ƒë·ªÅ ph√≤ng tr∆∞·ªùng h·ª£p HMM ra √≠t h∆°n 4 nh√≥m (hi·∫øm g·∫∑p)\n",
    "    if mask.sum() > 0:\n",
    "        axes[0].scatter(df['time'][mask], df['close'][mask], \n",
    "                       c=colors[regime], label=f'Regime {regime}', alpha=0.6, s=10)\n",
    "\n",
    "axes[0].set_title('FPT Close Price by Market Regime (HMM - 4 States)', fontsize=14)\n",
    "axes[0].set_ylabel('Price')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(df['time'], regimes, drawstyle='steps', color='blue', linewidth=1)\n",
    "axes[1].set_title('Regime Timeline', fontsize=14)\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Regime')\n",
    "\n",
    "axes[1].set_yticks([0, 1, 2, 3]) \n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRegime Distribution:\")\n",
    "for r in range(4):\n",
    "    count = (regimes == r).sum()\n",
    "    pct = count / len(regimes) * 100\n",
    "    print(f\"  Regime {r}: {count} days ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. RevIN (Reversible Instance Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RevIN(nn.Module):\n",
    "    \"\"\"\n",
    "    Reversible Instance Normalization.\n",
    "    Handles distribution shift in time series forecasting.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features: int, eps=1e-5, affine=True):\n",
    "        super(RevIN, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "        if self.affine:\n",
    "            self._init_params()\n",
    "\n",
    "    def _init_params(self):\n",
    "        self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n",
    "        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n",
    "\n",
    "    def _get_statistics(self, x):\n",
    "        dim2reduce = tuple(range(1, x.ndim-1))\n",
    "        self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
    "        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n",
    "\n",
    "    def _normalize(self, x):\n",
    "        x = x - self.mean\n",
    "        x = x / self.stdev\n",
    "        if self.affine:\n",
    "            x = x * self.affine_weight\n",
    "            x = x + self.affine_bias\n",
    "        return x\n",
    "\n",
    "    def _denormalize(self, x):\n",
    "        if self.affine:\n",
    "            x = x - self.affine_bias\n",
    "            x = x / (self.affine_weight + self.eps * self.eps)\n",
    "        x = x * self.stdev\n",
    "        x = x + self.mean\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, mode: str):\n",
    "        if mode == 'norm':\n",
    "            self._get_statistics(x)\n",
    "            x = self._normalize(x)\n",
    "        elif mode == 'denorm':\n",
    "            x = self._denormalize(x)\n",
    "        return x\n",
    "\n",
    "print(\"RevIN class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. LTSF-Linear Models (All Variants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# GLINEAR MODEL\n",
    "# =====================================================\n",
    "\n",
    "class GLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    GLinear: Gaussian-activated Linear Model\n",
    "    Structure: RevIN -> Permute -> Linear -> GELU -> Linear -> Permute -> RevIN\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_len, pred_len, num_features):\n",
    "        super(GLinear, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.num_features = num_features\n",
    "        \n",
    "        self.Linear = nn.Linear(self.seq_len, self.seq_len)\n",
    "        self.GeLU = nn.GELU()\n",
    "        self.Hidden1 = nn.Linear(self.seq_len, self.pred_len)\n",
    "        \n",
    "        self.revin_layer = RevIN(self.num_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [Batch, Seq_Len, Channels]\n",
    "        x = self.revin_layer(x, 'norm')\n",
    "        x = x.permute(0, 2, 1) # [Batch, Channels, Seq_Len]\n",
    "        \n",
    "        x = self.Linear(x)\n",
    "        x = self.GeLU(x)\n",
    "        x = self.Hidden1(x)\n",
    "        \n",
    "        x = x.permute(0, 2, 1) # [Batch, Pred_Len, Channels]\n",
    "        x = self.revin_layer(x, 'denorm')\n",
    "        return x\n",
    "\n",
    "class Uni_GLinear(nn.Module):\n",
    "    \"\"\"Univariate GLinear Wrapper\"\"\"\n",
    "    def __init__(self, seq_len, pred_len, num_features):\n",
    "        super().__init__()\n",
    "        # For univariate, num_features is 1\n",
    "        self.model = GLinear(seq_len, pred_len, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [Batch, Seq, 1]\n",
    "        x_in = x[:, :, 0:1]\n",
    "        out = self.model(x_in)\n",
    "        return out[:, :, 0] # Return [Batch, Pred]\n",
    "\n",
    "class Multi_GLinear(nn.Module):\n",
    "    \"\"\"Multivariate GLinear Wrapper\"\"\"\n",
    "    def __init__(self, seq_len, pred_len, num_features):\n",
    "        super().__init__()\n",
    "        self.model = GLinear(seq_len, pred_len, num_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [Batch, Seq, Features]\n",
    "        out = self.model(x)\n",
    "        # We only care about the target column (index 0) for loss/eval\n",
    "        return out[:, :, 0]\n",
    "\n",
    "print(\"GLinear model defined (Univariate & Multivariate)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merged into GLinear definitions above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# MODEL FACTORY\n",
    "# =====================================================\n",
    "\n",
    "MODEL_REGISTRY = {\n",
    "    ('Univariate', 'GLinear'): Uni_GLinear,\n",
    "    ('Multivariate', 'GLinear'): Multi_GLinear,\n",
    "}\n",
    "\n",
    "def create_model(variant, model_type, seq_len, pred_len, num_features):\n",
    "    \"\"\"Factory function to create model\"\"\"\n",
    "    key = (variant, model_type)\n",
    "    if key not in MODEL_REGISTRY:\n",
    "        raise ValueError(f\"Unknown model configuration: {key}\")\n",
    "    return MODEL_REGISTRY[key](seq_len, pred_len, num_features)\n",
    "\n",
    "print(f\"\\nTotal model configurations: {len(MODEL_REGISTRY)}\")\n",
    "for key in MODEL_REGISTRY:\n",
    "    print(f\"  - {key[0]}_{key[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"Training manager with early stopping and learning rate scheduling.\"\"\"\n",
    "    def __init__(self, model, criterion, optimizer, scheduler, patience=10):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.patience = patience\n",
    "        self.best_loss = float('inf')\n",
    "        self.best_state = None\n",
    "        self.counter = 0\n",
    "        \n",
    "    def fit(self, train_loader, val_loader, epochs, verbose=False):\n",
    "        \"\"\"Train the model with early stopping\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(X_batch)\n",
    "                loss = self.criterion(output, y_batch)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            val_loss = self.evaluate(val_loader)\n",
    "            self.scheduler.step(val_loss)\n",
    "            \n",
    "            if val_loss < self.best_loss:\n",
    "                self.best_loss = val_loss\n",
    "                self.best_state = deepcopy(self.model.state_dict())\n",
    "                self.counter = 0\n",
    "            else:\n",
    "                self.counter += 1\n",
    "                \n",
    "            if self.counter >= self.patience:\n",
    "                break\n",
    "                \n",
    "        if self.best_state:\n",
    "            self.model.load_state_dict(self.best_state)\n",
    "            \n",
    "    def evaluate(self, loader):\n",
    "        \"\"\"Evaluate model on a dataset\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                output = self.model(X_batch)\n",
    "                loss = self.criterion(output, y_batch)\n",
    "                total_loss += loss.item()\n",
    "        return total_loss / len(loader)\n",
    "\n",
    "print(\"Trainer class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Grid Search Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform(log_data):\n",
    "    \"\"\"Convert log-transformed data back to original scale\"\"\"\n",
    "    return np.expm1(log_data)\n",
    "\n",
    "\n",
    "def train_model_func(variant, model_type, seq_len, num_features, X_train, y_train, epochs=EPOCHS):\n",
    "    \"\"\"Helper function to train a single model\"\"\"\n",
    "    train_loader = DataLoader(\n",
    "        TimeSeriesDataset(X_train, y_train), \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=0, \n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    model = create_model(variant, model_type, seq_len, PRED_LEN, num_features).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    \n",
    "    trainer = Trainer(model, criterion, optimizer, scheduler, patience=PATIENCE)\n",
    "    trainer.fit(train_loader, train_loader, epochs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model_func(model, X, y):\n",
    "    \"\"\"Evaluate model and compute MSE on original price scale\"\"\"\n",
    "    model.eval()\n",
    "    loader = DataLoader(TimeSeriesDataset(X, y), batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    preds_log, trues_log = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_b, y_b in loader:\n",
    "            X_b = X_b.to(device)\n",
    "            out = model(X_b)\n",
    "            preds_log.append(out.cpu().numpy())\n",
    "            trues_log.append(y_b.numpy())\n",
    "    \n",
    "    preds_log = np.concatenate(preds_log)\n",
    "    trues_log = np.concatenate(trues_log)\n",
    "    \n",
    "    preds_price = inverse_transform(preds_log)\n",
    "    trues_price = inverse_transform(trues_log)\n",
    "    \n",
    "    return mean_squared_error(trues_price.flatten(), preds_price.flatten())\n",
    "\n",
    "\n",
    "def save_submission(predictions, filename):\n",
    "    \"\"\"Save predictions to CSV file\"\"\"\n",
    "    sub_df = pd.DataFrame({\n",
    "        'id': range(1, PRED_LEN + 1), \n",
    "        'close': predictions\n",
    "    })\n",
    "    filepath = os.path.join(SUBMISSION_DIR, filename)\n",
    "    sub_df.to_csv(filepath, index=False)\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def save_plot(history_prices, predictions, filename, title):\n",
    "    \"\"\"Save forecast plot\"\"\"\n",
    "    # fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    # ... (Keep commented out or implement if needed)\n",
    "    pass\n",
    "\n",
    "print(\"Training utilities defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grid_search():\n",
    "    \"\"\"\n",
    "    Main Grid Search Pipeline.\n",
    "    Iterates over all model configurations.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Feature columns\n",
    "    feature_cols = ['close_log', 'volume_log', 'HL_Spread', 'OC_Spread']\n",
    "    target_col_idx = df.columns.get_loc('close_log')\n",
    "    feature_cols_idx = [df.columns.get_loc(c) for c in feature_cols]\n",
    "    data_values = df.values\n",
    "    \n",
    "    # Calculate total combinations\n",
    "    total = 0\n",
    "    for variant in VARIANTS:\n",
    "        for model_type in MODEL_TYPES:\n",
    "            for seq_len in SEQ_LENS:\n",
    "                for use_hmm in USE_HMM_OPTIONS:\n",
    "                    # Logic b·ªè qua HMM cho seq d√†i\n",
    "                    if seq_len >= 120 and use_hmm:\n",
    "                        continue\n",
    "                    \n",
    "                    # C·ªông s·ªë l∆∞·ª£ng experiment\n",
    "                    if use_hmm:\n",
    "                        total += len(N_REGIMES_OPTIONS) * len(REGIME_WINDOWS)\n",
    "                    else:\n",
    "                        total += 1\n",
    "                            \n",
    "    pbar = tqdm(total=total, desc=\"Grid Search\")\n",
    "    \n",
    "    for variant in VARIANTS:\n",
    "        for model_type in MODEL_TYPES:\n",
    "            for seq_len in SEQ_LENS:\n",
    "                for use_hmm in USE_HMM_OPTIONS:\n",
    "                    \n",
    "                    # Skip HMM for long sequences\n",
    "                    if seq_len >= 120 and use_hmm:\n",
    "                        continue\n",
    "                    \n",
    "                    hmm_configs = [(None, None)]\n",
    "                    if use_hmm:\n",
    "                        hmm_configs = [(n, w) for n in N_REGIMES_OPTIONS for w in REGIME_WINDOWS]\n",
    "                    \n",
    "                    for n_regimes, regime_window in hmm_configs:\n",
    "                        try:\n",
    "                            seed_everything(42)\n",
    "                            \n",
    "                            # === Data Preparation ===\n",
    "                            regimes = None\n",
    "                            if use_hmm:\n",
    "                                detector = RegimeDetector(n_components=n_regimes, window=regime_window)\n",
    "                                regimes = detector.fit_predict(df)\n",
    "                            \n",
    "                            # Train/Val split (80/20)\n",
    "                            train_size = int(len(data_values) * 0.8)\n",
    "                            train_data = data_values[:train_size]\n",
    "                            val_data = data_values[train_size - seq_len:]\n",
    "                            \n",
    "                            X_train, y_train = create_sliding_window(train_data, seq_len, PRED_LEN, target_col_idx, feature_cols_idx)\n",
    "                            X_val, y_val = create_sliding_window(val_data, seq_len, PRED_LEN, target_col_idx, feature_cols_idx)\n",
    "                            \n",
    "                            num_features = len(feature_cols_idx)\n",
    "                            \n",
    "                            # === Train Global Model (Baseline) ===\n",
    "                            global_model = train_model_func(variant, model_type, seq_len, num_features, X_train, y_train)\n",
    "                            \n",
    "                            # === Evaluation ===\n",
    "                            if not use_hmm:\n",
    "                                val_mse = evaluate_model_func(global_model, X_val, y_val)\n",
    "                            else:\n",
    "                                val_mse = evaluate_with_hmm(\n",
    "                                    global_model, variant, model_type,\n",
    "                                    seq_len, num_features,\n",
    "                                    X_train, y_train, X_val, y_val,\n",
    "                                    regimes, train_size\n",
    "                                )\n",
    "                            \n",
    "                            # === Production Forecast ===\n",
    "                            # 1. Train Global Model tr√™n full data (ho·∫∑c 95% data)\n",
    "                            prod_train_size = int(len(data_values) * 0.95)\n",
    "                            prod_train_data = data_values[:prod_train_size]\n",
    "                            X_prod, y_prod = create_sliding_window(prod_train_data, seq_len, PRED_LEN, target_col_idx, feature_cols_idx)\n",
    "                            \n",
    "                            final_model = train_model_func(variant, model_type, seq_len, num_features, X_prod, y_prod, epochs=EPOCHS//2)\n",
    "                            \n",
    "                            # Prepare input for forecast\n",
    "                            last_sequence = data_values[-seq_len:, feature_cols_idx]\n",
    "                            last_seq_tensor = torch.tensor(last_sequence.astype(np.float32)).unsqueeze(0).to(device)\n",
    "                            \n",
    "                            # 2. Base Forecast (Global)\n",
    "                            final_model.eval()\n",
    "                            with torch.no_grad():\n",
    "                                pred_log = final_model(last_seq_tensor).cpu().numpy().flatten()\n",
    "                            \n",
    "                            # 3. Forecast Correction with HMM\n",
    "                            if use_hmm:\n",
    "                                current_regime = regimes[-1] # L·∫•y regime c·ªßa ng√†y cu·ªëi c√πng\n",
    "                                \n",
    "                                prod_regime_indices = []\n",
    "                                for i in range(len(X_prod)):\n",
    "                                    r_idx = i + seq_len - 1\n",
    "                                    if r_idx < len(regimes):\n",
    "                                        prod_regime_indices.append(regimes[r_idx])\n",
    "                                    else:\n",
    "                                        prod_regime_indices.append(-1)\n",
    "                                \n",
    "                                prod_regime_indices = np.array(prod_regime_indices)\n",
    "                                mask = (prod_regime_indices == current_regime)\n",
    "                                \n",
    "                                if mask.sum() > 30:\n",
    "                                    X_regime = X_prod[mask]\n",
    "                                    y_regime = y_prod[mask]\n",
    "                                    \n",
    "                                    regime_model = train_model_func(variant, model_type, seq_len, num_features, \n",
    "                                                                  X_regime, y_regime, epochs=EPOCHS//2)\n",
    "                                    \n",
    "                                    regime_model.eval()\n",
    "                                    with torch.no_grad():\n",
    "                                        pred_log = regime_model(last_seq_tensor).cpu().numpy().flatten()\n",
    "                            \n",
    "                            # === Save Results ===\n",
    "                            pred_price = inverse_transform(pred_log)\n",
    "                            \n",
    "                            hmm_status = f\"HMM{n_regimes}W{regime_window}\" if use_hmm else \"NoHMM\"\n",
    "                            filename = f\"Sub_{variant}_{model_type}_{hmm_status}_Seq{seq_len}_MSE{val_mse:.0f}.csv\"\n",
    "                            \n",
    "                            save_submission(pred_price, filename)\n",
    "                            save_plot(df['close'], pred_price, filename, \n",
    "                                     f\"{variant} {model_type} | Seq={seq_len} | MSE={val_mse:.0f}\")\n",
    "                            \n",
    "                            results.append({\n",
    "                                'Variant': variant,\n",
    "                                'Model': model_type,\n",
    "                                'HMM': hmm_status,\n",
    "                                'SeqLen': seq_len,\n",
    "                                'ValMSE': val_mse,\n",
    "                                'File': filename\n",
    "                            })\n",
    "                            \n",
    "                            pbar.set_postfix({'Last': f\"{variant[:3]}_{model_type}_{seq_len}\", 'MSE': f\"{val_mse:.0f}\"})\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"\\n‚úó Error: {variant}_{model_type}_Seq{seq_len}: {e}\")\n",
    "                            import traceback\n",
    "                            traceback.print_exc()\n",
    "                        \n",
    "                        pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_with_hmm(global_model, variant, model_type, seq_len, num_features,\n",
    "                      X_train, y_train, X_val, y_val, regimes, train_size):\n",
    "    \"\"\"Evaluate with regime-switching models\"\"\"\n",
    "    regime_models = {}\n",
    "    train_regimes = regimes[:train_size]\n",
    "    unique_regimes = np.unique(train_regimes)\n",
    "    \n",
    "    # Map index X_train sang Regime\n",
    "    train_regime_indices = []\n",
    "    for i in range(len(X_train)):\n",
    "        r_idx = i + seq_len - 1\n",
    "        if r_idx < len(train_regimes):\n",
    "            train_regime_indices.append(train_regimes[r_idx])\n",
    "        else:\n",
    "            train_regime_indices.append(-1)\n",
    "    train_regime_indices = np.array(train_regime_indices)\n",
    "    \n",
    "    # Train model ri√™ng cho t·ª´ng regime\n",
    "    for r in unique_regimes:\n",
    "        if r == -1:\n",
    "            continue\n",
    "        mask = (train_regime_indices == r)\n",
    "        if mask.sum() > 30:\n",
    "            X_r = X_train[mask]\n",
    "            y_r = y_train[mask]\n",
    "            regime_models[r] = train_model_func(variant, model_type, seq_len, num_features, X_r, y_r)\n",
    "    \n",
    "    val_preds_log, val_trues_log = [], []\n",
    "    \n",
    "    global_model.eval()\n",
    "    for model in regime_models.values():\n",
    "        model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(len(X_val)):\n",
    "            # X√°c ƒë·ªãnh regime c·ªßa m·∫´u validation hi·ªán t·∫°i\n",
    "            global_idx = train_size + i - 1\n",
    "            curr_regime = regimes[global_idx] if global_idx < len(regimes) else -1\n",
    "            \n",
    "            # Ch·ªçn model: N·∫øu c√≥ model regime th√¨ d√πng, ko th√¨ d√πng global\n",
    "            selected_model = regime_models.get(curr_regime, global_model)\n",
    "            \n",
    "            inp = torch.tensor(X_val[i]).unsqueeze(0).to(device)\n",
    "            pred = selected_model(inp).cpu().numpy()\n",
    "            \n",
    "            val_preds_log.append(pred)\n",
    "            val_trues_log.append(y_val[i])\n",
    "    \n",
    "    val_preds_log = np.concatenate(val_preds_log)\n",
    "    val_trues_log = np.array(val_trues_log)\n",
    "    \n",
    "    pred_price = inverse_transform(val_preds_log)\n",
    "    true_price = inverse_transform(val_trues_log)\n",
    "    \n",
    "    return mean_squared_error(true_price.flatten(), pred_price.flatten())\n",
    "\n",
    "print(\"Grid Search pipeline defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Run Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38e02f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_split():\n",
    "    # 1. L·∫•y d·ªØ li·ªáu g·ªëc\n",
    "    df_plot = pd.read_csv('data/FPT_train.csv') # ƒê·∫£m b·∫£o ƒë∆∞·ªùng d·∫´n ƒë√∫ng\n",
    "    data = df_plot['close'].values\n",
    "    total_len = len(data)\n",
    "    \n",
    "    # 2. T√≠nh to√°n c√°c ƒëi·ªÉm c·∫Øt (Theo logic c·ªßa code t·ªëi ∆∞u)\n",
    "    TEST_LEN = 100\n",
    "    \n",
    "    # Ph·∫ßn d√†nh cho ph√°t tri·ªÉn model (Dev Set)\n",
    "    dev_len = total_len - TEST_LEN\n",
    "    \n",
    "    # Trong Dev Set, chia 80% Train, 20% Val\n",
    "    train_len = int(dev_len * 0.8)\n",
    "    val_len = dev_len - train_len\n",
    "    \n",
    "    # 3. V·∫Ω bi·ªÉu ƒë·ªì\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # V·∫Ω v√πng Train\n",
    "    plt.plot(range(0, train_len), data[0:train_len], \n",
    "             color='#1f77b4', label=f'TRAIN Set ({train_len} days)')\n",
    "    \n",
    "    # V·∫Ω v√πng Validation (n·ªëi ti·∫øp Train)\n",
    "    plt.plot(range(train_len - 1, dev_len), data[train_len - 1:dev_len], \n",
    "             color='#ff7f0e', label=f'VALIDATION Set ({val_len} days)')\n",
    "    \n",
    "    # V·∫Ω v√πng Test (n·ªëi ti·∫øp Val)\n",
    "    plt.plot(range(dev_len - 1, total_len), data[dev_len - 1:total_len], \n",
    "             color='#2ca02c', linewidth=2, label=f'INTERNAL TEST Set ({TEST_LEN} days)')\n",
    "    \n",
    "    # T√¥ m√†u n·ªÅn ƒë·ªÉ d·ªÖ ph√¢n bi·ªát\n",
    "    plt.axvspan(0, train_len, color='#1f77b4', alpha=0.1)\n",
    "    plt.axvspan(train_len, dev_len, color='#ff7f0e', alpha=0.1)\n",
    "    plt.axvspan(dev_len, total_len, color='#2ca02c', alpha=0.1)\n",
    "    \n",
    "    # Ch√∫ th√≠ch\n",
    "    plt.title(f\"Data Splitting Strategy (Total: {total_len} days)\", fontsize=14)\n",
    "    plt.xlabel(\"Time Steps (Days)\")\n",
    "    plt.ylabel(\"Close Price\")\n",
    "    plt.legend(loc='upper left', frameon=True, shadow=True)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # In ra index c·ª• th·ªÉ\n",
    "    print(f\"{'Set':<15} | {'Start Index':<12} | {'End Index':<12} | {'Length':<10}\")\n",
    "    print(\"-\" * 55)\n",
    "    print(f\"{'TRAIN':<15} | {0:<12} | {train_len:<12} | {train_len}\")\n",
    "    print(f\"{'VALIDATION':<15} | {train_len:<12} | {dev_len:<12} | {val_len}\")\n",
    "    print(f\"{'INTERNAL TEST':<15} | {dev_len:<12} | {total_len:<12} | {TEST_LEN}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_data_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"STARTING GRID SEARCH\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nGrid Search Space:\")\n",
    "print(f\"  - Variants: {VARIANTS}\")\n",
    "print(f\"  - Models: {MODEL_TYPES}\")\n",
    "# print(f\"  - RevIN: {USE_REVIN_OPTIONS}\")\n",
    "print(f\"  - Seq Lengths: {SEQ_LENS}\")\n",
    "print(f\"  - HMM: {USE_HMM_OPTIONS}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "results = run_grid_search()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GRID SEARCH COMPLETED\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results).sort_values('ValMSE')\n",
    "results_df.to_csv('results/grid_search_results.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nTotal experiments: {len(results_df)}\")\n",
    "print(f\"\\nTop 15 Best Models:\")\n",
    "display(results_df.head(15))\n",
    "\n",
    "print(f\"\\nWorst 5 Models:\")\n",
    "display(results_df.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# By Variant\n",
    "print(\"\\nüìä Average MSE by Variant:\")\n",
    "print(results_df.groupby('Variant')['ValMSE'].agg(['mean', 'std', 'min', 'max']).round(2))\n",
    "\n",
    "# By Model Type\n",
    "print(\"\\nüìä Average MSE by Model Type:\")\n",
    "print(results_df.groupby('Model')['ValMSE'].agg(['mean', 'std', 'min', 'max']).round(2))\n",
    "\n",
    "# By Sequence Length\n",
    "print(\"\\nüìä Average MSE by Sequence Length:\")\n",
    "print(results_df.groupby('SeqLen')['ValMSE'].agg(['mean', 'std', 'min', 'max']).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Grid Search Results Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. MSE by Variant\n",
    "variant_mse = results_df.groupby('Variant')['ValMSE'].mean()\n",
    "axes[0, 0].bar(variant_mse.index, variant_mse.values, color=['blue', 'green'])\n",
    "axes[0, 0].set_title('Average MSE by Variant', fontsize=12)\n",
    "axes[0, 0].set_ylabel('MSE')\n",
    "\n",
    "# 2. MSE by Sequence Length\n",
    "seq_mse = results_df.groupby('SeqLen')['ValMSE'].mean()\n",
    "axes[0, 1].bar(seq_mse.index.astype(str), seq_mse.values, color='purple')\n",
    "axes[0, 1].set_title('Average MSE by Sequence Length', fontsize=12)\n",
    "axes[0, 1].set_xlabel('Sequence Length')\n",
    "axes[0, 1].set_ylabel('MSE')\n",
    "\n",
    "# 3. MSE by HMM\n",
    "hmm_mse = results_df.groupby('HMM')['ValMSE'].mean().sort_values()\n",
    "axes[1, 0].barh(hmm_mse.index, hmm_mse.values, color='orange')\n",
    "axes[1, 0].set_title('Average MSE by HMM Configuration', fontsize=12)\n",
    "axes[1, 0].set_xlabel('MSE')\n",
    "\n",
    "# 4. Top 15 Models\n",
    "top15 = results_df.head(15)\n",
    "labels = [f\"{r['Variant'][:3]}_{r['Model']}\\nSeq={r['SeqLen']}\" for _, r in top15.iterrows()]\n",
    "axes[1, 1].barh(range(len(top15)), top15['ValMSE'].values, color='green')\n",
    "axes[1, 1].set_yticks(range(len(top15)))\n",
    "axes[1, 1].set_yticklabels(labels, fontsize=8)\n",
    "axes[1, 1].set_title('Top 15 Best Models by MSE', fontsize=12)\n",
    "axes[1, 1].set_xlabel('MSE')\n",
    "axes[1, 1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/grid_search_analysis.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap: Variant vs Sequence Length\n",
    "pivot_seq = results_df.pivot_table(\n",
    "    values='ValMSE', index='Variant', columns='SeqLen', aggfunc='mean'\n",
    ")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pivot_seq, annot=True, fmt='.0f', cmap='RdYlGn_r')\n",
    "plt.title('Average MSE: Variant vs Sequence Length', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/heatmap_analysis.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary v√† K·∫øt lu·∫≠n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìä Dataset: FPT Stock ({len(df)} days)\")\n",
    "print(f\"üìÖ Date Range: {df['time'].min().date()} to {df['time'].max().date()}\")\n",
    "\n",
    "print(f\"\\nüî¨ Total Experiments: {len(results_df)}\")\n",
    "\n",
    "print(f\"\\nüèÜ TOP 5 BEST MODELS:\")\n",
    "for i, (_, row) in enumerate(results_df.head(5).iterrows(), 1):\n",
    "    print(f\"   {i}. {row['Variant']}_{row['Model']} | Seq={row['SeqLen']} | {row['HMM']} | MSE={row['ValMSE']:.2f}\")\n",
    "\n",
    "print(f\"\\nüìà KEY FINDINGS:\")\n",
    "best_variant = results_df.groupby('Variant')['ValMSE'].mean().idxmin()\n",
    "best_model = results_df.groupby('Model')['ValMSE'].mean().idxmin()\n",
    "best_seq = results_df.groupby('SeqLen')['ValMSE'].mean().idxmin()\n",
    "\n",
    "print(f\"   - Best Variant: {best_variant}\")\n",
    "print(f\"   - Best Model Type: {best_model}\")\n",
    "print(f\"   - Best Sequence Length: {best_seq}\")\n",
    "\n",
    "print(f\"\\nüìÅ OUTPUT FILES:\")\n",
    "print(f\"   - Results CSV: results/grid_search_results.csv\")\n",
    "print(f\"   - Analysis Plot: results/grid_search_analysis.png\")\n",
    "print(f\"   - Heatmap: results/heatmap_analysis.png\")\n",
    "print(f\"   - Submissions: {SUBMISSION_DIR}/ ({len(results_df)} files)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DONE! üéâ\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all submission files\n",
    "print(\"\\nüìÇ All Submission Files:\")\n",
    "print(\"-\" * 80)\n",
    "for i, filename in enumerate(sorted(results_df['File'].tolist()), 1):\n",
    "    print(f\"{i:3d}. {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}